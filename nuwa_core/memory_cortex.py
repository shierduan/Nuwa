"""
è®°å¿†çš®å±‚æ¨¡å— (Memory Cortex Module)

åŠŸèƒ½ï¼šå°è£… LanceDB è¿æ¥ï¼Œæä¾›åŸºäºæƒ…ç»ªä¸€è‡´æ€§çš„è®°å¿†æ£€ç´¢æ¥å£ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
- MemoryCortex: è®°å¿†çš®å±‚ç±»ï¼Œå°è£… LanceDB æ“ä½œ
- recall_by_emotion(): RAG æ£€ç´¢æ¥å£ï¼Œä¼˜å…ˆæ£€ç´¢è¯­ä¹‰ç›¸å…³çš„è®°å¿†ï¼Œå¹¶æ ¹æ®æƒ…ç»ªå‘é‡è¿›è¡ŒåŠ æƒ
"""

import os
import json
from typing import List, Dict, Any, Optional
import time
from datetime import datetime
from difflib import SequenceMatcher

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None
    NUMPY_AVAILABLE = False

try:
    import pyarrow as pa
    PYARROW_AVAILABLE = True
except ImportError:
    pa = None
    PYARROW_AVAILABLE = False

try:
    import lancedb
    LANCEDB_AVAILABLE = True
except ImportError:
    lancedb = None
    LANCEDB_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer
    EMBEDDING_AVAILABLE = True
except ImportError:
    SentenceTransformer = None
    EMBEDDING_AVAILABLE = False

from .model_utils import ensure_embedding_model_dir


class MemoryCortex:
    """
    è®°å¿†çš®å±‚ç±»
    
    å°è£… LanceDB è¿æ¥ï¼Œæä¾›è®°å¿†å­˜å‚¨å’Œæ£€ç´¢åŠŸèƒ½ã€‚
    æ”¯æŒåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦å’Œæƒ…ç»ªä¸€è‡´æ€§çš„è®°å¿†æ£€ç´¢ã€‚
    """
    
    # å‘é‡ç»´åº¦ï¼ˆall-MiniLM-L6-v2 çš„ç»´åº¦æ˜¯ 384ï¼‰
    VECTOR_DIM = 384
    
    def __init__(self, project_name: str, data_dir: str = "data"):
        """
        åˆå§‹åŒ–è®°å¿†çš®å±‚
        
        Args:
            project_name: é¡¹ç›®åç§°
            data_dir: æ•°æ®ç›®å½•ï¼ˆé»˜è®¤ "data"ï¼‰
        """
        self.project_name = project_name
        self.data_dir = data_dir
        self.db_path = os.path.join(data_dir, project_name, "memory.lance")
        
        # å®šä¹‰æ˜¾å¼ Schemaï¼ˆé¿å… PyArrow è‡ªåŠ¨æ¨æ–­é”™è¯¯ï¼‰
        self.schema = self._define_schema()
        
        # åˆå§‹åŒ– LanceDB è¿æ¥
        self.db = None
        self.table = None
        self._init_db()
        
        # åˆå§‹åŒ– Embedding æ¨¡å‹
        self.embedding_model = None
        self._init_embedding_model()
    
    def _define_schema(self) -> Optional[pa.Schema]:
        """
        å®šä¹‰æ˜¾å¼ Schema
        
        Returns:
            PyArrow Schema å¯¹è±¡
        """
        if not PYARROW_AVAILABLE or pa is None:
            return None
        
        return pa.schema([
            pa.field("id", pa.string()),
            pa.field("text", pa.string()),
            pa.field("vector", pa.list_(pa.float32(), self.VECTOR_DIM)),  # 384ç»´å‘é‡
            pa.field("emotion_vector", pa.string()),  # JSON å­—ç¬¦ä¸²ï¼ˆåµŒå…¥ä½¿ç”¨ï¼‰
            pa.field("timestamp", pa.float64()),
            pa.field("importance", pa.float32()),
            pa.field("type", pa.string()),  # raw / summary / other
            pa.field("emotions", pa.string()),  # JSON å­—å…¸ï¼ˆæƒ…ç»ªæ ‡ç­¾ï¼‰
            pa.field("access_count", pa.int64()),
        ])
    
    def _init_db(self):
        """åˆå§‹åŒ– LanceDB æ•°æ®åº“"""
        if not LANCEDB_AVAILABLE:
            print("âš ï¸ LanceDB ä¸å¯ç”¨ï¼Œè®°å¿†æ£€ç´¢åŠŸèƒ½å°†å—é™")
            return
        
        if not PYARROW_AVAILABLE:
            print("âš ï¸ PyArrow ä¸å¯ç”¨ï¼Œæ— æ³•å®šä¹‰ Schema")
            return
        
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
            
            # è¿æ¥æ•°æ®åº“
            self.db = lancedb.connect(self.db_path)
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            table_names = self.db.table_names()
            if "memory" not in table_names:
                # è¡¨ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ˜¾å¼ Schema åˆ›å»ºç©ºè¡¨
                if self.schema:
                    # åˆ›å»ºä¸€ä¸ªç©ºçš„æ•°æ®åˆ—è¡¨æ¥åˆå§‹åŒ–è¡¨
                    empty_data = [{
                        "id": "",
                        "text": "",
                        "vector": [0.0] * self.VECTOR_DIM,
                        "emotion_vector": "",
                        "timestamp": 0.0,
                        "importance": 0.0,
                        "type": "raw",
                        "emotions": "",
                        "access_count": 0,
                    }]
                    self.table = self.db.create_table(
                        "memory",
                        empty_data,
                        schema=self.schema,
                        mode="overwrite"
                    )
                    # åˆ›å»ºåç«‹å³åˆ é™¤è¿™ä¸ªç©ºè®°å½•
                    # æ³¨æ„ï¼šLanceDB å¯èƒ½ä¸æ”¯æŒç›´æ¥åˆ é™¤ï¼Œæ‰€ä»¥æˆ‘ä»¬å…ˆåˆ›å»ºç©ºè¡¨ï¼Œåç»­æ’å…¥æ—¶è¦†ç›–
                    print(f"ğŸ“ åˆ›å»ºæ–°çš„è®°å¿†è¡¨: {self.db_path}")
                else:
                    print("âš ï¸ Schema æœªå®šä¹‰ï¼Œæ— æ³•åˆ›å»ºè¡¨")
            else:
                # è¡¨å·²å­˜åœ¨ï¼Œå°è¯•æ‰“å¼€è¡¨
                try:
                    self.table = self.db.open_table("memory")
                    if self.table:
                        if not self._schema_is_compatible(self.table.schema.names):
                            print("âš ï¸ æ£€æµ‹åˆ°æ—§çš„è®°å¿†è¡¨ç»“æ„ï¼Œæ­£åœ¨é‡å»ºä»¥æ”¯æŒ Memory Dreamer...")
                            self.db.drop_table("memory")
                            self._create_empty_table()
                            self.table = self.db.open_table("memory")
                        else:
                            print(f"âœ… å·²åŠ è½½è®°å¿†è¡¨: {self.db_path}")
                    else:
                        print(f"âš ï¸ è¡¨æ‰“å¼€å¤±è´¥ï¼Œå°è¯•é‡æ–°åˆ›å»º...")
                        self._create_empty_table()
                        self.table = self.db.open_table("memory")
                        print(f"âœ… å·²é‡æ–°åˆ›å»ºè®°å¿†è¡¨: {self.db_path}")
                except Exception as e:
                    print(f"âš ï¸ æ‰“å¼€è¡¨å¤±è´¥: {e}ï¼Œå°è¯•é‡æ–°åˆ›å»º...")
                    try:
                        self.db.drop_table("memory")
                        self._create_empty_table()
                        self.table = self.db.open_table("memory")
                        print(f"âœ… å·²é‡æ–°åˆ›å»ºè®°å¿†è¡¨: {self.db_path}")
                    except Exception as e2:
                        print(f"âš ï¸ é‡æ–°åˆ›å»ºè¡¨ä¹Ÿå¤±è´¥: {e2}")
                        self.table = None
        except Exception as e:
            print(f"âš ï¸ åˆå§‹åŒ– LanceDB å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.db = None
            self.table = None

    def _create_empty_table(self):
        """åˆ›å»ºä¸€ä¸ªç©ºçš„ LanceDB è¡¨ä»¥åŒ¹é…æœ€æ–° Schema"""
        empty_data = [{
            "id": "",
            "text": "",
            "vector": [0.0] * self.VECTOR_DIM,
            "emotion_vector": "",
            "timestamp": 0.0,
            "importance": 0.0,
            "type": "raw",
            "emotions": "",
            "access_count": 0,
        }]
        self.table = self.db.create_table(
            "memory",
            empty_data,
            schema=self.schema,
            mode="overwrite"
        )

    def _schema_is_compatible(self, names: List[str]) -> bool:
        required = {"id", "text", "vector", "emotion_vector", "timestamp", "importance", "type", "emotions", "access_count"}
        return required.issubset(set(names))

    def _init_embedding_model(self):
        """åˆå§‹åŒ– Embedding æ¨¡å‹"""
        if not EMBEDDING_AVAILABLE or SentenceTransformer is None:
            print("âš ï¸ SentenceTransformer ä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆå‘é‡")
            return
        
        try:
            # è·å–æ¨¡å‹è·¯å¾„
            model_path = ensure_embedding_model_dir(SentenceTransformer)
            if not model_path:
                print("âš ï¸ æ— æ³•åŠ è½½ Embedding æ¨¡å‹ï¼šç¼ºå°‘å¯ç”¨çš„æœ¬åœ°ç›®å½•ï¼Œä¸”è‡ªåŠ¨ä¸‹è½½å¤±è´¥ã€‚")
                return
            
            # åŠ è½½æ¨¡å‹
            self.embedding_model = SentenceTransformer(model_path, local_files_only=True)
            print(f"âœ… å·²åŠ è½½ Embedding æ¨¡å‹: {model_path}")
        except Exception as e:
            print(f"âš ï¸ åˆå§‹åŒ– Embedding æ¨¡å‹å¤±è´¥: {e}")
            self.embedding_model = None
    
    def store_memory(self, text: str, metadata: Optional[Dict[str, Any]] = None, timestamp: Optional[datetime] = None):
        """
        å­˜å‚¨è®°å¿†ï¼ˆå†…åœ¨æ—¶é—´æ„ŸçŸ¥ç‰ˆæœ¬ï¼‰
        
        å…³é”®ç‰¹æ€§ï¼šå°†æ—¶é—´æˆ³ç›´æ¥èåˆåˆ°æ–‡æœ¬å†…å®¹ä¸­ï¼Œæ ¼å¼ä¸º [YYYY-MM-DD HH:MM:SS] æ–‡æœ¬å†…å®¹
        è¿™æ ·å°æ¨¡å‹ï¼ˆ4Bï¼‰å¯ä»¥ç›´æ¥ä»æ£€ç´¢åˆ°çš„æ–‡æœ¬ä¸­è¯»å–æ—¶é—´ä¿¡æ¯ï¼Œå®ç°æ—¶é—´æ„ŸçŸ¥æ¨ç†ã€‚
        
        Args:
            text: è®°å¿†æ–‡æœ¬ï¼ˆåŸå§‹æ–‡æœ¬ï¼Œæ—¶é—´æˆ³ä¼šè‡ªåŠ¨æ·»åŠ ï¼‰
            metadata: å¯é€‰çš„å…ƒæ•°æ®ï¼ˆå¦‚æ—¶é—´æˆ³ã€æƒ…ç»ªå‘é‡ç­‰ï¼‰
            timestamp: å¯é€‰çš„ datetime å¯¹è±¡ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å½“å‰æ—¶é—´
        """
        if not LANCEDB_AVAILABLE or self.db is None:
            return
        
        if not self.embedding_model:
            print("âš ï¸ Embedding æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•å­˜å‚¨è®°å¿†")
            return
        
        if not self.table:
            print("âš ï¸ è®°å¿†è¡¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•å­˜å‚¨è®°å¿†")
            return
        
        try:
            # å¤„ç†æ—¶é—´æˆ³ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ timestampï¼Œå…¶æ¬¡ä½¿ç”¨ metadata ä¸­çš„ timestampï¼Œæœ€åä½¿ç”¨å½“å‰æ—¶é—´
            if timestamp is not None:
                memory_datetime = timestamp
            elif metadata and "timestamp" in metadata and metadata["timestamp"] is not None:
                # å¦‚æœ metadata ä¸­æ˜¯ floatï¼ˆUnix æ—¶é—´æˆ³ï¼‰ï¼Œè½¬æ¢ä¸º datetime
                ts_value = metadata["timestamp"]
                if isinstance(ts_value, (int, float)):
                    memory_datetime = datetime.fromtimestamp(float(ts_value))
                elif isinstance(ts_value, datetime):
                    memory_datetime = ts_value
                else:
                    memory_datetime = datetime.now()
            else:
                memory_datetime = datetime.now()
            
            # æ ¼å¼åŒ–æ—¶é—´æˆ³å­—ç¬¦ä¸²ï¼šYYYY-MM-DD HH:MM:SS
            timestamp_str = memory_datetime.strftime('%Y-%m-%d %H:%M:%S')
            
            # å…³é”®ï¼šå°†æ—¶é—´æˆ³èåˆåˆ°æ–‡æœ¬å†…å®¹ä¸­ï¼ˆå†…åœ¨æ—¶é—´æ„ŸçŸ¥ï¼‰
            # æ ¼å¼ï¼š[YYYY-MM-DD HH:MM:SS] æ–‡æœ¬å†…å®¹
            fused_text = f"[{timestamp_str}] {text}"
            
            # ä½¿ç”¨èåˆåçš„æ–‡æœ¬ç”Ÿæˆå‘é‡ï¼ˆè¿™æ · embedding ä¹ŸåŒ…å«æ—¶é—´ä¿¡æ¯ï¼‰
            vector = self.embedding_model.encode(fused_text, convert_to_numpy=True)
            
            # ç¡®ä¿å‘é‡ç»´åº¦æ­£ç¡®
            if len(vector) != self.VECTOR_DIM:
                print(f"âš ï¸ å‘é‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.VECTOR_DIM}ï¼Œå®é™… {len(vector)}")
                return
            
            # å‡†å¤‡æ•°æ®ï¼ˆç¡®ä¿ç¬¦åˆ Schemaï¼‰
            memory_id = f"{int(time.time() * 1000)}_{hash(text) % 10000}"
            
            # å¤„ç† emotion_vectorï¼ˆè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ï¼‰
            emotion_vector_str = ""
            if metadata and "emotion_vector" in metadata and metadata["emotion_vector"] is not None:
                emotion_vec = metadata["emotion_vector"]
                if isinstance(emotion_vec, np.ndarray):
                    emotion_vec = emotion_vec.tolist()
                elif not isinstance(emotion_vec, list):
                    emotion_vec = list(emotion_vec)
                emotion_vector_str = json.dumps(emotion_vec, ensure_ascii=False)
            
            # æƒ…ç»ªå­—å…¸ï¼ˆç”¨äº Memory Dreamerï¼‰
            emotions_dict = {}
            if metadata and "emotions" in metadata and metadata["emotions"] is not None:
                emotions_dict = metadata["emotions"]
            elif hasattr(self, "state") and self.state is not None:
                emotions_dict = getattr(self.state, "emotional_spectrum", {})
            emotions_str = json.dumps(emotions_dict, ensure_ascii=False)
            
            # å°† datetime è½¬æ¢ä¸º Unix æ—¶é—´æˆ³ï¼ˆç”¨äºæ•°æ®åº“å­˜å‚¨ï¼‰
            memory_timestamp = float(memory_datetime.timestamp())
            
            # æ„å»ºç¬¦åˆ Schema çš„æ•°æ®
            # æ³¨æ„ï¼štext å­—æ®µå­˜å‚¨èåˆåçš„æ–‡æœ¬ï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰ï¼Œè¿™æ ·æ£€ç´¢æ—¶å¯ä»¥ç›´æ¥çœ‹åˆ°æ—¶é—´
            data = {
                "id": memory_id,
                "text": fused_text,  # å­˜å‚¨èåˆåçš„æ–‡æœ¬ï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
                "vector": vector.tolist(),  # ä½¿ç”¨èåˆæ–‡æœ¬ç”Ÿæˆçš„å‘é‡
                "emotion_vector": emotion_vector_str,  # JSON å­—ç¬¦ä¸²
                "timestamp": memory_timestamp,  # Unix æ—¶é—´æˆ³ï¼ˆç”¨äºæ’åºå’ŒæŸ¥è¯¢ï¼‰
                "importance": float(metadata.get("importance", 0.5)) if metadata else 0.5,
                "type": str(metadata.get("type", "raw")) if metadata else "raw",
                "emotions": emotions_str,
                "access_count": int(metadata.get("access_count", 0)) if metadata else 0,
            }
            
            # æ’å…¥æ•°æ®
            self.table.add([data])

            # ç»ˆç«¯è°ƒè¯•è¾“å‡ºï¼šå‘é‡å†™å…¥ï¼ˆæ˜¾ç¤ºèåˆåçš„æ–‡æœ¬ï¼‰
            preview = fused_text.replace("\n", " ")[:100]
            print(f"âœ… [Memory][WRITE] å·²å­˜å‚¨è®°å¿† (id={memory_id}, time={timestamp_str}, importance={data['importance']:.2f}): {preview}...")
        except Exception as e:
            print(f"âš ï¸ å­˜å‚¨è®°å¿†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _calculate_string_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆ0.0-1.0ï¼‰
        
        ä½¿ç”¨ SequenceMatcher è®¡ç®—ç›¸ä¼¼åº¦ï¼Œé€‚ç”¨äºæ£€æµ‹é‡å¤æŸ¥è¯¢ã€‚
        
        Args:
            text1: ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²
            text2: ç¬¬äºŒä¸ªå­—ç¬¦ä¸²
        
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0.0-1.0ï¼‰ï¼Œ1.0 è¡¨ç¤ºå®Œå…¨ç›¸åŒ
        """
        if not text1 or not text2:
            return 0.0
        return SequenceMatcher(None, text1.lower().strip(), text2.lower().strip()).ratio()
    
    def _extract_user_input_from_memory(self, memory_text: str) -> Optional[str]:
        """
        ä»è®°å¿†æ–‡æœ¬ä¸­æå–ç”¨æˆ·è¾“å…¥éƒ¨åˆ†
        
        è®°å¿†æ ¼å¼å¯èƒ½æ˜¯ï¼š
        - "[timestamp] ç”¨æˆ·: ... å¥³å¨²: ..."
        - "ç”¨æˆ·: ... å¥³å¨²: ..."
        
        Args:
            memory_text: è®°å¿†æ–‡æœ¬
        
        Returns:
            ç”¨æˆ·è¾“å…¥éƒ¨åˆ†ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å› None
        """
        if not memory_text:
            return None
        
        # ç§»é™¤æ—¶é—´æˆ³å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        text = memory_text
        if text.startswith("[") and "]" in text[:25]:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª ] çš„ä½ç½®
            end_idx = text.find("]", 1)
            if end_idx > 0:
                text = text[end_idx + 1:].strip()
        
        # æŸ¥æ‰¾ "ç”¨æˆ·:" æˆ– "ç”¨æˆ·ï¼š" æ ‡è®°
        user_markers = ["ç”¨æˆ·:", "ç”¨æˆ·ï¼š"]
        for marker in user_markers:
            if marker in text:
                # æå–ç”¨æˆ·è¾“å…¥éƒ¨åˆ†ï¼ˆåˆ° "å¥³å¨²:" æˆ– "å¥³å¨²ï¼š" ä¹‹å‰ï¼‰
                start_idx = text.find(marker)
                user_part = text[start_idx + len(marker):].strip()
                
                # æŸ¥æ‰¾å¥³å¨²å›å¤çš„å¼€å§‹ä½ç½®
                nuwa_markers = ["å¥³å¨²:", "å¥³å¨²ï¼š"]
                for nuwa_marker in nuwa_markers:
                    if nuwa_marker in user_part:
                        end_idx = user_part.find(nuwa_marker)
                        user_part = user_part[:end_idx].strip()
                        break
                
                return user_part
        
        return None
    
    def _migration_fix_timestamps(self, default_timestamp: Optional[datetime] = None) -> int:
        """
        æ•°æ®è¿ç§»è¾…åŠ©æ–¹æ³•ï¼šä¸ºæ—§è®°å¿†ï¼ˆæ²¡æœ‰æ—¶é—´æˆ³å‰ç¼€çš„ï¼‰æ·»åŠ é»˜è®¤æ—¶é—´æˆ³
        
        è¿™ä¸ªæ–¹æ³•ä¼šéå†æ‰€æœ‰è®°å¿†ï¼Œæ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä»¥ [YYYY-MM-DD HH:MM:SS] æ ¼å¼å¼€å¤´ã€‚
        å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨ timestamp å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰æˆ–é»˜è®¤æ—¶é—´æˆ³æ¥æ·»åŠ å‰ç¼€ã€‚
        
        Args:
            default_timestamp: é»˜è®¤æ—¶é—´æˆ³ï¼ˆå¦‚æœè®°å¿†æ²¡æœ‰ timestamp å­—æ®µï¼‰ã€‚å¦‚æœä¸º Noneï¼Œä½¿ç”¨å½“å‰æ—¶é—´ã€‚
        
        Returns:
            int: ä¿®å¤çš„è®°å¿†æ•°é‡
        
        Note:
            è¿™æ˜¯ä¸€ä¸ªå¯é€‰çš„æ•°æ®è¿ç§»æ–¹æ³•ï¼Œç”¨äºå¤„ç†å‡çº§å‰çš„æ—§æ•°æ®ã€‚
            å»ºè®®åœ¨é¦–æ¬¡éƒ¨ç½²æ–°ç‰ˆæœ¬æ—¶è¿è¡Œä¸€æ¬¡ã€‚
        """
        if not LANCEDB_AVAILABLE or self.table is None:
            print("âš ï¸ LanceDB ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œè¿ç§»")
            return 0
        
        if default_timestamp is None:
            default_timestamp = datetime.now()
        
        try:
            # è·å–æ‰€æœ‰è®°å¿†
            df = self.table.to_pandas()
            if df.empty:
                print("ğŸ“ æ²¡æœ‰è®°å¿†éœ€è¦è¿ç§»")
                return 0
            
            fixed_count = 0
            updates = []
            
            # æ£€æŸ¥æ—¶é—´æˆ³å‰ç¼€çš„æ­£åˆ™è¡¨è¾¾å¼
            import re
            timestamp_pattern = re.compile(r'^\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\]')
            
            for idx, row in df.iterrows():
                text = str(row.get("text", ""))
                memory_id = str(row.get("id", ""))
                timestamp_value = row.get("timestamp", 0.0)
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æ—¶é—´æˆ³å‰ç¼€
                if timestamp_pattern.match(text):
                    continue  # å·²ç»æœ‰æ—¶é—´æˆ³å‰ç¼€ï¼Œè·³è¿‡
                
                # ç¡®å®šè¦ä½¿ç”¨çš„æ—¶é—´æˆ³
                if timestamp_value and float(timestamp_value) > 0:
                    memory_datetime = datetime.fromtimestamp(float(timestamp_value))
                else:
                    memory_datetime = default_timestamp
                
                # æ ¼å¼åŒ–æ—¶é—´æˆ³å­—ç¬¦ä¸²
                timestamp_str = memory_datetime.strftime('%Y-%m-%d %H:%M:%S')
                
                # æ„å»ºèåˆåçš„æ–‡æœ¬
                fused_text = f"[{timestamp_str}] {text}"
                
                # é‡æ–°ç”Ÿæˆå‘é‡ï¼ˆä½¿ç”¨èåˆåçš„æ–‡æœ¬ï¼‰
                if self.embedding_model:
                    try:
                        vector = self.embedding_model.encode(fused_text, convert_to_numpy=True)
                        if len(vector) != self.VECTOR_DIM:
                            print(f"âš ï¸ è®°å¿† {memory_id} å‘é‡ç»´åº¦ä¸åŒ¹é…ï¼Œè·³è¿‡")
                            continue
                    except Exception as e:
                        print(f"âš ï¸ è®°å¿† {memory_id} å‘é‡ç”Ÿæˆå¤±è´¥: {e}ï¼Œè·³è¿‡")
                        continue
                else:
                    print(f"âš ï¸ Embedding æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•æ›´æ–°å‘é‡ï¼Œè·³è¿‡è®°å¿† {memory_id}")
                    continue
                
                # å‡†å¤‡æ›´æ–°æ•°æ®
                update_data = {
                    "id": memory_id,
                    "text": fused_text,
                    "vector": vector.tolist(),
                }
                updates.append(update_data)
                fixed_count += 1
            
            # æ‰¹é‡æ›´æ–°ï¼ˆå¦‚æœ LanceDB æ”¯æŒï¼‰
            if updates and self.table:
                # æ³¨æ„ï¼šLanceDB çš„æ›´æ–°æ“ä½œå¯èƒ½éœ€è¦å…ˆåˆ é™¤å†æ’å…¥
                # è¿™é‡Œä½¿ç”¨è¦†ç›–æ–¹å¼ï¼šåˆ é™¤æ—§è®°å½•ï¼Œæ’å…¥æ–°è®°å½•
                ids_to_update = [u["id"] for u in updates]
                
                # åˆ é™¤æ—§è®°å½•
                try:
                    # LanceDB çš„åˆ é™¤æ“ä½œ
                    for memory_id in ids_to_update:
                        self.table.delete(f"id = '{memory_id}'")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤æ—§è®°å½•æ—¶å‡ºé”™: {e}")
                    # å¦‚æœåˆ é™¤å¤±è´¥ï¼Œå°è¯•ç›´æ¥æ’å…¥ï¼ˆå¯èƒ½ä¼šäº§ç”Ÿé‡å¤ï¼Œä½†è‡³å°‘æ•°æ®ä¼šæ›´æ–°ï¼‰
                
                # æ’å…¥æ›´æ–°åçš„è®°å½•
                try:
                    # éœ€è¦è·å–å®Œæ•´çš„è®°å½•æ•°æ®ï¼ˆä¸ä»…ä»…æ˜¯æ›´æ–°çš„å­—æ®µï¼‰
                    full_updates = []
                    for update_data in updates:
                        memory_id = update_data["id"]
                        # ä»åŸå§‹æ•°æ®ä¸­è·å–å…¶ä»–å­—æ®µ
                        original_row = df[df["id"] == memory_id].iloc[0]
                        full_data = {
                            "id": update_data["id"],
                            "text": update_data["text"],
                            "vector": update_data["vector"],
                            "emotion_vector": str(original_row.get("emotion_vector", "")),
                            "timestamp": float(original_row.get("timestamp", 0.0)),
                            "importance": float(original_row.get("importance", 0.5)),
                            "type": str(original_row.get("type", "raw")),
                            "emotions": str(original_row.get("emotions", "{}")),
                            "access_count": int(original_row.get("access_count", 0)),
                        }
                        full_updates.append(full_data)
                    
                    self.table.add(full_updates)
                    print(f"âœ… å·²è¿ç§» {fixed_count} æ¡è®°å¿†ï¼Œæ·»åŠ äº†æ—¶é—´æˆ³å‰ç¼€")
                except Exception as e:
                    print(f"âš ï¸ æ’å…¥æ›´æ–°åçš„è®°å½•æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                    return 0
            
            return fixed_count
            
        except Exception as e:
            print(f"âš ï¸ è¿ç§»è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def recall_by_emotion(
        self,
        query_text: str,
        current_emotion_vector: Optional[np.ndarray] = None,
        top_k: int = 5,
        emotion_weight: float = 0.3,
    ) -> List[Dict[str, Any]]:
        """
        RAG æ£€ç´¢æ¥å£ï¼šä¼˜å…ˆæ£€ç´¢è¯­ä¹‰ç›¸å…³çš„è®°å¿†ï¼Œå¹¶æ ¹æ®æƒ…ç»ªå‘é‡è¿›è¡ŒåŠ æƒ
        
        é€»è¾‘ï¼š
        1. ä½¿ç”¨ query_text ç”ŸæˆæŸ¥è¯¢å‘é‡
        2. åœ¨ LanceDB ä¸­æ£€ç´¢è¯­ä¹‰ç›¸ä¼¼çš„è®°å¿†
        3. å¦‚æœæä¾›äº† current_emotion_vectorï¼Œè®¡ç®—æƒ…ç»ªä¸€è‡´æ€§åˆ†æ•°
        4. ç»¼åˆè¯­ä¹‰ç›¸ä¼¼åº¦å’Œæƒ…ç»ªä¸€è‡´æ€§ï¼Œè¿”å›åŠ æƒåçš„ç»“æœ
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            current_emotion_vector: å½“å‰æƒ…ç»ªå‘é‡ï¼ˆå¯é€‰ï¼Œç”¨äºæƒ…ç»ªä¸€è‡´æ€§åŠ æƒï¼‰
            top_k: è¿”å›çš„è®°å¿†æ•°é‡
            emotion_weight: æƒ…ç»ªä¸€è‡´æ€§æƒé‡ï¼ˆ0.0-1.0ï¼Œé»˜è®¤ 0.3ï¼‰
        
        Returns:
            è®°å¿†åˆ—è¡¨ï¼Œæ¯ä¸ªè®°å¿†åŒ…å«ï¼š
            - text: è®°å¿†æ–‡æœ¬
            - similarity: ç»¼åˆç›¸ä¼¼åº¦åˆ†æ•° (0.0-1.0)
            - semantic_similarity: è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°
            - emotion_similarity: æƒ…ç»ªä¸€è‡´æ€§åˆ†æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
            - metadata: å…ƒæ•°æ®
        """
        if not LANCEDB_AVAILABLE or self.db is None or self.table is None:
            return []
        
        if not self.embedding_model:
            print("âš ï¸ Embedding æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•æ£€ç´¢è®°å¿†")
            return []
        
        try:
            # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = self.embedding_model.encode(query_text, convert_to_numpy=True)
            
            # ç¡®ä¿å‘é‡ç»´åº¦æ­£ç¡®
            if len(query_vector) != self.VECTOR_DIM:
                print(f"âš ï¸ æŸ¥è¯¢å‘é‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.VECTOR_DIM}ï¼Œå®é™… {len(query_vector)}")
                return []
            
            # 2. åœ¨ LanceDB ä¸­æ£€ç´¢è¯­ä¹‰ç›¸ä¼¼çš„è®°å¿†
            # ä½¿ç”¨å‘é‡æœç´¢ï¼Œæ˜ç¡®æŒ‡å®šä½¿ç”¨ "vector" åˆ—
            # è¿”å› top_k * 2 ä¸ªç»“æœï¼ˆåç»­ä¼šåŠ æƒç­›é€‰ï¼‰
            results = self.table.search(
                query_vector.tolist(),
                vector_column_name="vector"
            ).limit(top_k * 2).to_pandas()
            
            if results.empty:
                return []
            
            # 3. è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦
            memories = []
            access_updates: List[Dict[str, Any]] = []
            current_ts = time.time()
            for _, row in results.iterrows():
                memory_text = row.get("text", "")
                memory_vector = row.get("vector", [])
                
                # æ£€æŸ¥ memory_text å’Œ memory_vector æ˜¯å¦æœ‰æ•ˆ
                if not memory_text:
                    continue
                if memory_vector is None or (isinstance(memory_vector, (list, np.ndarray)) and len(memory_vector) == 0):
                    continue
                
                # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
                if isinstance(memory_vector, list):
                    memory_vec = np.array(memory_vector, dtype=np.float32)
                else:
                    memory_vec = np.array(memory_vector, dtype=np.float32)
                
                # å½’ä¸€åŒ–å‘é‡
                query_norm = np.linalg.norm(query_vector)
                memory_norm = np.linalg.norm(memory_vec)
                
                if query_norm > 0 and memory_norm > 0:
                    semantic_similarity = float(np.dot(query_vector, memory_vec) / (query_norm * memory_norm))
                    # å°†ç›¸ä¼¼åº¦ä» [-1, 1] æ˜ å°„åˆ° [0, 1]
                    semantic_similarity = (semantic_similarity + 1.0) / 2.0
                else:
                    semantic_similarity = 0.0
                
                # è®¡ç®—æƒ…ç»ªä¸€è‡´æ€§åˆ†æ•°ï¼ˆå¦‚æœæœ‰æƒ…ç»ªå‘é‡ï¼‰
                emotion_similarity = 1.0  # é»˜è®¤å€¼ï¼ˆæ— æƒ…ç»ªå‘é‡æ—¶ä¸å½±å“ç»“æœï¼‰
                if current_emotion_vector is not None and NUMPY_AVAILABLE:
                    # æ£€æŸ¥è®°å¿†æ˜¯å¦æœ‰æƒ…ç»ªå‘é‡å…ƒæ•°æ®
                    memory_emotion_str = row.get("emotion_vector", "")
                    if memory_emotion_str:
                        try:
                            # emotion_vector å­˜å‚¨ä¸º JSON å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
                            memory_emotion_vec = np.array(json.loads(memory_emotion_str), dtype=np.float32)
                            
                            # è®¡ç®—æƒ…ç»ªå‘é‡ç›¸ä¼¼åº¦
                            emotion_norm = np.linalg.norm(current_emotion_vector)
                            memory_emotion_norm = np.linalg.norm(memory_emotion_vec)
                            
                            if emotion_norm > 0 and memory_emotion_norm > 0:
                                emotion_similarity = float(
                                    np.dot(current_emotion_vector, memory_emotion_vec) / 
                                    (emotion_norm * memory_emotion_norm)
                                )
                                # å°†ç›¸ä¼¼åº¦ä» [-1, 1] æ˜ å°„åˆ° [0, 1]
                                emotion_similarity = (emotion_similarity + 1.0) / 2.0
                            else:
                                emotion_similarity = 0.5  # ä¸­æ€§å€¼
                        except (json.JSONDecodeError, ValueError, TypeError) as e:
                            # JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                            emotion_similarity = 1.0
                
                # ç»¼åˆç›¸ä¼¼åº¦ = (1 - emotion_weight) * è¯­ä¹‰ç›¸ä¼¼åº¦ + emotion_weight * æƒ…ç»ªç›¸ä¼¼åº¦
                combined_similarity = (1.0 - emotion_weight) * semantic_similarity + emotion_weight * emotion_similarity
                
                # è¿‡æ»¤é‡å¤æŸ¥è¯¢ï¼šå¦‚æœè®°å¿†ä¸­çš„ç”¨æˆ·è¾“å…¥ä¸å½“å‰æŸ¥è¯¢é«˜åº¦ç›¸ä¼¼ï¼Œé™ä½å…¶åˆ†æ•°
                # è¿™æ ·å¯ä»¥é¿å…æ£€ç´¢åˆ°ç”¨æˆ·ä¹‹å‰é—®è¿‡çš„ç›¸åŒé—®é¢˜ï¼Œè€Œæ˜¯æ£€ç´¢åˆ°å®é™…çš„ç­”æ¡ˆ
                user_input_from_memory = self._extract_user_input_from_memory(memory_text)
                if user_input_from_memory:
                    query_similarity = self._calculate_string_similarity(query_text, user_input_from_memory)
                    
                    # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼ˆ0.75ï¼‰ï¼Œè¯´æ˜è¿™æ˜¯é‡å¤æŸ¥è¯¢
                    # é™ä½å…¶ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œä½¿å…¶æ’åœ¨åé¢
                    if query_similarity > 0.75:
                        # é™ä½ç›¸ä¼¼åº¦ï¼šç›¸ä¼¼åº¦è¶Šé«˜ï¼Œé™ä½è¶Šå¤š
                        # ä¾‹å¦‚ï¼š0.9 ç›¸ä¼¼åº¦ -> é™ä½åˆ°åŸæ¥çš„ 10%ï¼Œ0.8 ç›¸ä¼¼åº¦ -> é™ä½åˆ°åŸæ¥çš„ 20%
                        # ä½¿ç”¨ max(0.05, ...) ç¡®ä¿è‡³å°‘ä¿ç•™ 5% çš„åˆ†æ•°ï¼Œé¿å…å®Œå…¨æ’é™¤
                        penalty_factor = max(0.05, 1.0 - query_similarity)  # 0.05 åˆ° 0.25
                        original_similarity = combined_similarity
                        combined_similarity = combined_similarity * penalty_factor
                        print(f"ğŸš« [Memory][FILTER] æ£€æµ‹åˆ°é‡å¤æŸ¥è¯¢ï¼ˆç›¸ä¼¼åº¦={query_similarity:.2f}ï¼‰ï¼Œé™ä½è®°å¿†åˆ†æ•°: {original_similarity:.3f} -> {combined_similarity:.3f}")
                        print(f"   æŸ¥è¯¢: {query_text[:60]}...")
                        print(f"   è®°å¿†ä¸­çš„ç”¨æˆ·è¾“å…¥: {user_input_from_memory[:60]}...")
                
                # æå–å…¶ä»–å…ƒæ•°æ®
                timestamp_value = float(row.get("timestamp", 0.0) or 0.0)
                current_access = int(row.get("access_count", 0) or 0)
                metadata = {
                    "id": row.get("id", ""),
                    "timestamp": timestamp_value,
                    "timestamp_human": self._format_timestamp(timestamp_value),
                    "age_seconds": max(0.0, current_ts - timestamp_value),
                    "importance": row.get("importance", 0.5),
                    "type": row.get("type", "raw"),
                    "emotions": row.get("emotions", ""),
                    "access_count": current_access,
                }
                
                memories.append({
                    "text": memory_text,
                    "similarity": combined_similarity,
                    "semantic_similarity": semantic_similarity,
                    "emotion_similarity": emotion_similarity if current_emotion_vector is not None else None,
                    "metadata": metadata,
                })
                access_updates.append({"id": metadata["id"], "new_value": current_access + 1})
            
            # æ›´æ–°è®¿é—®è®¡æ•°
            if access_updates:
                self._increment_access_counts(access_updates)
            
            # 4. æŒ‰ç»¼åˆç›¸ä¼¼åº¦æ’åºï¼Œè¿”å› top_k
            memories.sort(key=lambda x: x["similarity"], reverse=True)
            top = memories[:top_k]

            # ç»ˆç«¯è°ƒè¯•è¾“å‡ºï¼šå‘é‡è¯»å– / æ£€ç´¢ç»“æœæ‘˜è¦
            if top:
                best = top[0]
                preview = best["text"].replace("\n", " ")[:80]
                print(
                    f"ğŸ” [Memory][READ] query='{query_text[:40]}' "
                    f"-> {len(top)} æ¡ï¼Œæœ€é«˜ç›¸ä¼¼åº¦={best['similarity']:.3f}ï¼Œç¤ºä¾‹: {preview}..."
                )
                # è¯¦ç»†è¾“å‡ºå‰3æ¡è®°å¿†ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                for i, mem in enumerate(top[:3], 1):
                    mem_text = mem.get("text", "")
                    mem_sim = mem.get("similarity", 0.0)
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´æˆ³å‰ç¼€
                    has_timestamp = mem_text.startswith("[") and "]" in mem_text[:20]
                    timestamp_marker = "â°" if has_timestamp else "  "
                    print(f"   {timestamp_marker} [{i}] ç›¸ä¼¼åº¦={mem_sim:.3f}: {mem_text[:60]}...")
            else:
                print(f"ğŸ” [Memory][READ] query='{query_text[:40]}' -> æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")

            return top
        except Exception as e:
            print(f"âš ï¸ è®°å¿†æ£€ç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _format_timestamp(self, ts: float) -> str:
        """å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æœ¬åœ°æ—¶é—´æè¿°"""
        if not ts:
            return "(æœªçŸ¥æ—¶é—´)"
        try:
            dt = datetime.fromtimestamp(float(ts)).astimezone()
            return dt.strftime("%Y-%m-%d %H:%M:%S %Z%z")
        except Exception:
            return "(æ—¶é—´è§£æå¤±è´¥)"

    def _increment_access_counts(self, updates: List[Dict[str, Any]]):
        """å°†æŒ‡å®šè®°å¿†çš„è®¿é—®æ¬¡æ•° +1"""
        if not updates or not self.table:
            return
        for item in updates:
            mem_id = item.get("id")
            new_value = item.get("new_value")
            if not mem_id:
                continue
            try:
                self.table.update(
                    where=f"id == '{mem_id}'",
                    values={"access_count": int(new_value)},
                )
            except Exception:
                try:
                    row_df = self.table.to_pandas()
                    row = row_df[row_df["id"] == mem_id]
                    if not row.empty:
                        fallback_value = int(row.iloc[0].get("access_count", 0)) + 1
                        self.table.update(
                            where=f"id == '{mem_id}'",
                            values={"access_count": fallback_value},
                        )
                except Exception:
                    continue

    def get_recent_memories(self, limit: int = 1000, memory_type: str = "raw") -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„è®°å¿†åˆ—è¡¨"""
        if not self.table:
            return []
        try:
            df = self.table.to_pandas()
            if memory_type:
                df = df[df["type"] == memory_type]
            df = df.sort_values("timestamp", ascending=False).head(limit)
            return df.to_dict(orient="records")
        except Exception as e:
            print(f"âš ï¸ è·å–è®°å¿†å¤±è´¥: {e}")
            return []

    def delete_memories(self, memory_ids: List[str]):
        """åˆ é™¤æŒ‡å®š ID çš„è®°å¿†"""
        if not memory_ids or not self.table:
            return
        for mem_id in memory_ids:
            try:
                self.table.delete(where=f"id == '{mem_id}'")
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤è®°å¿† {mem_id} å¤±è´¥: {e}")
