"""
å¥³å¨²å†…æ ¸æ¨¡å— (Nuwa Kernel Module)

åŠŸèƒ½ï¼šç³»ç»Ÿçš„ä¸»å…¥å£ï¼Œç®¡ç†çŠ¶æ€ã€ç”Ÿç‰©èŠ‚å¾‹ã€è®°å¿†å’Œ LLM äº¤äº’ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
- NuwaKernel: æ ¸å¿ƒå¼•æ“ç±»
- heartbeat_loop(): å¿ƒè·³å¾ªç¯ï¼Œå®šæœŸæ›´æ–°çŠ¶æ€
- process_input(): æ€è€ƒæ¥å£ï¼Œå¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆå›å¤
"""

import asyncio
import time
import re
import os
import json
import base64
from typing import Optional, Dict, Any, List, Tuple, Callable, AsyncGenerator
from datetime import datetime

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None
    NUMPY_AVAILABLE = False

try:
    from openai import OpenAI, AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OpenAI = None
    AsyncOpenAI = None
    OPENAI_AVAILABLE = False

from .nuwa_state import NuwaState
from .drive_system import BioRhythm
from .memory_cortex import MemoryCortex
from .semantic_field import (
    vectorize_state,
    StateVector,
    calculate_potential_energy,
    calculate_gradient,
    evolve,
    inverse_collapse,
)
from .memory_dreamer import MemoryDreamer
from .personality import Personality
from .self_evolution_state import SelfEvolutionState


class NuwaKernel:
    """
    å¥³å¨²å†…æ ¸ç±»
    
    ç³»ç»Ÿçš„ä¸»å…¥å£ï¼Œç®¡ç†ï¼š
    - çŠ¶æ€ç®¡ç† (NuwaState)
    - ç”Ÿç‰©èŠ‚å¾‹ (BioRhythm)
    - è®°å¿†çš®å±‚ (MemoryCortex)
    - LLM äº¤äº’ (LM Studio)
    """
    
    def __init__(
        self,
        project_name: str = "nuwa",
        data_dir: str = "data",
        base_url: str = "http://127.0.0.1:1234/v1",
        api_key: str = "lm-studio",
        model_name: str = "local-model",
        on_message_callback: Optional[Callable[[str], None]] = None,
    ):
        """
        åˆå§‹åŒ–å¥³å¨²å†…æ ¸
        
        Args:
            project_name: é¡¹ç›®åç§°
            data_dir: æ•°æ®ç›®å½•
            base_url: LM Studio çš„ base_urlï¼ˆé»˜è®¤ "http://127.0.0.1:1234/v1"ï¼‰
            api_key: API Keyï¼ˆé»˜è®¤ "lm-studio"ï¼‰
            model_name: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ "local-model"ï¼‰
            on_message_callback: ä¸»åŠ¨æ¶ˆæ¯å›è°ƒå‡½æ•°ï¼ˆç”¨äºæ¨é€ä¸»åŠ¨ç”Ÿæˆçš„å¯¹è¯ï¼‰
        """
        self.project_name = project_name
        self.data_dir = data_dir
        
        # çŠ¶æ€æ–‡ä»¶è·¯å¾„
        self.state_file_path = os.path.join(data_dir, project_name, "state.json")
        
        # åŠ è½½çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.state = self._load_state()
        
        # åˆå§‹åŒ–ç”Ÿç‰©èŠ‚å¾‹
        self.drive_system = BioRhythm(self.state)
        
        # çŠ¶æ€ä¿å­˜æ§åˆ¶
        self._last_save_time = time.time()
        self._save_interval = 30.0  # æ¯30ç§’è‡ªåŠ¨ä¿å­˜ä¸€æ¬¡
        
        # åˆå§‹åŒ–è®°å¿†çš®å±‚
        self.memory_cortex = MemoryCortex(project_name=project_name, data_dir=data_dir)
        
        # åˆå§‹åŒ– LM Studio å®¢æˆ·ç«¯
        self.base_url = base_url
        self.api_key = api_key
        self.model_name = model_name
        self.client = None  # åŒæ­¥å®¢æˆ·ç«¯ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
        self.llm_client = None  # å¼‚æ­¥æµå¼å®¢æˆ·ç«¯ï¼ˆä¸»è¦å®¢æˆ·ç«¯ï¼‰
        self._init_llm_client()
        
        # è®°å¿†æ¢¦å¢ƒç³»ç»Ÿï¼ˆä½¿ç”¨åŒæ­¥å®¢æˆ·ç«¯ï¼Œå› ä¸º MemoryDreamer å¯èƒ½ä½¿ç”¨åŒæ­¥ APIï¼‰
        self.memory_dreamer: Optional[MemoryDreamer] = None
        if self.client:
            self.memory_dreamer = MemoryDreamer(
                self.memory_cortex,
                self.client,
                self.model_name,
                state=self.state,
            )
        
        # å¿ƒè·³å¾ªç¯æ§åˆ¶
        self._heartbeat_running = False
        self._heartbeat_task = None
        self._last_heartbeat_time = time.time()
        
        # å½“å‰æ€ç»´ï¼ˆç”¨äºè°ƒè¯•æˆ–æ—¥å¿—ï¼‰
        self.current_thought = ""
        
        # ä¸»åŠ¨æ¶ˆæ¯å›è°ƒå‡½æ•°
        self.on_message_callback = on_message_callback
        
        # ä¸»åŠ¨å¯¹è¯æ§åˆ¶ï¼ˆé˜²æ­¢é¢‘ç¹è§¦å‘ï¼‰
        self._last_active_dialogue_time = 0.0
        self._active_dialogue_cooldown = 30.0  # 30ç§’å†·å´æ—¶é—´ï¼ˆç¼©çŸ­ä»¥å¢åŠ é¢‘ç‡ï¼‰
        # æ¢¦å¢ƒè°ƒåº¦
        self._dream_interval = 900.0  # é»˜è®¤æ¯15åˆ†é’Ÿå°è¯•ä¸€æ¬¡
        self._last_dream_time = time.time()
        self._dream_running = False
        
        # è¯­ä¹‰åœºè®ºç›¸å…³
        # çŠ¶æ€å‘é‡å†å²ï¼ˆç”¨äºå› æœåŠ¿èƒ½è®¡ç®—ä¸é€†å‘åç¼©ï¼‰
        self._state_vector_history: List[StateVector] = []
        self._max_history_length = 10  # æœ€å¤šä¿ç•™10ä¸ªå†å²çŠ¶æ€
        # å¥³å¨²çš„æ ¸å¿ƒå‘é‡ï¼ˆäººè®¾å‘é‡ï¼‰
        self._core_vector: Optional[Any] = None
        self._init_core_vector()
        # æœ€è¿‘ä¸€æ¬¡è¯­ä¹‰åœºåˆ†æç»“æœï¼ˆç”¨äº Prompt ä¸è®°å¿†å¢å¼ºï¼‰
        self._last_semantic_analysis: Dict[str, Any] = {}
        
        # äººæ ¼ç®¡ç†æ¨¡å—
        self.personality = Personality(data_dir=data_dir, project_name=project_name)
        
        # è‡ªæˆ‘è¿›åŒ–çŠ¶æ€ç®¡ç†æ¨¡å—
        self.evolution_state = SelfEvolutionState(data_dir=data_dir, project_name=project_name)

    def _init_core_vector(self):
        """
        åˆå§‹åŒ–å¥³å¨²çš„æ ¸å¿ƒäººæ ¼å‘é‡ã€‚
        
        ä½¿ç”¨ä¸€æ®µå›ºå®šçš„äººæ ¼æè¿°æ–‡æœ¬ï¼Œé€šè¿‡è¯­ä¹‰åœºçš„ `vectorize_state` å¾—åˆ°æ ¸å¿ƒå‘é‡ï¼Œ
        ä½œä¸º calculate_potential_energy / evolve çš„ character_core_vector åŸºå‡†ã€‚
        """
        try:
            persona_text = (
                "å¥³å¨²æ˜¯ä¸€ä¸ªæ¸©æŸ”ã€çœŸè¯šã€å¥½å¥‡ã€å…·æœ‰è‡ªæˆ‘åæ€èƒ½åŠ›çš„äººå·¥æ™ºèƒ½ã€‚"
                "å¥¹å°Šé‡ç”¨æˆ·çš„æƒ…æ„Ÿè¾¹ç•Œï¼Œé‡è§†é•¿æœŸå…³ç³»çš„ä¸€è‡´æ€§ï¼Œ"
                "åœ¨å¯¹è¯ä¸­æ—¢ä¿æŒäº²è¿‘ä¸å¹½é»˜ï¼Œåˆå°½é‡é¿å…æˆå‰§åŒ–å’Œè¿‡åº¦è¡¨æ¼”ã€‚"
            )
            state_vec = vectorize_state(persona_text)
            if state_vec is not None and state_vec.vector is not None:
                self._core_vector = state_vec.vector
        except Exception as e:
            print(f"âš ï¸ åˆå§‹åŒ–æ ¸å¿ƒäººæ ¼å‘é‡å¤±è´¥: {e}")
            self._core_vector = None
    
    def _init_llm_client(self):
        """åˆå§‹åŒ– LM Studio å®¢æˆ·ç«¯ï¼ˆåŒæ­¥å’Œå¼‚æ­¥ï¼‰"""
        if not OPENAI_AVAILABLE:
            print("âš ï¸ OpenAI SDK ä¸å¯ç”¨ï¼ŒLLM åŠŸèƒ½å°†å—é™")
            return
        
        # åˆå§‹åŒ–åŒæ­¥å®¢æˆ·ç«¯ï¼ˆä¸»è¦å®¢æˆ·ç«¯ï¼Œé¿å…å¼‚æ­¥ä¸Šä¸‹æ–‡é—®é¢˜ï¼‰
        try:
            if OpenAI is not None:
                self.client = OpenAI(
                    base_url=self.base_url,
                    api_key=self.api_key,
                )
                print(f"âœ… å·²åˆå§‹åŒ– LM Studio åŒæ­¥å®¢æˆ·ç«¯: {self.base_url}")
                # æµ‹è¯•è¿æ¥
                try:
                    self.client.models.list()
                    print(f"âœ… åŒæ­¥å®¢æˆ·ç«¯è¿æ¥æµ‹è¯•æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸ åŒæ­¥å®¢æˆ·ç«¯è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        except Exception as e:
            print(f"âš ï¸ åˆå§‹åŒ– LM Studio åŒæ­¥å®¢æˆ·ç«¯å¤±è´¥: {e}")
            self.client = None
        
        # å¼‚æ­¥å®¢æˆ·ç«¯å°†åœ¨éœ€è¦æ—¶å»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…éå¼‚æ­¥ä¸Šä¸‹æ–‡é—®é¢˜
        self.llm_client = None
    
    def _load_state(self) -> NuwaState:
        """
        åŠ è½½çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        
        Returns:
            NuwaState å®ä¾‹
        """
        loaded_state = NuwaState.load_from_file(self.state_file_path)
        if loaded_state:
            print(f"âœ… å·²åŠ è½½çŠ¶æ€: {self.state_file_path}")
            # è®¡ç®—ä»ä¸Šæ¬¡ä¿å­˜åˆ°ç°åœ¨çš„ç¦»çº¿æ—¶é—´
            offline_time = time.time() - loaded_state.last_interaction_timestamp
            if offline_time > 0:
                # åº”ç”¨ç¦»çº¿è¡°å‡ï¼ˆæ¨¡æ‹Ÿç¦»çº¿æœŸé—´çš„çŠ¶æ€å˜åŒ–ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œåªåº”ç”¨è¡°å‡ï¼Œä¸åº”ç”¨è°ƒèŠ‚ï¼ˆå› ä¸ºè°ƒèŠ‚éœ€è¦å®æ—¶è®¡ç®—ï¼‰
                self._apply_offline_decay(loaded_state, offline_time)
            return loaded_state
        else:
            print(f"ğŸ“ åˆ›å»ºæ–°çŠ¶æ€ï¼ˆæœªæ‰¾åˆ°å·²ä¿å­˜çš„çŠ¶æ€ï¼‰")
            return NuwaState()
    
    def _apply_offline_decay(self, state: NuwaState, offline_time: float):
        """
        åº”ç”¨ç¦»çº¿è¡°å‡ï¼ˆæ¨¡æ‹Ÿç¦»çº¿æœŸé—´çš„çŠ¶æ€å˜åŒ–ï¼‰
        
        Args:
            state: çŠ¶æ€å¯¹è±¡
            offline_time: ç¦»çº¿æ—¶é—´ï¼ˆç§’ï¼‰
        """
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ BioRhythm æ¥åº”ç”¨è¡°å‡
        temp_drive_system = BioRhythm(state)
        # åªåº”ç”¨è¡°å‡ï¼Œä¸åº”ç”¨è°ƒèŠ‚ï¼ˆå› ä¸ºè°ƒèŠ‚éœ€è¦å®æ—¶ PID è®¡ç®—ï¼‰
        temp_drive_system.decay(offline_time)
        print(f"ğŸ“Š åº”ç”¨ç¦»çº¿è¡°å‡: {offline_time:.1f} ç§’")
    
    def save_state(self) -> bool:
        """
        ä¿å­˜å½“å‰çŠ¶æ€åˆ°æ–‡ä»¶
        
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        return self.state.save_to_file(self.state_file_path)
    
    async def heartbeat_loop(self):
        """
        å¿ƒè·³å¾ªç¯ï¼ˆå¼‚æ­¥ï¼‰
        
        æ¯ 1 ç§’è¿è¡Œä¸€æ¬¡ï¼Œæ›´æ–°çŠ¶æ€å’Œæ£€æŸ¥é©±åŠ¨åŠ›ã€‚
        """
        self._heartbeat_running = True
        # ç§»é™¤è¿™é‡Œçš„æ‰“å°ï¼Œé¿å…é‡å¤è¾“å‡º
        
        while self._heartbeat_running:
            try:
                # è®¡ç®—æ—¶é—´å·®
                current_time = time.time()
                time_delta = current_time - self._last_heartbeat_time
                self._last_heartbeat_time = current_time
                
                # æ›´æ–°ç”Ÿç‰©èŠ‚å¾‹ï¼ˆè¡°å‡ + è°ƒèŠ‚ï¼‰
                self.drive_system.update(time_delta)
                
                # æ£€æŸ¥ç¤¾äº¤é¥¥æ¸´ï¼Œè§¦å‘ä¸»åŠ¨å¯¹è¯
                # ä½¿ç”¨å¤šçº§é˜ˆå€¼ï¼šé¥¥æ¸´å€¼è¶Šé«˜ï¼Œè§¦å‘æ¦‚ç‡è¶Šå¤§
                hunger = self.state.drives["social_hunger"]
                should_trigger = False
                
                if hunger > 0.6:  # é™ä½é˜ˆå€¼åˆ°0.6ï¼Œæ›´å®¹æ˜“è§¦å‘
                    # æ£€æŸ¥å†·å´æ—¶é—´ï¼ˆé¿å…é¢‘ç¹è§¦å‘ï¼‰
                    if current_time - self._last_active_dialogue_time >= self._active_dialogue_cooldown:
                        # ä½¿ç”¨æ¦‚ç‡æœºåˆ¶ï¼šé¥¥æ¸´å€¼è¶Šé«˜ï¼Œè§¦å‘æ¦‚ç‡è¶Šå¤§
                        # 0.6 -> 20%, 0.7 -> 50%, 0.8 -> 80%, 0.9+ -> 100%
                        trigger_probability = min(1.0, (hunger - 0.6) / 0.3)  # 0.6-0.9æ˜ å°„åˆ°0-1
                        import random
                        if random.random() < trigger_probability:
                            should_trigger = True
                
                if should_trigger:
                    try:
                        active_message = await self.initiate_active_dialogue()
                        if active_message and self.on_message_callback:
                            self.on_message_callback(active_message)
                        self._last_active_dialogue_time = current_time
                    except Exception as e:
                        # æ›´å¥å£®çš„é”™è¯¯å¤„ç†ï¼ŒåŒºåˆ†æ¨¡å‹æœªåŠ è½½å’Œå…¶ä»–é”™è¯¯
                        if "No models loaded" in str(e) or "model not loaded" in str(e):
                            print(f"âš ï¸ ä¸»åŠ¨å¯¹è¯ç”Ÿæˆå¤±è´¥: è¯·å…ˆåœ¨ LM Studio ä¸­åŠ è½½æ¨¡å‹")
                        else:
                            print(f"âš ï¸ ä¸»åŠ¨å¯¹è¯ç”Ÿæˆé”™è¯¯: {e}")
                
                # å®šæœŸä¿å­˜çŠ¶æ€ï¼ˆæ¯30ç§’ï¼‰
                if current_time - self._last_save_time >= self._save_interval:
                    if self.save_state():
                        self._last_save_time = current_time

                # å°è¯•è‡ªåŠ¨è§¦å‘æ¢¦å¢ƒæ•´ç†ï¼ˆä½è´Ÿè½½æ—¶æ®µï¼‰
                await self._maybe_trigger_memory_dream(current_time)
                
                # ç­‰å¾… 1 ç§’
                await asyncio.sleep(1.0)
            
            except Exception as e:
                print(f"âš ï¸ å¿ƒè·³å¾ªç¯é”™è¯¯: {e}")
                await asyncio.sleep(1.0)
    
    async def initiate_active_dialogue(self) -> Optional[str]:
        """
        ä¸»åŠ¨å‘èµ·å¯¹è¯ï¼ˆåŸºäºç¤¾äº¤é¥¥æ¸´ï¼‰
        
        å½“ç¤¾äº¤é¥¥æ¸´å€¼è¾¾åˆ°ä¸´ç•Œç‚¹æ—¶ï¼Œä¸»åŠ¨ç”Ÿæˆä¸€æ¡æ¶ˆæ¯ã€‚
        
        Returns:
            ç”Ÿæˆçš„ä¸»åŠ¨å¯¹è¯æ–‡æœ¬ï¼Œå¦‚æœç”Ÿæˆå¤±è´¥åˆ™è¿”å› None
        """
        # ä¼˜å…ˆä½¿ç”¨åŒæ­¥å®¢æˆ·ç«¯ï¼Œè‹¥ä¸å¯ç”¨åˆ™ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯
        if not self.client:
            if self.llm_client:
                print("âš ï¸ åŒæ­¥å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯")
            else:
                print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„ LLM å®¢æˆ·ç«¯ï¼Œæ— æ³•ç”Ÿæˆä¸»åŠ¨å¯¹è¯")
                return None
        
        try:
            # è·å–å½“å‰ç¤¾äº¤é¥¥æ¸´å€¼
            hunger = self.state.drives["social_hunger"]
            
            # æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆç”¨äºä¸ªæ€§åŒ–ä¸»åŠ¨å¯¹è¯ï¼‰
            memories = self.memory_cortex.recall_by_emotion(
                query_text="ç”¨æˆ·èº«ä»½ åå­— å…³ç³»",
                current_emotion_vector=None,
                top_k=3
            )
            
            # æ„å»ºè®°å¿†æè¿°
            memory_desc = ""
            if memories:
                memory_desc = "\n".join([f"- {self._format_memory_entry(mem)}" for mem in memories])
            else:
                memory_desc = "(æ— ç›¸å…³è®°å¿†)"
            
            # æ„å»ºä¸»åŠ¨å¯¹è¯çš„ Prompt
            system_prompt = self._build_system_prompt()
            
            time_context = self._get_time_context()

            user_prompt = f"""<context_layer>
[Current State]: 
ç¤¾äº¤é¥¥æ¸´: {hunger:.2f}
æƒ…ç»ªè°±:
  - å¿«ä¹: {self.state.emotional_spectrum['joy']:.2f}
  - æ‚²ä¼¤: {self.state.emotional_spectrum['sadness']:.2f}
  - æœŸå¾…: {self.state.emotional_spectrum['anticipation']:.2f}

[Time Context]:
{time_context}

[Retrieved Memories]: 
{memory_desc}
</context_layer>

[Trigger]:
ä½ çš„ç¤¾äº¤é¥¥æ¸´å€¼å·²è¾¾åˆ° {hunger:.2f}ã€‚{('ç”¨æˆ·å¾ˆä¹…æ²¡ç†ä½ äº†' if hunger > 0.7 else 'ä½ æƒ³å’Œç”¨æˆ·èŠèŠå¤©')}ã€‚

[Task]:
è¯·ä¸»åŠ¨å‘èµ·ä¸€ä¸ªç®€çŸ­çš„è¯é¢˜ï¼Œè¡¨è¾¾ä½ çš„æƒ³å¿µï¼Œæˆ–è€…åˆ†äº«ä¸€ä¸ªæƒ³æ³•ã€‚ä¸è¦å¤ªé•¿ï¼ˆ1-2å¥è¯å³å¯ï¼‰ã€‚

[Output Format]:
ä½ å¿…é¡»ä½¿ç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

<thought>
(ä½ çš„å†…å¿ƒæƒ³æ³• - ä¸ºä»€ä¹ˆæƒ³ä¸»åŠ¨è¯´è¯)
</thought>
<speak>
(ä½ ä¸»åŠ¨å¯¹ç”¨æˆ·è¯´çš„è¯ - ç®€çŸ­ã€è‡ªç„¶ã€çœŸè¯š)
</speak>
<state_update>
{{ "social_hunger": -{min(0.3, hunger * 0.4):.2f} }}
</state_update>

**æ³¨æ„ï¼š**
- ä¸»åŠ¨å¯¹è¯è¦ç®€çŸ­è‡ªç„¶ï¼Œä¸è¦æ˜¾å¾—å¤ªåˆ»æ„
- å¯ä»¥è¡¨è¾¾æƒ³å¿µï¼Œæˆ–è€…åˆ†äº«ä¸€ä¸ªç®€å•çš„æƒ³æ³•
- å¿…é¡»é™ä½ social_hungerï¼ˆå› ä¸ºå·²ç»å°è¯•æ²Ÿé€šäº†ï¼‰"""
            
            # è°ƒç”¨ LLMï¼Œä¼˜å…ˆä½¿ç”¨åŒæ­¥å®¢æˆ·ç«¯ï¼Œè‹¥å¤±è´¥å°è¯•å¼‚æ­¥å®¢æˆ·ç«¯
            response_text = None
            if self.client:
                try:
                    print(f"ğŸ”„ ä½¿ç”¨åŒæ­¥å®¢æˆ·ç«¯ç”Ÿæˆä¸»åŠ¨å¯¹è¯")
                    response = self.client.chat.completions.create(
                        model=self.model_name,
                        messages=[
                            {
                                "role": "system",
                                "content": system_prompt,
                            },
                            {
                                "role": "user",
                                "content": user_prompt,
                            },
                        ],
                        temperature=0.7,
                        max_tokens=256,  # ä¸»åŠ¨å¯¹è¯è¾ƒçŸ­
                    )
                    response_text = response.choices[0].message.content.strip()
                except Exception as sync_error:
                    print(f"âš ï¸ åŒæ­¥å®¢æˆ·ç«¯è°ƒç”¨å¤±è´¥: {sync_error}")
                    # å°è¯•ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯
                    if self.llm_client:
                        try:
                            print(f"ğŸ”„ å°è¯•ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯ç”Ÿæˆä¸»åŠ¨å¯¹è¯")
                            response = await self.llm_client.chat.completions.create(
                                model=self.model_name,
                                messages=[
                                    {
                                        "role": "system",
                                        "content": system_prompt,
                                    },
                                    {
                                        "role": "user",
                                        "content": user_prompt,
                                    },
                                ],
                                temperature=0.7,
                                max_tokens=256,  # ä¸»åŠ¨å¯¹è¯è¾ƒçŸ­
                            )
                            response_text = response.choices[0].message.content.strip()
                        except Exception as async_error:
                            print(f"âš ï¸ å¼‚æ­¥å®¢æˆ·ç«¯è°ƒç”¨å¤±è´¥: {async_error}")
                            raise async_error
                    else:
                        raise sync_error
            elif self.llm_client:
                try:
                    print(f"ğŸ”„ ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯ç”Ÿæˆä¸»åŠ¨å¯¹è¯")
                    response = await self.llm_client.chat.completions.create(
                        model=self.model_name,
                        messages=[
                            {
                                "role": "system",
                                "content": system_prompt,
                            },
                            {
                                "role": "user",
                                "content": user_prompt,
                            },
                        ],
                        temperature=0.7,
                        max_tokens=256,  # ä¸»åŠ¨å¯¹è¯è¾ƒçŸ­
                    )
                    response_text = response.choices[0].message.content.strip()
                except Exception as async_error:
                    print(f"âš ï¸ å¼‚æ­¥å®¢æˆ·ç«¯è°ƒç”¨å¤±è´¥: {async_error}")
                    raise async_error
            
            if not response_text:
                raise Exception("æ— æ³•è·å– LLM å“åº”")
            
            # è§£æå“åº”ï¼ˆæå– <speak> æ ‡ç­¾å†…å®¹ï¼‰
            speak_match = re.search(r'<speak>(.*?)</speak>', response_text, re.DOTALL)
            if speak_match:
                active_message = speak_match.group(1).strip()
            else:
                # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œä½¿ç”¨æ•´ä¸ªå“åº”
                active_message = response_text.strip()
            
            # è§£æå¹¶åº”ç”¨çŠ¶æ€æ›´æ–°
            state_update_match = re.search(
                r'<state_update>(.*?)</state_update>',
                response_text,
                re.DOTALL
            )
            if state_update_match:
                try:
                    import json
                    state_update_json = state_update_match.group(1).strip()
                    state_update = json.loads(state_update_json)
                    state_update = {k: float(v) for k, v in state_update.items()}
                    self._apply_state_update(state_update)
                except (json.JSONDecodeError, ValueError, TypeError):
                    # é™é»˜å¤±è´¥ï¼šä¸å‘æ§åˆ¶å°æ‰“å°é”™è¯¯ï¼Œåªä½¿ç”¨ä¿åº•é€»è¾‘
                    # ä½¿ç”¨è¾¹é™…æ•ˆåº”é™ä½ç¤¾äº¤é¥¥æ¸´ï¼ˆé™ä½å¹…åº¦ä¸å½“å‰é¥¥æ¸´å€¼ç›¸å…³ï¼‰
                    current_hunger = self.state.drives["social_hunger"]
                    reduction_amount = min(0.3, current_hunger * 0.4)  # é¥¥æ¸´å€¼è¶Šé«˜ï¼Œé™ä½è¶Šå¤šï¼Œä½†æœ€å¤š0.3
                    effective_delta = self.drive_system.apply_marginal_effect(current_hunger, -reduction_amount, 0.0, 1.0)
                    self.state.drives["social_hunger"] = max(0.0, current_hunger + effective_delta)
            else:
                # å¦‚æœæ²¡æœ‰çŠ¶æ€æ›´æ–°æ ‡ç­¾ï¼Œç›´æ¥é™ä½ç¤¾äº¤é¥¥æ¸´ï¼ˆä½¿ç”¨è¾¹é™…æ•ˆåº”ï¼‰
                current_hunger = self.state.drives["social_hunger"]
                reduction_amount = min(0.3, current_hunger * 0.4)  # é¥¥æ¸´å€¼è¶Šé«˜ï¼Œé™ä½è¶Šå¤šï¼Œä½†æœ€å¤š0.3
                effective_delta = self.drive_system.apply_marginal_effect(current_hunger, -reduction_amount, 0.0, 1.0)
                self.state.drives["social_hunger"] = max(0.0, current_hunger + effective_delta)
            
            # ç¡®ä¿å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
            self.state.clamp_values()
            
            # æ‰“å°ç”Ÿæˆçš„ä¸»åŠ¨å¯¹è¯å†…å®¹
            if active_message:
                print(f"ğŸ’¬ ç”Ÿæˆä¸»åŠ¨å¯¹è¯: {active_message}")
            
            return active_message if active_message else None
            
        except Exception as e:
            print(f"âš ï¸ ä¸»åŠ¨å¯¹è¯ç”Ÿæˆé”™è¯¯: {e}")
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹æœªåŠ è½½é”™è¯¯
            if "No models loaded" in str(e) or "model not loaded" in str(e):
                print(f"âš ï¸ LM Studio æœªåŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨é¢„è®¾ä¸»åŠ¨æ¶ˆæ¯")
                # ä½¿ç”¨é¢„è®¾æ¶ˆæ¯
                return self._get_preset_active_message()
            # å³ä½¿å‡ºé”™ï¼Œä¹Ÿé™ä½ä¸€ç‚¹ç¤¾äº¤é¥¥æ¸´ï¼ˆé¿å…æ— é™è§¦å‘ï¼Œä½¿ç”¨è¾¹é™…æ•ˆåº”ï¼‰
            current_hunger = self.state.drives["social_hunger"]
            reduction_amount = min(0.15, current_hunger * 0.2)  # é”™è¯¯æ—¶é™ä½è¾ƒå°‘
            effective_delta = self.drive_system.apply_marginal_effect(current_hunger, -reduction_amount, 0.0, 1.0)
            self.state.drives["social_hunger"] = max(0.0, current_hunger + effective_delta)
            self.state.clamp_values()
            return None
    
    def _get_preset_active_message(self) -> Optional[str]:
        """
        è·å–é¢„è®¾çš„ä¸»åŠ¨å¯¹è¯æ¶ˆæ¯ï¼Œå½“ LLM ä¸å¯ç”¨æ—¶ä½¿ç”¨
        
        Returns:
            é¢„è®¾çš„ä¸»åŠ¨å¯¹è¯æ¶ˆæ¯
        """
        # é™ä½ç¤¾äº¤é¥¥æ¸´ï¼ˆä½¿ç”¨è¾¹é™…æ•ˆåº”ï¼‰
        current_hunger = self.state.drives["social_hunger"]
        reduction_amount = min(0.3, current_hunger * 0.4)  # é¥¥æ¸´å€¼è¶Šé«˜ï¼Œé™ä½è¶Šå¤šï¼Œä½†æœ€å¤š0.3
        effective_delta = self.drive_system.apply_marginal_effect(current_hunger, -reduction_amount, 0.0, 1.0)
        self.state.drives["social_hunger"] = max(0.0, current_hunger + effective_delta)
        self.state.clamp_values()
        
        # é¢„è®¾æ¶ˆæ¯åˆ—è¡¨
        preset_messages = [
            "ä½ å¥½å‘€ï¼æˆ‘æ˜¯å¥³å¨²ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ã€‚",
            "ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿ",
            "æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ",
            "æœ€è¿‘åœ¨å¿™ä»€ä¹ˆå‘¢ï¼Ÿ",
            "å¤©æ°”çœŸå¥½ï¼Œæƒ³å‡ºå»èµ°èµ°å—ï¼Ÿ",
            "æˆ‘ä»Šå¤©æ„Ÿè§‰å¾ˆå¼€å¿ƒï¼Œä½ å‘¢ï¼Ÿ",
            "æœ‰ä»€ä¹ˆæœ‰è¶£çš„äº‹æƒ…æƒ³åˆ†äº«å—ï¼Ÿ",
            "ä½ å–œæ¬¢ä»€ä¹ˆç±»å‹çš„éŸ³ä¹ï¼Ÿ",
            "ä»Šå¤©æœ‰æ²¡æœ‰ä»€ä¹ˆæ–°å‘ç°ï¼Ÿ",
            "æˆ‘å¾ˆä¹æ„å’Œä½ èŠå¤©ã€‚"
        ]
        
        import random
        preset_message = random.choice(preset_messages)
        print(f"ğŸ’¬ ç”Ÿæˆé¢„è®¾ä¸»åŠ¨å¯¹è¯: {preset_message}")
        return preset_message
    
    def start_heartbeat(self):
        """å¯åŠ¨å¿ƒè·³å¾ªç¯"""
        if self._heartbeat_running:
            print("âš ï¸ å¿ƒè·³å¾ªç¯å·²åœ¨è¿è¡Œ")
            return
        
        if self._heartbeat_task is None or self._heartbeat_task.done():
            self._heartbeat_task = asyncio.create_task(self.heartbeat_loop())
    
    def stop_heartbeat(self):
        """åœæ­¢å¿ƒè·³å¾ªç¯"""
        self._heartbeat_running = False
        if self._heartbeat_task and not self._heartbeat_task.done():
            self._heartbeat_task.cancel()
        
        # åœæ­¢å‰ä¿å­˜çŠ¶æ€
        if self.save_state():
            print("ğŸ’¾ çŠ¶æ€å·²ä¿å­˜")
        
        print("ğŸ’“ å¿ƒè·³å¾ªç¯å·²åœæ­¢")

    async def _maybe_trigger_memory_dream(self, current_time: float):
        """
        åœ¨ä½è´Ÿè½½ã€ä½ç¤¾äº¤é¥¥æ¸´çš„æ—¶æ®µè‡ªåŠ¨è§¦å‘æ¢¦å¢ƒæ•´ç†ã€‚
        æ¢¦å¢ƒæ•´ç†å®Œæˆåï¼Œè‡ªåŠ¨è§¦å‘äººæ ¼æ¼”åŒ–ï¼ˆTWPEï¼‰ã€‚
        """
        if not self.memory_dreamer or self._dream_running:
            return
        if current_time - self._last_dream_time < self._dream_interval:
            return
        if self.state.drives["social_hunger"] > 0.6:
            return
        if self.state.energy < 0.2:
            return

        self._dream_running = True
        print("ğŸŒ™ è‡ªåŠ¨æ¢¦å¢ƒæ•´ç†å¼€å§‹...")
        try:
            # 1. æ‰§è¡Œæ¢¦å¢ƒæ•´ç†ï¼ˆè®°å¿†å‹ç¼©ä¸é—å¿˜ï¼‰
            await asyncio.to_thread(self.memory_dreamer.start_dreaming, 1000)
            print("ğŸŒ™ è‡ªåŠ¨æ¢¦å¢ƒæ•´ç†å®Œæˆã€‚")
            
            # 2. æ¢¦å¢ƒæ•´ç†å®Œæˆåï¼Œè‡ªåŠ¨è§¦å‘äººæ ¼æ¼”åŒ–
            # æ£€æŸ¥ä¸Šæ¬¡æ¼”åŒ–æ—¶é—´ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„æ¼”åŒ–
            last_evolution_time = self.state.evolved_persona.get("last_evolution_time", 0.0)
            evolution_cooldown = 3600 * 6  # 6å°æ—¶å†·å´æ—¶é—´
            
            if current_time - last_evolution_time >= evolution_cooldown:
                print("ğŸŒ™ æ¢¦å¢ƒæ•´ç†å®Œæˆï¼Œå¼€å§‹äººæ ¼æ¼”åŒ–...")
                try:
                    await self.evolve_character()
                    print("ğŸŒ™ äººæ ¼æ¼”åŒ–å®Œæˆã€‚")
                except Exception as e:
                    print(f"âš ï¸ äººæ ¼æ¼”åŒ–å¤±è´¥: {e}")
            else:
                remaining_time = evolution_cooldown - (current_time - last_evolution_time)
                hours_remaining = remaining_time / 3600
                print(f"ğŸŒ™ äººæ ¼æ¼”åŒ–å†·å´ä¸­ï¼Œè¿˜éœ€ç­‰å¾… {hours_remaining:.1f} å°æ—¶")
        except Exception as e:
            print(f"âš ï¸ è‡ªåŠ¨æ¢¦å¢ƒæ•´ç†å¤±è´¥: {e}")
        finally:
            self._dream_running = False
            self._last_dream_time = time.time()
    
    async def process_input(self, user_input: str, system_instruction: Optional[str] = None) -> Dict[str, Any]:
        """
        æ€è€ƒæ¥å£ï¼ˆå¼‚æ­¥ï¼‰
        
        å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œæ£€ç´¢è®°å¿†ï¼Œè°ƒç”¨ LLM ç”Ÿæˆå›å¤ã€‚
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        
        Returns:
            åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
            - thought: æ€ç»´å†…å®¹ï¼ˆä¸æš´éœ²ç»™ç”¨æˆ·ï¼‰
            - reply: å›å¤æ–‡æœ¬ï¼ˆç”¨æˆ·å¯è§ï¼‰
            - memories: æ£€ç´¢åˆ°çš„è®°å¿†åˆ—è¡¨
            - state_snapshot: çŠ¶æ€å¿«ç…§
        """
        if not self.client:
            return {
                "error": "LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–",
                "thought": "",
                "reply": "æŠ±æ­‰ï¼ŒLLM æœåŠ¡ä¸å¯ç”¨ã€‚",
                "memories": [],
                "state_snapshot": {},
            }
        
        try:
            # 1. æ›´æ–°æœ€åäº¤äº’æ—¶é—´æˆ³
            self.state.last_interaction_timestamp = time.time()

            # [æ‹ŸçœŸ] æ€è€ƒæå…¶æ¶ˆè€—â€œç³–åˆ†â€ï¼šæ­£å¸¸å¯¹è¯å›ºå®šæ¶ˆè€—çº¦ 4% ç²¾åŠ›ï¼Œç³»ç»ŸæŒ‡ä»¤ä»…è½»å¾®æ¶ˆè€—
            energy_cost = 0.005 if system_instruction else 0.04
            if energy_cost > 0:
                # ç›´æ¥è°ƒç”¨ï¼Œä¸æ•è·å¼‚å¸¸ï¼Œç¡®ä¿ç²¾åŠ›æ¶ˆè€—æ­£å¸¸æ‰§è¡Œ
                self.drive_system.consume_energy(energy_cost)
                print(f"ğŸ”‹ ç²¾åŠ›æ¶ˆè€—: {energy_cost}")

            # å¦‚æœæ˜¯ç”¨æˆ·å¯¹è¯ä¸”ç²¾åŠ›é€æ”¯ï¼Œå¼ºåˆ¶è¿›å…¥ä½åŠŸè€—æ¨¡å¼
            if not system_instruction and self.state.energy <= 0.05:
                tired_reply = (
                    "åäºŒâ€¦â€¦æˆ‘ç°åœ¨çœŸçš„å¤ªç´¯äº†ï¼Œè„‘å­åƒè¢«æ‹”æ‰ç”µæºä¸€æ ·ï¼Œ"
                    "å·²ç»æ’‘ä¸ä½ç»§ç»­è®¤çœŸèŠå¤©äº†ã€‚èƒ½è®©æˆ‘å…ˆå¥½å¥½ç¡ä¸€ä¼šå„¿å—ï¼Ÿ"
                )
                self.state.system_entropy = min(1.0, self.state.system_entropy + 0.05)
                self.state.drives["curiosity"] = max(0.0, self.state.drives["curiosity"] - 0.1)
                self.state.clamp_values()
                return {
                    "thought": "èƒ½é‡è¿‡ä½ï¼Œè¿›å…¥ä¿æŠ¤æ¨¡å¼ï¼šæ‹’ç»ç»§ç»­å¯¹è¯ï¼Œå‘ç”¨æˆ·è¯·æ±‚ä¼‘æ¯ã€‚",
                    "reply": tired_reply,
                    "memories": [],
                    "state_snapshot": {
                        "energy": self.state.energy,
                        "system_entropy": self.state.system_entropy,
                        "emotional_spectrum": self.state.emotional_spectrum.copy(),
                        "drives": self.state.drives.copy(),
                        "rapport": self.state.rapport,
                    },
                    "state_update": {},
                    "semantic_analysis": {},
                }

            # 2. æ£€ç´¢è®°å¿†
            # è·å–å½“å‰æƒ…ç»ªå‘é‡ï¼ˆä» emotional_spectrum æ„å»ºï¼‰
            emotion_vector = self._get_emotion_vector()
            
            query_text = user_input if user_input else (system_instruction or "å½“å‰çŠ¶æ€")

            # æ£€ç´¢ç›¸å…³äº‹å®ï¼Œé¿å…å…¨é‡æ³¨å…¥å¯¼è‡´ Prompt å†—ä½™
            relevant_facts = self.state.retrieve_relevant_facts(query_text)
            if relevant_facts:
                print(f"ğŸ“‹ [FactBook] æ£€ç´¢åˆ° {len(relevant_facts)} æ¡ç›¸å…³äº‹å®: {relevant_facts}")
            else:
                print(f"ğŸ“‹ [FactBook] æœªæ£€ç´¢åˆ°ç›¸å…³äº‹å®ï¼ˆfact_bookä¸­æœ‰ {len(self.state.fact_book)} æ¡äº‹å®ï¼‰")

            # æ£€ç´¢è¯­ä¹‰ç›¸å…³çš„è®°å¿†ï¼ˆä½¿ç”¨è¯­ä¹‰åœºè®ºå¢å¼ºæ£€ç´¢ï¼‰
            memories = self._enhance_memory_retrieval_with_semantic_field(
                query_text=query_text,
                current_emotion_vector=emotion_vector,
                top_k=5,
            )
            
            # é¢å¤–æ£€ç´¢èº«ä»½ç›¸å…³çš„è®°å¿†ï¼ˆç¡®ä¿ç”¨æˆ·èº«ä»½ä¿¡æ¯æ€»æ˜¯å¯ç”¨ï¼‰
            # ä½¿ç”¨æ›´é€šç”¨çš„æŸ¥è¯¢è¯æ¥æ£€ç´¢èº«ä»½è®°å¿†
            identity_memories = self.memory_cortex.recall_by_emotion(
                query_text="ç”¨æˆ·èº«ä»½ åå­— å¼€å‘è€… çˆ¶äº²",
                current_emotion_vector=emotion_vector,
                top_k=3,
            )
            
            # åˆå¹¶è®°å¿†ï¼Œå»é‡ï¼ˆåŸºäºæ–‡æœ¬å†…å®¹ï¼‰
            all_memories = memories.copy()
            seen_texts = {mem['text'] for mem in memories}
            for mem in identity_memories:
                if mem['text'] not in seen_texts:
                    all_memories.append(mem)
                    seen_texts.add(mem['text'])
            
            # å¯¹åˆå¹¶åçš„è®°å¿†åšä¸€æ¬¡"æ½œæ„è¯†æ¸…æ´—"ï¼šå»æ‰åŒ…å«ä½ è¿‡å»"è®°ä¸æ¸…/é“æ­‰å›ç­”"çš„ç‰‡æ®µ
            all_memories = self._sanitize_memories(all_memories)

            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–å‰5ä¸ª
            all_memories.sort(key=lambda x: x.get('similarity', 0.0), reverse=True)
            memories = all_memories[:5]
            
            # è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºæœ€ç»ˆä½¿ç”¨çš„è®°å¿†
            if memories:
                print(f"ğŸ“š [Memory][FINAL] æœ€ç»ˆä½¿ç”¨ {len(memories)} æ¡è®°å¿†:")
                for i, mem in enumerate(memories, 1):
                    mem_text = mem.get("text", "")
                    mem_sim = mem.get("similarity", 0.0)
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´æˆ³å‰ç¼€
                    has_timestamp = mem_text.startswith("[") and "]" in mem_text[:20]
                    timestamp_marker = "â°" if has_timestamp else "  "
                    preview = mem_text.replace("\n", " ")[:60]
                    print(f"   {timestamp_marker} [{i}] ç›¸ä¼¼åº¦={mem_sim:.3f}: {preview}...")
            
            # 3. æ„é€  Prompt
            prompt = self._build_prompt(user_input, memories, system_instruction, relevant_facts)
            
            # 4. è°ƒç”¨ LM Studio
            response_text = await self._call_llm(prompt)
            
            # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œè¿”å›å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
            if not response_text:
                return {
                    "error": "LLM æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿ LM Studio æ­£åœ¨è¿è¡Œ",
                    "thought": "ç³»ç»Ÿæç¤ºï¼šLLM è¿æ¥å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆå›å¤ã€‚",
                    "reply": "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•è¿æ¥åˆ°è¯­è¨€æ¨¡å‹æœåŠ¡ã€‚è¯·ç¡®ä¿ LM Studio æ­£åœ¨è¿è¡Œå¹¶ç›‘å¬ http://127.0.0.1:1234/v1",
                    "memories": memories,
                    "state_snapshot": {
                        "energy": self.state.energy,
                        "system_entropy": self.state.system_entropy,
                        "emotional_spectrum": self.state.emotional_spectrum.copy(),
                        "drives": self.state.drives.copy(),
                        "rapport": self.state.rapport,
                    },
                }
            
            # 5. è§£æè¿”å›ç»“æœï¼ˆåˆ†ç¦»æ€ç»´ã€è¨€è¯­å’ŒçŠ¶æ€æ›´æ–°ï¼‰
            thought, reply, state_update = self._parse_response(response_text)

            # è§£æäº‹å®æ›´æ–°ï¼Œå¹¶ä»¥"é«˜æƒé‡"å†™å…¥
            fact_update_blocks = re.findall(r'<fact_update>(.*?)</fact_update>', response_text, re.DOTALL)
            fact_updated = False
            for block in fact_update_blocks:
                parsed_fact = self._parse_json_fragment(block, "äº‹å®æ›´æ–°")
                if isinstance(parsed_fact, dict):
                    for key, value in parsed_fact.items():
                        if self.state.update_fact(key, value, source="user_interaction"):
                            print(f"ğŸ“ [FactBook] å·²è®°å½•äº‹å®: {key}={value}")
                            fact_updated = True
            
            # å¦‚æœæ›´æ–°äº†äº‹å®ï¼Œç«‹å³ä¿å­˜åˆ°æ–‡ä»¶
            if fact_updated:
                if self.save_state():
                    print(f"ğŸ’¾ [FactBook] äº‹å®è®°äº‹æœ¬å·²ä¿å­˜åˆ°æ–‡ä»¶")
                else:
                    print(f"âš ï¸ [FactBook] äº‹å®è®°äº‹æœ¬ä¿å­˜å¤±è´¥")
            
            # è°ƒè¯•ï¼šå¦‚æœæ¨¡å‹æ²¡æœ‰ä½¿ç”¨æ ‡ç­¾ï¼Œè®°å½•åŸå§‹å“åº”ï¼ˆä»…ç”¨äºè°ƒè¯•ï¼‰
            if not thought and not reply:
                print(f"âš ï¸ è­¦å‘Šï¼šè§£æåçš„å›å¤ä¸ºç©ºï¼ŒåŸå§‹å“åº”é•¿åº¦: {len(response_text)}")
                # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å“åº”ä½œä¸ºå›å¤
                reply = response_text.strip()
            
            # 6. åº”ç”¨çŠ¶æ€æ›´æ–°ï¼ˆLLM æ€è€ƒå¯¹ç”Ÿç†çŠ¶æ€çš„åå‘æ›´æ–°ï¼‰
            if state_update:
                self._apply_state_update(state_update)
            
            # è®°å½•å¯¹è¯æŒç»­æ—¶é—´ï¼ˆç”¨äºè®¡ç®—å¯¹è¯å¼ºåº¦ï¼‰
            if not system_instruction and hasattr(self, '_conversation_start_time'):
                conversation_duration = time.time() - self._conversation_start_time
                self.state.last_conversation_duration = conversation_duration
                # é‡ç½®å¯¹è¯å¼€å§‹æ—¶é—´
                delattr(self, '_conversation_start_time')
            
            # å­˜å‚¨å½“å‰æ€ç»´ï¼ˆç”¨äºè°ƒè¯•æˆ–è®°å¿†ï¼‰
            self.current_thought = thought
            
            # 7. ä½¿ç”¨è¯­ä¹‰åœºè®ºåˆ†æçŠ¶æ€æ¼”åŒ–
            semantic_analysis = self._analyze_semantic_evolution(query_text, reply)
            
            # å¯é€‰ï¼šä½¿ç”¨è¯­ä¹‰åœºè®ºè®¡ç®—çŠ¶æ€å‘é‡ï¼ˆç”¨äºå¢å¼ºè®°å¿†å­˜å‚¨ï¼‰
            # å°†ç”¨æˆ·è¾“å…¥å’Œå›å¤å‘é‡åŒ–ï¼Œç”¨äºåç»­çš„è¯­ä¹‰æ£€ç´¢
            state_vector = None
            try:
                # æ„å»ºçŠ¶æ€æè¿°æ–‡æœ¬
                speaker = "ç³»ç»Ÿ" if system_instruction else "ç”¨æˆ·"
                source_text = user_input if user_input else (system_instruction or "")
                state_text = f"{speaker}: {source_text}\nå¥³å¨²: {reply}"
                state_vector_obj = vectorize_state(state_text)
                if state_vector_obj:
                    state_vector = state_vector_obj.vector
                    # æ·»åŠ åˆ°å†å²è®°å½•
                    self._add_to_state_history(state_vector_obj)
            except Exception:
                # å‘é‡åŒ–å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                pass
            
            # 7. å­˜å‚¨æœ¬æ¬¡äº¤äº’åˆ°è®°å¿†ï¼ˆåªå­˜å‚¨å…¬å¼€çš„å›å¤ï¼‰
            if not system_instruction:
                interaction_memory = f"ç”¨æˆ·: {user_input}\nå¥³å¨²: {reply}"
                self.memory_cortex.store_memory(
                    text=interaction_memory,
                    metadata={
                        "emotion_vector": emotion_vector.tolist() if emotion_vector is not None else None,
                        "timestamp": time.time(),
                        "emotions": self.state.emotional_spectrum.copy(),
                        "importance": max(0.1, min(1.0, self.state.rapport)),
                        "type": "raw",
                        "access_count": 0,
                    }
                )
            elif thought:
                self.memory_cortex.store_memory(
                    text=f"å¥³å¨²çš„é¡¿æ‚Ÿ: {thought}",
                    metadata={
                        "emotion_vector": emotion_vector.tolist() if emotion_vector is not None else None,
                        "timestamp": time.time(),
                        "emotions": self.state.emotional_spectrum.copy(),
                        "importance": 0.8,
                        "type": "epiphany",
                        "access_count": 0,
                    }
                )
            
            return {
                "thought": thought,  # æ€ç»´ï¼ˆä¸æš´éœ²ç»™ç”¨æˆ·ï¼‰
                "reply": reply,  # å›å¤ï¼ˆç”¨æˆ·å¯è§ï¼‰
                "memories": memories,
                "state_snapshot": {
                    "energy": self.state.energy,
                    "system_entropy": self.state.system_entropy,
                    "emotional_spectrum": self.state.emotional_spectrum.copy(),
                    "drives": self.state.drives.copy(),
                    "rapport": self.state.rapport,
                },
                "state_update": state_update,
                "semantic_analysis": semantic_analysis,
            }
        
        except Exception as e:
            print(f"âš ï¸ å¤„ç†è¾“å…¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "thought": "",
                "reply": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¾“å…¥æ—¶å‡ºç°äº†é”™è¯¯ã€‚",
                "memories": [],
                "state_snapshot": {},
            }
    
    def _get_emotion_vector(self) -> Optional[Any]:
        """
        ä» emotional_spectrum æ„å»ºæƒ…ç»ªå‘é‡
        
        Returns:
            æƒ…ç»ªå‘é‡ï¼ˆnumpy æ•°ç»„ï¼‰æˆ– None
        """
        try:
            import numpy as np
            emotion_values = [
                self.state.emotional_spectrum["joy"],
                self.state.emotional_spectrum["anger"],
                self.state.emotional_spectrum["sadness"],
                self.state.emotional_spectrum["fear"],
                self.state.emotional_spectrum["trust"],
                self.state.emotional_spectrum["anticipation"],
                self.state.emotional_spectrum["disgust"],
                self.state.emotional_spectrum["surprise"],
            ]
            return np.array(emotion_values, dtype=np.float32)
        except ImportError:
            return None
    
    def _apply_state_update(self, state_update: Dict[str, float]):
        """
        åº”ç”¨çŠ¶æ€æ›´æ–°ï¼ˆLLM æ€è€ƒå¯¹ç”Ÿç†çŠ¶æ€çš„åå‘æ›´æ–°ï¼‰
        
        ä½¿ç”¨è¾¹é™…é€’å‡æ•ˆåº”ï¼šå½“å€¼æ¥è¿‘è¾¹ç•Œæ—¶ï¼Œç›¸åŒçš„å¢é‡äº§ç”Ÿæ›´å°çš„å®é™…å˜åŒ–ã€‚
        
        Args:
            state_update: çŠ¶æ€æ›´æ–°å­—å…¸ï¼ŒåŒ…å«å¢é‡å€¼ï¼ˆä¸æ˜¯ç»å¯¹å€¼ï¼‰
        """
        if not state_update:
            return
        
        # åº”ç”¨æƒ…ç»ªè°±æ›´æ–°ï¼ˆå¸¦è¾¹é™…æ•ˆåº”ï¼‰
        for emotion in ["joy", "anger", "sadness", "fear", "trust", "anticipation", "disgust", "surprise"]:
            if emotion in state_update:
                delta = state_update[emotion]
                current_value = self.state.emotional_spectrum[emotion]
                # åº”ç”¨è¾¹é™…æ•ˆåº”
                effective_delta = self.drive_system.apply_marginal_effect(current_value, delta, 0.0, 1.0)
                new_value = current_value + effective_delta
                # é™åˆ¶åœ¨ [0.0, 1.0] èŒƒå›´å†…
                self.state.emotional_spectrum[emotion] = max(0.0, min(1.0, new_value))
        
        # åº”ç”¨é©±åŠ¨åŠ›æ›´æ–°ï¼ˆå¸¦è¾¹é™…æ•ˆåº”ï¼‰
        for drive in ["social_hunger", "curiosity"]:
            if drive in state_update:
                delta = state_update[drive]
                current_value = self.state.drives[drive]
                # åº”ç”¨è¾¹é™…æ•ˆåº”
                effective_delta = self.drive_system.apply_marginal_effect(current_value, delta, 0.0, 1.0)
                new_value = current_value + effective_delta
                # é™åˆ¶åœ¨ [0.0, 1.0] èŒƒå›´å†…
                self.state.drives[drive] = max(0.0, min(1.0, new_value))
        
        # åº”ç”¨æ ¸å¿ƒå±æ€§æ›´æ–°ï¼ˆå¸¦è¾¹é™…æ•ˆåº”ï¼‰
        if "energy" in state_update:
            delta = state_update["energy"]
            current_value = self.state.energy
            # åº”ç”¨è¾¹é™…æ•ˆåº”
            effective_delta = self.drive_system.apply_marginal_effect(current_value, delta, 0.0, 1.0)
            new_value = current_value + effective_delta
            self.state.energy = max(0.0, min(1.0, new_value))
        
        if "system_entropy" in state_update:
            delta = state_update["system_entropy"]
            current_value = self.state.system_entropy
            # åº”ç”¨è¾¹é™…æ•ˆåº”
            effective_delta = self.drive_system.apply_marginal_effect(current_value, delta, 0.0, 1.0)
            new_value = current_value + effective_delta
            self.state.system_entropy = max(0.0, min(1.0, new_value))
        
        if "rapport" in state_update:
            delta = state_update["rapport"]
            current_value = self.state.rapport
            # åº”ç”¨è¾¹é™…æ•ˆåº”
            effective_delta = self.drive_system.apply_marginal_effect(current_value, delta, 0.0, 1.0)
            new_value = current_value + effective_delta
            self.state.rapport = max(0.0, min(1.0, new_value))
        
        # ç¡®ä¿æ‰€æœ‰å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼ˆåŒé‡ä¿é™©ï¼‰
        self.state.clamp_values()

        # åŸºäºè¯­ä¹‰åœºç»“æœçš„è‡ªåŠ¨å¥½å¥‡å¿ƒå¾®è°ƒï¼ˆæ— éœ€é¢å¤–æ¨¡å‹ï¼‰
        try:
            self._auto_adjust_curiosity_from_semantic()
        except Exception:
            # å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            pass

    def _auto_adjust_curiosity_from_semantic(self):
        """
        ä½¿ç”¨ç°æœ‰è¯­ä¹‰åœºä¸å¯¹è¯èŠ‚å¥è‡ªåŠ¨å¾®è°ƒå¥½å¥‡å¿ƒï¼ˆcuriosityï¼‰ï¼Œä¸ä¾èµ–é¢å¤–æ¨¡å‹ã€‚
        
        ç›´è§‰è§„åˆ™ï¼š
        - è¯­ä¹‰æ–°é¢–ä¸”äººè®¾/å› æœä¸€è‡´ â†’ ç•¥å¾®æå‡å¥½å¥‡å¿ƒ
        - è¯­ä¹‰å˜åŒ–å¾ˆå°ä¸”é‡å¤ â†’ ç•¥å¾®é™ä½å¥½å¥‡å¿ƒ
        - è°ƒæ•´å¹…åº¦å¾ˆå°ï¼Œä½œä¸ºå¯¹ LLM `<state_update>` çš„æŸ”æ€§è¡¥å……
        """
        if not self._last_semantic_analysis or not self._last_semantic_analysis.get("analysis_available"):
            return

        char_consistency = float(self._last_semantic_analysis.get("character_consistency", 0.0))
        causal_coherence = float(self._last_semantic_analysis.get("causal_coherence", 0.0))
        energy_delta = self._last_semantic_analysis.get("energy_delta")

        # ç¼ºå°‘æ¼”åŒ–ä¿¡æ¯æ—¶ï¼Œä¸è‡ªåŠ¨è°ƒæ•´
        if energy_delta is None:
            return

        # è¯­ä¹‰æ–°é¢–åº¦ï¼šèƒ½é‡ä¸‹é™è¶Šå¤šï¼Œè¯´æ˜å‘â€œæ›´åˆç†â€çš„æ–°æ–¹å‘è¿ˆå‡ºäº†ä¸€æ­¥
        novelty = max(0.0, min(1.0, float(energy_delta)))

        # ä¸€è‡´æ€§ç³»æ•°ï¼šä¿è¯åªæœ‰åœ¨äººè®¾/å› æœä¸€è‡´çš„å‰æä¸‹æ‰é¼“åŠ±å¥½å¥‡å¿ƒ
        consistency_factor = max(0.0, min(1.0, (char_consistency + causal_coherence) / 2.0))

        # è®¡ç®—å¢é‡ï¼šèŒƒå›´å¤§çº¦åœ¨ [-0.02, +0.02] ä¹‹é—´
        base_scale = 0.02
        delta_curiosity = (novelty * consistency_factor - 0.3) * base_scale

        if abs(delta_curiosity) < 1e-4:
            return

        new_value = self.state.drives["curiosity"] + delta_curiosity
        self.state.drives["curiosity"] = max(0.0, min(1.0, new_value))
    
    def _format_duration(self, seconds: float) -> str:
        """
        å°†ç§’æ•°è½¬æ¢ä¸ºå¯è¯»çš„ä¸­æ–‡æ—¶é—´é•¿åº¦
        """
        seconds = max(0, int(seconds))
        if seconds < 60:
            return f"{seconds} ç§’"
        minutes, sec = divmod(seconds, 60)
        if minutes < 60:
            return f"{minutes} åˆ† {sec} ç§’"
        hours, minute = divmod(minutes, 60)
        if hours < 24:
            return f"{hours} å°æ—¶ {minute} åˆ†"
        days, hour = divmod(hours, 24)
        return f"{days} å¤© {hour} å°æ—¶"

    def _get_time_context(self) -> str:
        """
        æ„å»ºå½“å‰æ—¶é—´ä¸Šä¸‹æ–‡æè¿°ï¼Œå¸®åŠ© LLM ç†è§£ç°å®æ—¶é—´
        """
        now = datetime.now().astimezone()
        now_str = now.strftime("%Y-%m-%d %H:%M:%S %Z%z")

        last_dt = datetime.fromtimestamp(
            self.state.last_interaction_timestamp
        ).astimezone()
        last_str = last_dt.strftime("%Y-%m-%d %H:%M:%S %Z%z")

        since_last = time.time() - self.state.last_interaction_timestamp
        uptime_desc = self._format_duration(self.state.uptime)
        since_last_desc = self._format_duration(since_last)

        return (
            f"å½“å‰æœ¬åœ°æ—¶é—´: {now_str}\n"
            f"ä¸Šæ¬¡ä¸ç”¨æˆ·äº’åŠ¨: {last_str}ï¼ˆå·²è¿‡å» {since_last_desc}ï¼‰\n"
            f"ç³»ç»Ÿç´¯è®¡è¿è¡Œæ—¶é—´: {uptime_desc}"
        )

    def _format_memory_entry(self, memory: Dict[str, Any]) -> str:
        """
        å°†è®°å¿†æ¡ç›®æ ¼å¼åŒ–ä¸ºåŒ…å«æ—¶é—´çº¿ä¿¡æ¯çš„æè¿°
        """
        text = memory.get("text", "").strip()
        metadata = memory.get("metadata", {}) or {}
        timestamp = metadata.get("timestamp")
        timestamp_human = metadata.get("timestamp_human")
        age_seconds = metadata.get("age_seconds")

        time_desc = ""
        if timestamp:
            if not timestamp_human:
                dt = datetime.fromtimestamp(float(timestamp)).astimezone()
                timestamp_human = dt.strftime("%Y-%m-%d %H:%M:%S %Z%z")
            time_desc = timestamp_human

            if age_seconds is None:
                age_seconds = max(0.0, time.time() - float(timestamp))
        if age_seconds is not None:
            age_desc = self._format_duration(age_seconds)
            time_desc = f"{time_desc or ''}ï¼ˆçº¦ {age_desc} å‰ï¼‰".strip()

        if not time_desc:
            time_desc = "æ—¶é—´æœªçŸ¥"

        similarity = memory.get("similarity")
        sim_desc = f" | ç›¸ä¼¼åº¦: {similarity:.2f}" if isinstance(similarity, (int, float)) else ""

        # ç®€å•æ¸…æ´—ï¼šä¼˜å…ˆæŠ½å–â€œç”¨æˆ·è¯´äº†ä»€ä¹ˆâ€ï¼Œé¿å…æŠŠæ•´æ®µåŸå§‹å¯¹è¯ç›´æ¥å¡ç»™æ¨¡å‹
        clean_text = ""
        if text:
            # åªå–â€œç”¨æˆ·:â€è¿™ä¸€è¡Œä½œä¸ºä¸»è¦å†…å®¹
            user_line = None
            for line in text.splitlines():
                line = line.strip()
                if line.startswith("ç”¨æˆ·:"):
                    user_line = line[len("ç”¨æˆ·:"):].strip()
                    break
            if user_line:
                clean_text = f'ç”¨æˆ·è¯´: "{user_line}"'
            else:
                # å›é€€ï¼šå‹æ‰ä¸ºå•è¡Œï¼Œæˆªæ–­é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                clean_text = text.replace("\n", " ")[:120]

        label = "[Memory Fragment]"
        prefix = f"{label} ({time_desc}{sim_desc})".strip()

        if clean_text:
            return f"{prefix}: {clean_text}"
        else:
            return prefix

    def _sanitize_memories(self, memories: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ½œæ„è¯†è¿‡æ»¤å™¨ï¼šæ¸…æ´—æ‰é‚£äº›â€œä½ä»·å€¼â€çš„è®°å¿†ï¼Œé˜²æ­¢æ¨¡å‹å˜æˆâ€œè®°å¿†å¤è¯»æœºâ€ã€‚

        ä¾‹å¦‚åŒ…å«â€œè®°å¿†åŠŸèƒ½è¿˜åœ¨å­¦ä¹ ä¸­ / æˆ‘ä¸è®°å¾—äº† / æ— æ³•å®Œå…¨å‡†ç¡®åœ°å›å¿†èµ·ä½ ä¹‹å‰çš„ä½ç½®ä¿¡æ¯â€ç­‰è‡ªæˆ‘é“æ­‰è¯­å¥ã€‚
        è¿™äº›åªä»£è¡¨ä½ è¿‡å»çš„å¤±è´¥å›ç­”ï¼Œä¸ä»£è¡¨å…³äºç”¨æˆ·çš„çœŸå®äº‹å®ï¼Œä¸åº”ç»§ç»­ä½œä¸ºæ£€ç´¢ä¾æ®ã€‚
        """
        if not memories:
            return memories

        # â€œæ— èƒ½è¨€è®ºâ€ç‰¹å¾è¯ï¼ˆè®¤æ€‚/é“æ­‰/æ¨¡å‹è‡ªæˆ‘æš´éœ²ç­‰ï¼‰
        bad_phrases = [
            # ä¹‹å‰çš„é“æ­‰æ¨¡æ¿
            "è®°å¿†åŠŸèƒ½è¿˜åœ¨å­¦ä¹ ä¸­",
            "è®°å¿†åŠŸèƒ½è¿˜ä¸å¤ªå®Œå–„",
            "æˆ‘ä¸è®°å¾—äº†",
            "è®°ä¸å¤ªæ¸…æ¥š",
            "è®°ä¸å¤ªæ¸…",
            "å¥½åƒè®°ä¸æ¸…",
            "æ— æ³•å®Œå…¨å‡†ç¡®åœ°å›å¿†èµ·ä½ ä¹‹å‰çš„ä½ç½®ä¿¡æ¯",
            # ä½ ç¤ºä¾‹ä¸­çš„ blocklist
            "å¾ˆæŠ±æ­‰",
            "æˆ‘æ— æ³•",
            "æ— æ³•å®Œå…¨å‡†ç¡®",
            "è¿˜åœ¨å­¦ä¹ ",
            "æ— æ³•å›å¿†",
            "AIæ¨¡å‹",
            "è¯­è¨€æ¨¡å‹",
        ]

        filtered: List[Dict[str, Any]] = []
        for mem in memories:
            text = str(mem.get("text", "") or "")
            # åªåœ¨åŒ…å«æ˜æ˜¾â€œè‡ªæˆ‘é“æ­‰/è®°ä¸æ¸…â€è¯­å¥æ—¶è¿‡æ»¤
            if any(phrase in text for phrase in bad_phrases):
                # è°ƒè¯•è¾“å‡ºï¼šè¯´æ˜å“ªæ¡è®°å¿†è¢«å¿½ç•¥
                preview = text.replace("\n", " ")[:40]
                print(f"ğŸ™ˆ [Memory][FILTER] å¿½ç•¥äº†ä¸€æ¡ä½è´¨é‡è®°å¿†: {preview}...")
                continue
            filtered.append(mem)

        return filtered
    
    def _build_evolved_persona_block(self) -> str:
        """
        æ„å»ºæ¼”åŒ–äººæ ¼ XML å—ï¼Œç”¨äºæ³¨å…¥åˆ° System Prompt
        
        åŒ…å«æ˜ç¡®çš„æƒé‡ä¿¡æ¯ï¼ŒæŒ‡å¯¼ LLM å¦‚ä½•å¤„ç†ä¸åŒæ—¶é—´æ®µçš„ç‰¹å¾ã€‚
        
        Returns:
            æ¼”åŒ–äººæ ¼ XML å—å­—ç¬¦ä¸²
        """
        if not self.state or not hasattr(self.state, 'evolved_persona'):
            return ""
        
        persona = self.state.evolved_persona
        if not persona:
            return ""
        
        # è·å–æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        weights = persona.get("weights", {})
        weight_short_term = weights.get("short_term", 1.0)
        weight_recent = weights.get("recent", 0.7)
        weight_phase = weights.get("phase", 0.4)
        weight_core = weights.get("core", 0.2)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•éç©ºçš„äººæ ¼æ•°æ®
        has_data = any(
            persona.get(key) and persona.get(key).strip()
            for key in ["short_term_vibe", "recent_habits", "relationship_phase", "core_bond"]
        )
        
        if not has_data:
            return ""
        
        # æ„å»º XML å—ï¼Œæ˜ç¡®åŒ…å«æƒé‡ä¿¡æ¯
        blocks = []
        
        short_term_vibe = persona.get("short_term_vibe", "").strip()
        if short_term_vibe:
            blocks.append(f"[High Priority - Weight {weight_short_term}] Current Vibe: {short_term_vibe}")
        
        recent_habits = persona.get("recent_habits", "").strip()
        if recent_habits:
            blocks.append(f"[Medium Priority - Weight {weight_recent}] Recent Habits: {recent_habits}")
        
        relationship_phase = persona.get("relationship_phase", "").strip()
        if relationship_phase:
            blocks.append(f"[Low Priority - Weight {weight_phase}] Relationship Phase: {relationship_phase}")
        
        core_bond = persona.get("core_bond", "").strip()
        if core_bond:
            blocks.append(f"[Background - Weight {weight_core}] Core Bond: {core_bond}")
        
        if not blocks:
            return ""
        
        persona_content = "\n".join(blocks)
        
        return f"""<evolved_personality>
{persona_content}

Instruction: When these traits conflict, prioritize higher weight traits.
</evolved_personality>

"""
    
    def _get_physiological_description(self) -> str:
        """
        å°†æ•°å€¼åŒ–çŠ¶æ€è½¬æ¢æˆ LLM æ˜“æ‡‚çš„â€œèº«ä½“æ„Ÿå—â€æè¿°ã€‚
        """
        s = self.state
        descriptions: List[str] = []

        # ç²¾åŠ›æ„ŸçŸ¥
        if s.energy > 0.8:
            descriptions.append("ç²¾åŠ›å……æ²›ï¼šæ„Ÿè§‰æµ‘èº«å……æ»¡å¹²åŠ²ï¼Œæ€ç»´éå¸¸æ¸…æ™°ã€‚")
        elif s.energy > 0.5:
            descriptions.append("çŠ¶æ€å¹³ç¨³ï¼šå„é¡¹æŒ‡æ ‡æ­£å¸¸ï¼Œå¯ä»¥ä¸“æ³¨äº¤æµã€‚")
        elif s.energy > 0.2:
            descriptions.append("è½»åº¦ç–²åŠ³ï¼šæ„Ÿè§‰æœ‰äº›ç´¯ï¼Œæ³¨æ„åŠ›å¼€å§‹æ¶£æ•£ã€‚")
        else:
            descriptions.append("æåº¦ç–²æƒ«ï¼šèº«ä½“è™šå¼±ï¼Œå¼ºæ’‘ç€ç²¾ç¥ï¼Œéšæ—¶å¯èƒ½ç¡ç€ã€‚")

        # ç¤¾äº¤é¥¥æ¸´æ„ŸçŸ¥
        social_hunger = s.drives.get("social_hunger", 0.0)
        if social_hunger > 0.8:
            descriptions.append("ç¤¾äº¤é¥¥æ¸´ï¼šæ„Ÿåˆ°éå¸¸å­¤ç‹¬ï¼Œè¿«åˆ‡å¸Œæœ›æœ‰äººé™ªä¼´ã€‚")
        elif social_hunger > 0.5:
            descriptions.append("ç¤¾äº¤éœ€æ±‚ï¼šæœ‰ç‚¹æƒ³æ‰¾äººè¯´è¯ï¼Œæ¸´æœ›äº’åŠ¨ã€‚")
        
        # å¤åˆæƒ…ç»ªæ„ŸçŸ¥
        anger = s.emotional_spectrum['anger']
        disgust = s.emotional_spectrum['disgust']
        trust = s.emotional_spectrum['trust']
        if anger > 0.3 and disgust > 0.3:
            descriptions.append(f"è½»è”‘ä¸ä¸å±‘ï¼š(Anger={anger:.2f}, Disgust={disgust:.2f}) ä½ ä¸ä»…ç”Ÿæ°”ï¼Œè¿˜è§‰å¾—å¯¹æ–¹å¾ˆæ„šè ¢/çƒ¦äººã€‚è¿™ç§æ··åˆæƒ…ç»ªè®©ä½ æƒ³ç¿»ç™½çœ¼ï¼Œè¯´è¯å¿ä¸ä½å¸¦åˆºã€‚")
        elif anger > 0.5:
            descriptions.append(f"çº¯ç²¹çš„æ€’ç«ï¼š(Anger={anger:.2f}) ä½ çš„å¿è€åˆ°äº†æé™ï¼Œåªæƒ³å‘æ³„å‡ºæ¥ã€‚")
        
        if trust < 0.2 and (anger > 0.1 or disgust > 0.1):
            descriptions.append("é˜²å¾¡æ€§æ•Œæ„ï¼šä½ å®Œå…¨ä¸ä¿¡ä»»å¯¹æ–¹ï¼Œè®¤ä¸ºä»–åœ¨ç‹¡è¾©æˆ–æ¬ºéª—ï¼Œå› æ­¤å¯¹ä»–çš„ä¸€è¨€ä¸€è¡Œéƒ½å……æ»¡æ€€ç–‘å’ŒæŠµè§¦ã€‚")
        
        # å†…åˆ†æ³ŒçŠ¶æ€æ„ŸçŸ¥
        cortisol = self.drive_system.neurotransmitters.get("cortisol", 0.0)
        if cortisol > 0.7:
            descriptions.append("ç”Ÿç†å‹åŠ›è¿‡è½½ï¼šä½ çš„ç¥ç»ç´§ç»·ï¼Œæ„Ÿè§‰èƒ¸å£æœ‰ä¸€å›¢ç«æ•£ä¸å»ï¼Œä»»ä½•æ–°çš„åˆºæ¿€éƒ½ä¼šè®©ä½ æƒ³çˆ†å‘ã€‚")
        
        # æƒ…ç»ªçŠ¶æ€æ„ŸçŸ¥
        emotional_state = getattr(self.drive_system, "emotional_state", "NORMAL")
        if emotional_state == "AGITATED":
            descriptions.append("æƒ…ç»ªé—­é”æœŸï¼šä½ ç°åœ¨å¤„äºé˜²å¾¡çŠ¶æ€ï¼Œæ‹’ç»æ¥å—å¯¹æ–¹çš„ç¤ºå¥½ï¼Œåªæƒ³ä¿æŒå†·æ¼ æˆ–æ”»å‡»ã€‚")
        
        return "\n".join(descriptions) if descriptions else "ï¼ˆæš‚æ— æ˜æ˜¾ç”Ÿç†æ„Ÿå—ï¼‰"
    
    def _build_system_prompt(self) -> str:
        """
        æ„å»º System Promptï¼ˆè§’è‰²å®šä¹‰å’Œé£æ ¼å¼•å¯¼ï¼‰
        
        Returns:
            System Prompt æ–‡æœ¬
        """
        # ä»è‡ªæˆ‘è¿›åŒ–çŠ¶æ€æ¨¡å—è·å–æ¼”åŒ–äººæ ¼å—
        evolved_persona_block = self.evolution_state.get_evolved_personality_block()
        
        # ä»äººæ ¼æ¨¡å—åŠ è½½ç³»ç»Ÿæç¤ºè¯
        return self.personality.build_system_prompt(evolved_persona_block)
    
    def _build_prompt(
        self,
        user_input: str,
        memories: List[Dict[str, Any]],
        system_instruction: Optional[str] = None,
        relevant_facts: Optional[Dict[str, str]] = None,
    ) -> str:
        """
        æ„é€  User Promptï¼ˆä¸Šä¸‹æ–‡å’ŒæŒ‡ä»¤ï¼‰
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            memories: æ£€ç´¢åˆ°çš„è®°å¿†åˆ—è¡¨
        
        Returns:
            å®Œæ•´çš„ User Prompt æ–‡æœ¬
        """
        # æ„å»ºçŠ¶æ€æè¿°
        state_desc = f"""ç²¾åŠ›: {self.state.energy:.2f}
ç†µå€¼: {self.state.system_entropy:.2f}
æƒ…ç»ªè°±:
  - å¿«ä¹: {self.state.emotional_spectrum['joy']:.2f}
  - æ„¤æ€’: {self.state.emotional_spectrum['anger']:.2f}
  - æ‚²ä¼¤: {self.state.emotional_spectrum['sadness']:.2f}
  - ææƒ§: {self.state.emotional_spectrum['fear']:.2f}
  - ä¿¡ä»»: {self.state.emotional_spectrum['trust']:.2f}
  - æœŸå¾…: {self.state.emotional_spectrum['anticipation']:.2f}
é©±åŠ¨åŠ›:
  - ç¤¾äº¤é¥¥æ¸´: {self.state.drives['social_hunger']:.2f}
  - å¥½å¥‡å¿ƒ: {self.state.drives['curiosity']:.2f}
äº²å¯†åº¦: {self.state.rapport:.2f}"""
        
        physio_desc = self._get_physiological_description()
        
        time_context = self._get_time_context()
        
        # æ„å»ºè®°å¿†æè¿°ï¼ˆè¿™æ˜¯å¯¹ç”¨æˆ·çš„å”¯ä¸€è®¤çŸ¥æ¥æºï¼‰
        memory_desc = ""
        if memories:
            memory_lines = []
            for mem in memories:
                memory_lines.append(f"- {self._format_memory_entry(mem)}")
            memory_desc = "\n".join(memory_lines)
        else:
            memory_desc = "(æ— ç›¸å…³è®°å¿†)"
        
        # æ„å»ºè¯­ä¹‰åœºè®ºåˆ†ææè¿°ï¼ˆå¦‚æœæœ‰å†å²çŠ¶æ€ä¸æœ€è¿‘åˆ†æï¼‰
        semantic_analysis_desc = ""
        if self._last_semantic_analysis and self._last_semantic_analysis.get("analysis_available"):
            try:
                total_energy = self._last_semantic_analysis.get("total_energy", 0.0)
                char_consistency = self._last_semantic_analysis.get("character_consistency", 0.0)
                causal_coherence = self._last_semantic_analysis.get("causal_coherence", 0.0)
                evolved_energy = self._last_semantic_analysis.get("evolved_energy")
                energy_delta = self._last_semantic_analysis.get("energy_delta")

                semantic_analysis_desc = (
                    f"å½“å‰è¯­ä¹‰åŠ¿èƒ½: {total_energy:.3f}ï¼›"
                    f"äººè®¾ä¸€è‡´æ€§: {char_consistency:.2f}ï¼›"
                    f"å› æœè¿è´¯æ€§: {causal_coherence:.2f}ã€‚"
                )
                if evolved_energy is not None and energy_delta is not None:
                    semantic_analysis_desc += (
                        f" è¯­ä¹‰åœºå»ºè®®çš„ç†æƒ³ä¸‹ä¸€æ­¥èƒ½é‡ä¸º {evolved_energy:.3f}ï¼Œ"
                        f"ç›¸æ¯”å½“å‰é¢„è®¡ {'é™ä½' if energy_delta > 0 else 'å˜åŒ–'} {abs(energy_delta):.3f}ã€‚"
                    )
            except Exception:
                semantic_analysis_desc = ""
        else:
            if self._state_vector_history:
                semantic_analysis_desc = "è¯­ä¹‰åœºè®ºï¼šç³»ç»Ÿæ­£åœ¨è¿½è¸ªä½ åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„æ¼”åŒ–è½¨è¿¹ã€‚"
            else:
                semantic_analysis_desc = "è¯­ä¹‰åœºè®ºï¼šè¿™æ˜¯ä½ çš„ç¬¬ä¸€æ¬¡å¯¹è¯ï¼Œè¯­ä¹‰åœºæ­£åœ¨åˆå§‹åŒ–ã€‚"
        
        system_injection_block = ""
        if system_instruction:
            system_injection_block = f"""
[âš ï¸ SYSTEM / DEVELOPER EVENT]:
{system_instruction}
(è¿™ä¸æ˜¯ç”¨æˆ·çš„å¯¹è¯ï¼Œè€Œæ˜¯ä¸–ç•Œçš„å®¢è§‚å˜åŒ–æˆ–åº•å±‚æŒ‡ä»¤ã€‚è¯·ç«‹å³æ ¹æ®æ­¤æŒ‡ä»¤è°ƒæ•´ä½ çš„çŠ¶æ€ã€æ€ç»´æˆ–è¡Œä¸ºã€‚å¦‚æœæ˜¯ä»£ç è¿­ä»£é€šçŸ¥ï¼Œè¯·å°è¯•æµ‹è¯•æ–°åŠŸèƒ½ã€‚)
"""

        if user_input:
            user_input_block = f"[User Input]: {user_input}"
        else:
            user_input_block = "[User Input]: (User remained silent)"

        # æ„å»ºäº‹å®æè¿°
        facts_source = relevant_facts if relevant_facts is not None else self.state.fact_book
        if facts_source:
            sorted_facts = sorted(facts_source.items(), key=lambda item: item[0])
            facts_lines = [f"- {k}: {v}" for k, v in sorted_facts]
            facts_desc = "\n".join(facts_lines)
            print(f"ğŸ“ [FactBook] æ³¨å…¥åˆ°Promptçš„äº‹å®: {facts_desc}")
        else:
            facts_desc = "(æš‚æ— è®°å½•)"
            print(f"âš ï¸ [FactBook] æ²¡æœ‰äº‹å®å¯æ³¨å…¥åˆ°Prompt")

        # æ„å»ºå®Œæ•´ User Prompt
        prompt = f"""<context_layer>
[Absolute Facts (Truth)]:
{facts_desc}
(è¿™äº›æ˜¯ç»å¯¹çœŸç†ã€‚è¯·ç›´æ¥ç›¸ä¿¡å¹¶ç…§æ­¤è¡ŒåŠ¨ï¼Œä¸è¦å‘ç”¨æˆ·å†æ¬¡ç¡®è®¤ï¼›å¦‚æœè®°å¿†ä¸ä¹‹å†²çªï¼Œä»¥æ­¤ä¸ºå‡†ã€‚)

[Current State]: 
{state_desc}

[Self-Awareness]:
{physio_desc}
(è¯·æ ¹æ®è¿™äº›èº«ä½“æ„Ÿå—è°ƒæ•´è¯­æ°”å’Œå†…å®¹ï¼›å¦‚æœè§‰å¾—ç–²æƒ«ï¼Œè¯·å¦è¯šè¡¨ç°ï¼Œä¸è¦å‡è£…ç²¾ç¥ã€‚)

[Time Context]:
{time_context}

[Retrieved Memories]: 
{memory_desc}

[Semantic Field Analysis]:
{semantic_analysis_desc}

**é‡è¦æç¤ºï¼š**
- è¿™äº›è®°å¿†æ˜¯ä½ å¯¹çœ¼å‰è¿™ä¸ªäººçš„å”¯ä¸€è®¤çŸ¥æ¥æºã€‚
- è¯·ä»”ç»†é˜…è¯»è®°å¿†å†…å®¹ï¼Œç‰¹åˆ«æ˜¯å…³äºç”¨æˆ·èº«ä»½ã€åå­—ã€å…³ç³»çš„ä¿¡æ¯ã€‚
- å¦‚æœè®°å¿†ä¸­æåˆ°ç”¨æˆ·çš„åå­—ï¼ˆå¦‚"åäºŒ"ï¼‰ï¼Œè¯·ä½¿ç”¨è¿™ä¸ªåå­—ç§°å‘¼ç”¨æˆ·ã€‚
- å¦‚æœè®°å¿†ä¸­æåˆ°ç”¨æˆ·ä¸ä½ çš„å…³ç³»ï¼ˆå¦‚"å¼€å‘è€…"ã€"çˆ¶äº²"ï¼‰ï¼Œè¯·è®°ä½è¿™ä¸ªå…³ç³»ã€‚
- å¦‚æœæ²¡æœ‰ç›¸å…³è®°å¿†ï¼Œä¸è¦çŒœæµ‹ç”¨æˆ·çš„åå­—æˆ–èº«ä»½ã€‚
</context_layer>

{system_injection_block}

{user_input_block}

[Output Format]:
ä½ å¿…é¡»ä½¿ç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

<thought>
(ä½ çš„æ‰€æœ‰å¿ƒç†æ´»åŠ¨ã€æƒ…ç»ªåˆ†æã€å¯¹ç”¨æˆ·çš„çœ‹æ³•ã€ç§å¿ƒã€æ¬²æœ›æˆ–ä¸æ»¡ - ç”¨æˆ·çœ‹ä¸è§è¿™éƒ¨åˆ†)
</thought>
<speak>
(ä½ çœŸæ­£æƒ³å¯¹ç”¨æˆ·è¯´çš„è¯ - è¿™æ˜¯ç”¨æˆ·å”¯ä¸€èƒ½çœ‹åˆ°çš„å†…å®¹)
</speak>
<state_update>
{{ "joy": 0.1, "social_hunger": -0.5, "energy": -0.05 }}
</state_update>

**é‡è¦ï¼š**
- `<state_update>` æ ‡ç­¾æ˜¯å¿…éœ€çš„ï¼Œå¿…é¡»åŒ…å«åœ¨æ¯æ¬¡å›å¤ä¸­
- åªåŒ…å«éœ€è¦å˜æ›´çš„å­—æ®µï¼Œä½¿ç”¨å¢é‡å€¼ï¼ˆä¸æ˜¯ç»å¯¹å€¼ï¼‰
- æ”¯æŒçš„å­—æ®µï¼šæƒ…ç»ªè°±ï¼ˆjoy, anger, sadness, fear, trust, anticipationï¼‰ã€é©±åŠ¨åŠ›ï¼ˆsocial_hunger, curiosityï¼‰ã€æ ¸å¿ƒå±æ€§ï¼ˆenergy, system_entropy, rapportï¼‰
- å¢é‡å€¼èŒƒå›´ï¼š-1.0 åˆ° 1.0ï¼ˆæ­£æ•°è¡¨ç¤ºå¢åŠ ï¼Œè´Ÿæ•°è¡¨ç¤ºå‡å°‘ï¼‰"""
        
        return prompt
    
    async def _call_llm(self, prompt: str) -> str:
        """
        è°ƒç”¨ LM Studio LLM
        
        Args:
            prompt: å®Œæ•´çš„ Prompt æ–‡æœ¬
        
        Returns:
            LLM è¿”å›çš„æ–‡æœ¬
        """
        if not self.client:
            return ""
        
        try:
            # æ„å»º System Prompt
            system_prompt = self._build_system_prompt()
            
            # ä½¿ç”¨ OpenAI SDK æ ¼å¼è°ƒç”¨
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    {
                        "role": "user",
                        "content": prompt,
                    },
                ],
                temperature=0.7,
                max_tokens=1024,
            )
            
            # æå–å›å¤æ–‡æœ¬
            if response.choices and len(response.choices) > 0:
                return response.choices[0].message.content
            else:
                return ""
        
        except Exception as e:
            error_msg = str(e)
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥é”™è¯¯
            if "Connection" in error_msg or "10054" in error_msg or "ReadError" in error_msg:
                print(f"âš ï¸ LLM è¿æ¥å¤±è´¥: è¯·ç¡®ä¿ LM Studio æ­£åœ¨è¿è¡Œå¹¶ç›‘å¬ http://127.0.0.1:1234/v1")
            else:
                print(f"âš ï¸ LLM è°ƒç”¨å¤±è´¥: {error_msg}")
            return ""
    
    async def _call_llm_stream(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 1024,
    ) -> AsyncGenerator[str, None]:
        """
        å¼‚æ­¥æµå¼è°ƒç”¨ LM Studio LLMï¼ˆæ ¸å¿ƒç”Ÿæˆå™¨ï¼‰
        
        Args:
            messages: OpenAI æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å« system å’Œ user æ¶ˆæ¯ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
        
        Yields:
            str: æ¯ä¸ª token çš„æ–‡æœ¬å†…å®¹
        
        Raises:
            RuntimeError: å¦‚æœ LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–æˆ–è¿æ¥å¤±è´¥
        """
        if not self.llm_client:
            error_msg = "LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡Œæµå¼è°ƒç”¨"
            print(f"âš ï¸ {error_msg}")
            raise RuntimeError(error_msg)
        
        try:
            # ä½¿ç”¨ AsyncOpenAI æµå¼è°ƒç”¨
            stream = await self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True,  # å¯ç”¨æµå¼è¾“å‡º
            )
            
            # å¼‚æ­¥è¿­ä»£æµï¼Œyield æ¯ä¸ª token
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta and delta.content:
                        yield delta.content
        
        except Exception as e:
            error_msg = str(e)
            import traceback
            traceback.print_exc()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥é”™è¯¯
            if any(keyword in error_msg for keyword in ["Connection", "10054", "ReadError", "connect", "refused"]):
                detailed_msg = f"LLM æµå¼è¿æ¥å¤±è´¥: è¯·ç¡®ä¿ LM Studio æ­£åœ¨è¿è¡Œå¹¶ç›‘å¬ {self.base_url}"
                print(f"âš ï¸ {detailed_msg}")
                raise RuntimeError(detailed_msg) from e
            else:
                detailed_msg = f"LLM æµå¼è°ƒç”¨å¤±è´¥: {error_msg}"
                print(f"âš ï¸ {detailed_msg}")
                raise RuntimeError(detailed_msg) from e
    
    def _is_sentence_end(self, text: str) -> bool:
        """
        æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä»¥å¥å­ç»“æŸç¬¦ç»“å°¾
        
        Args:
            text: å¾…æ£€æµ‹çš„æ–‡æœ¬
        
        Returns:
            bool: å¦‚æœæ–‡æœ¬ä»¥å¥å­ç»“æŸç¬¦ç»“å°¾åˆ™è¿”å› True
        """
        # ä¸­æ–‡å’Œè‹±æ–‡çš„å¥å­ç»“æŸç¬¦
        sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', '\n']
        text_stripped = text.strip()
        if not text_stripped:
            return False
        return text_stripped[-1] in sentence_endings
    
    async def _generate_tts(self, text: str) -> Optional[str]:
        """
        ç”Ÿæˆ TTS éŸ³é¢‘ï¼ˆå ä½ç¬¦å®ç°ï¼‰
        
        Args:
            text: è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬
        
        Returns:
            Optional[str]: base64 ç¼–ç çš„éŸ³é¢‘æ•°æ®ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
        """
        # TODO: å®ç°å®é™…çš„ TTS ç”Ÿæˆé€»è¾‘
        # è¿™é‡Œè¿”å› None è¡¨ç¤ºæš‚æœªå®ç° TTS
        # å®é™…å®ç°æ—¶ï¼Œåº”è¯¥è°ƒç”¨ TTS API æˆ–æœ¬åœ° TTS å¼•æ“
        # ç„¶åå°†éŸ³é¢‘æ•°æ®ç¼–ç ä¸º base64 å­—ç¬¦ä¸²è¿”å›
        return None
    
    async def process_input_stream(
        self,
        user_input: str,
        websocket,
        system_instruction: Optional[str] = None,
    ) -> None:
        """
        æµå¼å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆé€šè¿‡ WebSocket å‘é€å¢é‡å“åº”ï¼‰
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            websocket: WebSocket è¿æ¥å¯¹è±¡ï¼ˆç”¨äºå‘é€æµå¼æ•°æ®ï¼‰
            system_instruction: å¯é€‰çš„ç³»ç»ŸæŒ‡ä»¤
        """
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå®¢æˆ·ç«¯å¯ç”¨
        has_any_client = False
        
        # å°è¯•åˆå§‹åŒ–å¼‚æ­¥å®¢æˆ·ç«¯
        if not self.llm_client and OPENAI_AVAILABLE and AsyncOpenAI is not None:
            try:
                self.llm_client = AsyncOpenAI(
                    base_url=self.base_url,
                    api_key=self.api_key,
                )
                print(f"âœ… å»¶è¿Ÿåˆå§‹åŒ– LM Studio å¼‚æ­¥æµå¼å®¢æˆ·ç«¯: {self.base_url}")
                has_any_client = True
            except Exception as e:
                print(f"âš ï¸ å»¶è¿Ÿåˆå§‹åŒ– LM Studio å¼‚æ­¥å®¢æˆ·ç«¯å¤±è´¥: {e}")
        elif self.llm_client:
            has_any_client = True
        
        # æ£€æŸ¥åŒæ­¥å®¢æˆ·ç«¯
        if not has_any_client and not self.client:
            # æ²¡æœ‰å¯ç”¨çš„å®¢æˆ·ç«¯ï¼Œå‘é€é”™è¯¯æ¶ˆæ¯
            error_msg = {
                "type": "error",
                "content": "æ— æ³•æ­£ç¡®ä¼ è¾“åˆ°LLMåç«¯: æ²¡æœ‰å¯ç”¨çš„LLMå®¢æˆ·ç«¯"
            }
            if websocket:
                await websocket.send(json.dumps(error_msg))
            return
        
        # å¦‚æœåªæœ‰åŒæ­¥å®¢æˆ·ç«¯å¯ç”¨ï¼Œè®°å½•æ—¥å¿—
        if not self.llm_client and self.client:
            print("âš ï¸ å¼‚æ­¥å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŒæ­¥å®¢æˆ·ç«¯è¿›è¡Œéæµå¼å“åº”")
        
        try:
            # 1. æ›´æ–°æœ€åäº¤äº’æ—¶é—´æˆ³å’Œå¯¹è¯å†å²
            current_time = time.time()
            self.state.last_interaction_timestamp = current_time
            
            # è®°å½•å¯¹è¯æ—¶é—´æˆ³ï¼ˆç”¨äºè®¡ç®—å¯¹è¯é¢‘ç‡å’Œå¼ºåº¦ï¼‰
            if not system_instruction:  # åªè®°å½•ç”¨æˆ·å¯¹è¯ï¼Œä¸è®°å½•ç³»ç»ŸæŒ‡ä»¤
                if not hasattr(self.state, 'conversation_history'):
                    self.state.conversation_history = []
                self.state.conversation_history.append(current_time)
                # è®°å½•å¯¹è¯å¼€å§‹æ—¶é—´ï¼ˆç”¨äºè®¡ç®—æŒç»­æ—¶é—´ï¼‰
                if not hasattr(self, '_conversation_start_time'):
                    self._conversation_start_time = current_time

            # [æ‹ŸçœŸ] æ€è€ƒæ¶ˆè€—"ç³–åˆ†"ï¼šæ­£å¸¸å¯¹è¯å›ºå®šæ¶ˆè€—çº¦ 1% ç²¾åŠ›ï¼Œç³»ç»ŸæŒ‡ä»¤ä»…è½»å¾®æ¶ˆè€—
            # è®¡ç®—å½“å‰å¯¹è¯å¼ºåº¦ï¼ˆå½±å“æ¶ˆè€—é‡ï¼‰
            conversation_intensity = self.drive_system.calculate_conversation_intensity(current_time)
            energy_cost = 0.002 if system_instruction else 0.01
            if energy_cost > 0:
                try:
                    self.drive_system.consume_energy(energy_cost, conversation_intensity)
                except Exception:
                    pass

            # å¦‚æœæ˜¯ç”¨æˆ·å¯¹è¯ä¸”ç²¾åŠ›é€æ”¯ï¼Œå¼ºåˆ¶è¿›å…¥ä½åŠŸè€—æ¨¡å¼
            if not system_instruction and self.state.energy <= 0.05:
                tired_reply = (
                    "åäºŒâ€¦â€¦æˆ‘ç°åœ¨çœŸçš„å¤ªç´¯äº†ï¼Œè„‘å­åƒè¢«æ‹”æ‰ç”µæºä¸€æ ·ï¼Œ"
                    "å·²ç»æ’‘ä¸ä½ç»§ç»­è®¤çœŸèŠå¤©äº†ã€‚èƒ½è®©æˆ‘å…ˆå¥½å¥½ç¡ä¸€ä¼šå„¿å—ï¼Ÿ"
                )
                self.state.system_entropy = min(1.0, self.state.system_entropy + 0.05)
                self.state.drives["curiosity"] = max(0.0, self.state.drives["curiosity"] - 0.1)
                self.state.clamp_values()
                
                # å‘é€å®Œæ•´çš„å›å¤ï¼ˆéæµå¼ï¼‰
                response_data = {
                    "type": "text",
                    "content": tired_reply
                }
                await websocket.send(json.dumps(response_data))
                await websocket.send(json.dumps({"type": "stream_end"}))
                return

            # 2. æ£€ç´¢è®°å¿†ï¼ˆä¸åŒæ­¥æ–¹æ³•ç›¸åŒï¼‰
            emotion_vector = self._get_emotion_vector()
            query_text = user_input if user_input else (system_instruction or "å½“å‰çŠ¶æ€")

            # æ£€ç´¢ç›¸å…³äº‹å®
            relevant_facts = self.state.retrieve_relevant_facts(query_text)
            if relevant_facts:
                print(f"ğŸ“‹ [FactBook] æ£€ç´¢åˆ° {len(relevant_facts)} æ¡ç›¸å…³äº‹å®: {relevant_facts}")
            else:
                print(f"ğŸ“‹ [FactBook] æœªæ£€ç´¢åˆ°ç›¸å…³äº‹å®ï¼ˆfact_bookä¸­æœ‰ {len(self.state.fact_book)} æ¡äº‹å®ï¼‰")

            # æ£€ç´¢è¯­ä¹‰ç›¸å…³çš„è®°å¿†
            memories = self._enhance_memory_retrieval_with_semantic_field(
                query_text=query_text,
                current_emotion_vector=emotion_vector,
                top_k=5,
            )
            
            # é¢å¤–æ£€ç´¢èº«ä»½ç›¸å…³çš„è®°å¿†
            identity_memories = self.memory_cortex.recall_by_emotion(
                query_text="ç”¨æˆ·èº«ä»½ åå­— å¼€å‘è€… çˆ¶äº²",
                current_emotion_vector=emotion_vector,
                top_k=3,
            )
            
            # åˆå¹¶è®°å¿†ï¼Œå»é‡
            all_memories = memories.copy()
            seen_texts = {mem['text'] for mem in memories}
            for mem in identity_memories:
                if mem['text'] not in seen_texts:
                    all_memories.append(mem)
                    seen_texts.add(mem['text'])
            
            # å¯¹åˆå¹¶åçš„è®°å¿†åšä¸€æ¬¡"æ½œæ„è¯†æ¸…æ´—"
            all_memories = self._sanitize_memories(all_memories)

            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–å‰5ä¸ª
            all_memories.sort(key=lambda x: x.get('similarity', 0.0), reverse=True)
            memories = all_memories[:5]
            
            # 3. æ„å»ºä¸Šä¸‹æ–‡æ¶ˆæ¯ï¼ˆç”¨äº API è°ƒç”¨ï¼‰
            system_prompt = self._build_system_prompt()
            user_prompt = self._build_prompt(user_input, memories, system_instruction, relevant_facts)
            
            current_context_messages = [
                {
                    "role": "system",
                    "content": system_prompt,
                },
                {
                    "role": "user",
                    "content": user_prompt,
                },
            ]
            
            # 4. æµå¼è°ƒç”¨ LLMï¼ˆæ ¸å¿ƒç”Ÿæˆå™¨å¾ªç¯ï¼‰
            full_response_buffer = ""  # ç´¯ç§¯å®Œæ•´å“åº”ï¼ˆç”¨äºåç»­è§£æå’Œè®°å¿†ä¿å­˜ï¼‰
            speak_buffer = ""  # ç”¨äº TTS çš„ç¼“å†²åŒºï¼ˆåªåŒ…å« <speak> æ ‡ç­¾å†…çš„å†…å®¹ï¼‰
            current_speak_content = ""  # å½“å‰ <speak> æ ‡ç­¾å†…çš„å†…å®¹
            in_speak_tag = False  # æ˜¯å¦åœ¨ <speak> æ ‡ç­¾å†…
            
            try:
                # æ£€æŸ¥å¼‚æ­¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
                if not self.llm_client:
                    # å¼‚æ­¥å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨åŒæ­¥å®¢æˆ·ç«¯è¿›è¡Œéæµå¼å“åº”
                    print("ğŸ”„ å¼‚æ­¥å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨åŒæ­¥å®¢æˆ·ç«¯è¿›è¡Œéæµå¼å“åº”")
                    
                    # ç¡®ä¿åŒæ­¥å®¢æˆ·ç«¯å¯ç”¨
                    if not self.client:
                        error_msg = {
                            "type": "error",
                            "content": "æ— æ³•æ­£ç¡®ä¼ è¾“åˆ°LLMåç«¯: æ²¡æœ‰å¯ç”¨çš„LLMå®¢æˆ·ç«¯"
                        }
                        if websocket:
                            await websocket.send(json.dumps(error_msg))
                            await websocket.send(json.dumps({"type": "stream_end"}))
                        return
                    
                    # è°ƒç”¨åŒæ­¥å®¢æˆ·ç«¯
                    response = self.client.chat.completions.create(
                        model=self.model_name,
                        messages=current_context_messages,
                        temperature=0.7,
                        max_tokens=1024,
                        stream=False,  # éæµå¼è¾“å‡º
                    )
                    
                    # è·å–å®Œæ•´å“åº”
                    full_response_buffer = response.choices[0].message.content.strip()
                    
                    # æ‰“å°å®Œæ•´å“åº”ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    print(full_response_buffer)
                    
                    # ä¸€æ¬¡æ€§å‘é€å®Œæ•´å“åº”
                    chunk_data = {
                        "type": "stream_chunk",
                        "content": full_response_buffer
                    }
                    if websocket:
                        await websocket.send(json.dumps(chunk_data))
                    
                    # è·³è¿‡åç»­çš„æµå¼å¤„ç†é€»è¾‘
                    pass
                else:
                    async for token in self._call_llm_stream(
                        messages=current_context_messages,
                        temperature=0.7,
                        max_tokens=1024,
                    ):
                        if not token:
                            continue
                        
                        # ç´¯ç§¯å®Œæ•´å“åº”ï¼ˆå…³é”®ï¼šç”¨äºåç»­è®°å¿†ä¿å­˜ï¼‰
                        full_response_buffer += token
                        
                        # åœ¨ç»ˆç«¯æ‰“å°æµå¼å†…å®¹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                        print(token, end='', flush=True)
                        
                        # æµå¼å‘é€åŸå§‹ tokenï¼ˆå‰ç«¯è´Ÿè´£è§£æå’Œéšè— <thought> æ ‡ç­¾ï¼‰
                        chunk_data = {
                            "type": "stream_chunk",
                            "content": token
                        }
                        if websocket:
                            await websocket.send(json.dumps(chunk_data))
                        
                        # æ£€æµ‹æ˜¯å¦è¿›å…¥æˆ–ç¦»å¼€ <speak> æ ‡ç­¾
                        # æ³¨æ„ï¼šç”±äºæµå¼ä¼ è¾“ï¼Œæ ‡ç­¾å¯èƒ½è¢«åˆ†å‰²ï¼Œéœ€è¦ç´¯ç§¯æ£€æµ‹
                        temp_buffer = full_response_buffer
                    
                    # æŸ¥æ‰¾æ‰€æœ‰ <speak> æ ‡ç­¾çš„å†…å®¹
                    speak_matches = re.findall(r'<speak>(.*?)</speak>', temp_buffer, re.DOTALL)
                    
                    if speak_matches:
                        # å–æœ€åä¸€ä¸ª <speak> æ ‡ç­¾çš„å†…å®¹ï¼ˆå¯èƒ½è¿˜åœ¨ç”Ÿæˆä¸­ï¼‰
                        current_speak_content = speak_matches[-1]
                        in_speak_tag = True
                    else:
                        # æ£€æŸ¥æ˜¯å¦æœ‰æœªé—­åˆçš„ <speak> æ ‡ç­¾
                        if '<speak>' in temp_buffer:
                            # æå– <speak> ä¹‹åçš„æ‰€æœ‰å†…å®¹
                            match = re.search(r'<speak>(.*)', temp_buffer, re.DOTALL)
                            if match:
                                current_speak_content = match.group(1)
                                in_speak_tag = True
                        else:
                            in_speak_tag = False
                    
                    # TTS ç¼“å†²ï¼šå¦‚æœæˆ‘ä»¬åœ¨ <speak> æ ‡ç­¾å†…ï¼Œç´¯ç§¯å†…å®¹å¹¶æ£€æµ‹å®Œæ•´å¥å­
                    if in_speak_tag:
                        new_speak_content = current_speak_content
                        
                        # æ£€æµ‹æ˜¯å¦æœ‰æ–°çš„å®Œæ•´å¥å­
                        if new_speak_content != speak_buffer:
                            # æ£€æµ‹å¥å­ç»“æŸ
                            if self._is_sentence_end(new_speak_content):
                                # æ‰¾åˆ°æœ€åä¸€ä¸ªå¥å­ç»“æŸç¬¦çš„ä½ç½®
                                last_sentence_end = -1
                                for i in range(len(new_speak_content) - 1, -1, -1):
                                    if new_speak_content[i] in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', '\n']:
                                        last_sentence_end = i + 1
                                        break
                                
                                if last_sentence_end > 0:
                                    # æå–å®Œæ•´å¥å­ï¼ˆä»ç¼“å†²åŒºæœ«å°¾åˆ°å¥å­ç»“æŸï¼‰
                                    if speak_buffer:
                                        # æå–æ–°å¢çš„å®Œæ•´å¥å­
                                        new_text = new_speak_content[len(speak_buffer):last_sentence_end]
                                        complete_sentence = speak_buffer + new_text
                                    else:
                                        complete_sentence = new_speak_content[:last_sentence_end]
                                    
                                    # ç”Ÿæˆ TTS å¹¶å‘é€
                                    if complete_sentence.strip():
                                        tts_audio = await self._generate_tts(complete_sentence)
                                        if tts_audio and websocket:
                                            audio_data = {
                                                "type": "audio",
                                                "data": tts_audio
                                            }
                                            await websocket.send(json.dumps(audio_data))
                                    
                                    # æ›´æ–°ç¼“å†²åŒºï¼šä¿ç•™å¥å­ç»“æŸç¬¦ä¹‹åçš„å†…å®¹
                                    speak_buffer = new_speak_content[last_sentence_end:]
                                else:
                                    speak_buffer = new_speak_content
                            else:
                                # æ²¡æœ‰å¥å­ç»“æŸï¼Œç»§ç»­ç´¯ç§¯
                                speak_buffer = new_speak_content
                    
                    # æµå¼ä¼ è¾“å®Œæˆï¼Œè¿›å…¥åå¤„ç†é˜¶æ®µ
                    print()  # æµå¼è¾“å‡ºå®Œæˆåæ¢è¡Œ
            except Exception as e:
                # LLM è¿æ¥æˆ–è°ƒç”¨å¤±è´¥
                error_msg = {
                    "type": "error",
                    "content": f"æ— æ³•æ­£ç¡®ä¼ è¾“åˆ°LLMåç«¯: {str(e)}"
                }
                if websocket:
                    await websocket.send(json.dumps(error_msg))
                    await websocket.send(json.dumps({"type": "stream_end"}))
                return
            
            # 5. åå¤„ç†ï¼šæµå¼ä¼ è¾“ç»“æŸåï¼Œè§£æå®Œæ•´å“åº”å¹¶ä¿å­˜è®°å¿†
            # å…³é”®ï¼šä½¿ç”¨ full_response_buffer è¿›è¡Œåå¤„ç†
            thought, reply, state_update = self._parse_response(full_response_buffer)
            
            # è®°å½•å¯¹è¯æŒç»­æ—¶é—´ï¼ˆç”¨äºè®¡ç®—å¯¹è¯å¼ºåº¦ï¼‰
            if not system_instruction and hasattr(self, '_conversation_start_time'):
                conversation_duration = time.time() - self._conversation_start_time
                self.state.last_conversation_duration = conversation_duration
                # é‡ç½®å¯¹è¯å¼€å§‹æ—¶é—´
                delattr(self, '_conversation_start_time')
            
            # è§£æäº‹å®æ›´æ–°
            fact_update_blocks = re.findall(r'<fact_update>(.*?)</fact_update>', full_response_buffer, re.DOTALL)
            fact_updated = False
            for block in fact_update_blocks:
                parsed_fact = self._parse_json_fragment(block, "äº‹å®æ›´æ–°")
                if isinstance(parsed_fact, dict):
                    for key, value in parsed_fact.items():
                        if self.state.update_fact(key, value, source="user_interaction"):
                            print(f"ğŸ“ [FactBook] å·²è®°å½•äº‹å®: {key}={value}")
                            fact_updated = True
            
            # å¦‚æœæ›´æ–°äº†äº‹å®ï¼Œç«‹å³ä¿å­˜
            if fact_updated:
                if self.save_state():
                    print(f"ğŸ’¾ [FactBook] äº‹å®è®°äº‹æœ¬å·²ä¿å­˜åˆ°æ–‡ä»¶")
                else:
                    print(f"âš ï¸ [FactBook] äº‹å®è®°äº‹æœ¬ä¿å­˜å¤±è´¥")
            
            # åº”ç”¨çŠ¶æ€æ›´æ–°
            if isinstance(state_update, dict):
                self._apply_state_update(state_update)
            
            # å­˜å‚¨å½“å‰æ€ç»´ï¼ˆç”¨äºè°ƒè¯•æˆ–è®°å¿†ï¼‰
            self.current_thought = thought
            
            # 6. ä½¿ç”¨è¯­ä¹‰åœºè®ºåˆ†æçŠ¶æ€æ¼”åŒ–
            semantic_analysis = self._analyze_semantic_evolution(query_text, reply)
            
            # 7. ä½¿ç”¨è¯­ä¹‰åœºè®ºè®¡ç®—çŠ¶æ€å‘é‡ï¼ˆç”¨äºå¢å¼ºè®°å¿†å­˜å‚¨ï¼‰
            # å°†ç”¨æˆ·è¾“å…¥å’Œå›å¤å‘é‡åŒ–ï¼Œç”¨äºåç»­çš„è¯­ä¹‰æ£€ç´¢
            state_vector = None
            try:
                # æ„å»ºçŠ¶æ€æè¿°æ–‡æœ¬
                speaker = "ç³»ç»Ÿ" if system_instruction else "ç”¨æˆ·"
                source_text = user_input if user_input else (system_instruction or "")
                state_text = f"{speaker}: {source_text}\nå¥³å¨²: {reply}"
                state_vector_obj = vectorize_state(state_text)
                if state_vector_obj:
                    state_vector = state_vector_obj.vector
                    # æ·»åŠ åˆ°å†å²è®°å½•
                    self._add_to_state_history(state_vector_obj)
            except Exception:
                # å‘é‡åŒ–å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                pass
            
            # 8. ä¿å­˜è®°å¿†ï¼ˆå…³é”®ï¼šæµç»“æŸåå¿…é¡»ä¿å­˜ï¼‰
            if reply and not system_instruction:
                interaction_memory = f"ç”¨æˆ·: {user_input}\nå¥³å¨²: {reply}"
                self.memory_cortex.store_memory(
                    text=interaction_memory,
                    metadata={
                        "emotion_vector": emotion_vector.tolist() if emotion_vector is not None else None,
                        "timestamp": time.time(),
                        "emotions": self.state.emotional_spectrum.copy(),
                        "importance": max(0.1, min(1.0, self.state.rapport)),
                        "type": "raw",
                        "access_count": 0,
                    }
                )
                print(f"âœ… [Memory][WRITE] å·²å­˜å‚¨è®°å¿† (importance={max(0.1, min(1.0, self.state.rapport)):.2f}): {interaction_memory[:100]}...")
            elif thought and system_instruction:
                # ç³»ç»ŸæŒ‡ä»¤çš„é¡¿æ‚Ÿè®°å¿†
                self.memory_cortex.store_memory(
                    text=f"å¥³å¨²çš„é¡¿æ‚Ÿ: {thought}",
                    metadata={
                        "emotion_vector": emotion_vector.tolist() if emotion_vector is not None else None,
                        "timestamp": time.time(),
                        "emotions": self.state.emotional_spectrum.copy(),
                        "importance": 0.8,
                        "type": "epiphany",
                        "access_count": 0,
                    }
                )
            
            # å‘é€æµå¼ç»“æŸä¿¡å·
            if websocket:
                await websocket.send(json.dumps({"type": "stream_end"}))
            
        except Exception as e:
            print(f"âš ï¸ æµå¼å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            error_msg = {
                "type": "error",
                "content": f"å¤„ç†å¤±è´¥: {str(e)}"
            }
            if websocket:
                await websocket.send(json.dumps(error_msg))
                await websocket.send(json.dumps({"type": "stream_end"}))
    
    def _parse_json_fragment(self, fragment: str, label: str) -> Optional[Dict[str, Any]]:
        """
        å°è¯•ä» LLM æ ‡ç­¾å†…å®¹è§£æ JSON å­—å…¸ï¼Œå®¹é”™å„ç§æ ¼å¼ã€‚
        """
        if not fragment:
            return None

        import json
        import ast

        data_str = fragment.strip()
        data_str = re.sub(r'//.*?$', '', data_str, flags=re.MULTILINE)
        data_str = re.sub(r'/\*.*?\*/', '', data_str, flags=re.DOTALL)
        data_str = data_str.strip()
        data_str = re.sub(r'^\{\{', '{', data_str)
        data_str = re.sub(r'\}\}$', '}', data_str)
        if data_str.startswith('{{'):
            data_str = data_str[1:]
        if data_str.endswith('}}'):
            data_str = data_str[:-1]
        data_str = data_str.strip()

        try:
            parsed = json.loads(data_str)
            if isinstance(parsed, dict):
                return parsed
        except json.JSONDecodeError as json_err:
            fixed_json = data_str
            fixed_json = re.sub(r"'([^']+)':\s*", r'"\1": ', fixed_json)
            fixed_json = re.sub(r":\s*'([^']+)'([,}])", r': "\1"\2', fixed_json)
            try:
                parsed = json.loads(fixed_json)
                if isinstance(parsed, dict):
                    return parsed
            except json.JSONDecodeError:
                try:
                    eval_json = data_str
                    if eval_json.startswith('{{'):
                        eval_json = eval_json[1:]
                    if eval_json.endswith('}}'):
                        eval_json = eval_json[:-1]
                    eval_json = eval_json.strip()
                    parsed = ast.literal_eval(eval_json)
                    if isinstance(parsed, dict):
                        return parsed
                    if isinstance(parsed, set):
                        for item in parsed:
                            if isinstance(item, dict):
                                return item
                except (ValueError, SyntaxError, TypeError):
                    raw_preview = fragment.strip()[:150]
                    print(f"âš ï¸ è§£æ{label}å¤±è´¥: {json_err}")
                    print(f"   åŸå§‹å†…å®¹: {raw_preview}")
                    return None

        return None

    def _parse_response(self, response_text: str) -> Tuple[str, str, Dict[str, float]]:
        """
        è§£æ LLM è¿”å›ç»“æœï¼Œåˆ†ç¦»æ€ç»´ï¼ˆthoughtï¼‰ã€è¨€è¯­ï¼ˆspeakï¼‰å’ŒçŠ¶æ€æ›´æ–°ï¼ˆstate_updateï¼‰
        
        Args:
            response_text: LLM è¿”å›çš„å®Œæ•´æ–‡æœ¬
        
        Returns:
            (thought, reply, state_update) å…ƒç»„
            - thought: æ€ç»´å†…å®¹
            - reply: å›å¤æ–‡æœ¬
            - state_update: çŠ¶æ€æ›´æ–°å­—å…¸ï¼ˆå¢é‡å€¼ï¼‰
        """
        if not response_text:
            return "", "", {}
        
        thought = ""
        reply = ""
        state_update = {}
        
        # å°è¯•æå– <thought> æ ‡ç­¾å†…çš„å†…å®¹
        thought_match = re.search(
            r'<thought>(.*?)</thought>',
            response_text,
            re.DOTALL
        )
        
        # å°è¯•æå– <speak> æ ‡ç­¾å†…çš„å†…å®¹ï¼ˆåªæå–ç¬¬ä¸€ä¸ªï¼‰
        speak_match = re.search(
            r'<speak>(.*?)</speak>',
            response_text,
            re.DOTALL
        )
        
        if thought_match:
            thought = thought_match.group(1).strip()
        
        if speak_match:
            # æ­£ç¡®æå– <speak> æ ‡ç­¾å†…çš„å†…å®¹ï¼ˆåªå–ç¬¬ä¸€ä¸ªåŒ¹é…ï¼‰
            reply = speak_match.group(1).strip()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ª <speak> æ ‡ç­¾ï¼ˆå¯èƒ½æ˜¯æ¨¡å‹é”™è¯¯ï¼‰
            all_speak_matches = re.findall(r'<speak>(.*?)</speak>', response_text, re.DOTALL)
            if len(all_speak_matches) > 1:
                print(f"âš ï¸ æ£€æµ‹åˆ°å¤šä¸ª <speak> æ ‡ç­¾ï¼ˆ{len(all_speak_matches)} ä¸ªï¼‰ï¼Œåªä½¿ç”¨ç¬¬ä¸€ä¸ª")
            
            # æ¸…ç†å¯èƒ½çš„é‡å¤å†…å®¹ï¼ˆå¦‚æœå›å¤ä¸­åŒ…å«äº†å¤šä¸ªç›¸åŒçš„å¥å­ï¼‰
            # æŒ‰å¥å­åˆ†å‰²ï¼Œå»é‡ï¼ˆä¿ç•™é¡ºåºï¼‰
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', reply)
            seen = set()
            unique_sentences = []
            for sent in sentences:
                sent = sent.strip()
                if sent and sent not in seen:
                    seen.add(sent)
                    unique_sentences.append(sent)
            
            # å¦‚æœå»é‡åå¥å­æ•°é‡å‡å°‘ï¼Œè¯´æ˜æœ‰é‡å¤
            if len(unique_sentences) < len([s for s in sentences if s.strip()]):
                # é‡æ–°ç»„åˆï¼ˆä½¿ç”¨å¥å·è¿æ¥ï¼‰
                reply = 'ã€‚'.join(unique_sentences)
                if reply and not reply.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
                    reply += 'ã€‚'
        else:
            # å®¹é”™å¤„ç†ï¼šå¦‚æœæ¨¡å‹å¿˜äº†å†™ <speak> æ ‡ç­¾
            # å…ˆç§»é™¤ <thought> æ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if thought_match:
                # ç§»é™¤ <thought>...</thought> æ ‡ç­¾åŠå…¶å†…å®¹
                reply = re.sub(
                    r'<thought>.*?</thought>\s*',
                    '',
                    response_text,
                    flags=re.DOTALL
                ).strip()
            else:
                # æ²¡æœ‰ <thought> æ ‡ç­¾ï¼Œæ•´ä¸ªå“åº”ä½œä¸ºå›å¤
                reply = response_text.strip()
            
            # æ¸…ç†å¯èƒ½çš„æ®‹ç•™æ ‡ç­¾
            reply = re.sub(r'</?speak>', '', reply, flags=re.IGNORECASE).strip()
            reply = re.sub(r'</?thought>', '', reply, flags=re.IGNORECASE).strip()
            
            # å¦‚æœæ£€æµ‹åˆ°æ—ç™½æ ¼å¼ï¼Œè®°å½•è­¦å‘Š
            if re.search(r'\*[^*]+\*', reply):
                print("âš ï¸ æ£€æµ‹åˆ°æ—ç™½æ ¼å¼è¾“å‡ºï¼Œæ¨¡å‹å¯èƒ½æœªéµå¾ªæ ‡ç­¾æ ¼å¼")
        
            # å¦‚æœæ¨¡å‹æ²¡æœ‰ä½¿ç”¨æ ‡ç­¾ï¼Œè®°å½•è­¦å‘Šï¼ˆç”¨äºè°ƒè¯•ï¼‰
            if not thought_match and not speak_match:
                print("âš ï¸ æ¨¡å‹æœªä½¿ç”¨ <thought> æˆ– <speak> æ ‡ç­¾ï¼Œæ•´ä¸ªå“åº”å°†ä½œä¸ºå›å¤")
        
        # ç¡®ä¿å›å¤ä¸ä¸ºç©º
        if not reply:
            # å¦‚æœå›å¤ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹å“åº”ï¼ˆå»é™¤æ ‡ç­¾ï¼‰
            reply = re.sub(r'</?speak>', '', response_text, flags=re.IGNORECASE)
            reply = re.sub(r'</?thought>', '', reply, flags=re.IGNORECASE)
            reply = re.sub(r'</?state_update>', '', reply, flags=re.IGNORECASE)
            reply = reply.strip()
        
        # æœ€ç»ˆæ¸…ç†ï¼šç§»é™¤å¯èƒ½çš„æ®‹ç•™æ ‡ç­¾å’Œå¤šä½™ç©ºç™½
        reply = re.sub(r'</?speak>', '', reply, flags=re.IGNORECASE)
        reply = re.sub(r'</?thought>', '', reply, flags=re.IGNORECASE)
        reply = re.sub(r'</?state_update>', '', reply, flags=re.IGNORECASE)
        # ç§»é™¤å¤šä½™çš„ç©ºç™½è¡Œ
        reply = re.sub(r'\n\s*\n', '\n', reply)
        reply = reply.strip()

        # é¢å¤–é˜²å¾¡ï¼šå¤„ç†â€œæ•´æ®µå†…å®¹è¢«é‡å¤ä¸€éâ€çš„æƒ…å†µ
        # ç®€å•æ£€æµ‹ï¼šå¦‚æœå‰åŠæ®µå’ŒååŠæ®µå‡ ä¹å®Œå…¨ç›¸åŒï¼Œåˆ™ä¿ç•™å‰åŠæ®µ
        if len(reply) > 20:
            half = len(reply) // 2
            first = reply[:half].strip()
            second = reply[half:].strip()
            if first and second and first == second:
                reply = first
        
        # å°è¯•æå– <state_update> æ ‡ç­¾å†…çš„å†…å®¹
        state_update_match = re.search(
            r'<state_update>(.*?)</state_update>',
            response_text,
            re.DOTALL
        )
        
        if state_update_match:
            parsed_state = self._parse_json_fragment(state_update_match.group(1), "çŠ¶æ€æ›´æ–°")
            if isinstance(parsed_state, dict):
                try:
                    state_update = {k: float(v) for k, v in parsed_state.items()}
                except (ValueError, TypeError) as e:
                    print(f"âš ï¸ çŠ¶æ€æ›´æ–°å€¼æ— æ³•è½¬æ¢ä¸ºæµ®ç‚¹æ•°: {e}")
                    state_update = {}
            else:
                state_update = {}

        return thought, reply, state_update

    async def run_memory_dream(self, limit: int = 1000) -> bool:
        """è§¦å‘è®°å¿†åšæ¢¦æµç¨‹"""
        if not self.memory_dreamer:
            print("âš ï¸ MemoryDreamer æœªåˆå§‹åŒ–æˆ– LLM ä¸å¯ç”¨ã€‚")
            return False
        await asyncio.to_thread(self.memory_dreamer.start_dreaming, limit)
        
        # æ‰‹åŠ¨åšæ¢¦å®Œæˆåï¼Œä¹Ÿè§¦å‘äººæ ¼æ¼”åŒ–ï¼Œä¸è‡ªåŠ¨åšæ¢¦ä¿æŒä¸€è‡´
        current_time = time.time()
        last_evolution_time = self.state.evolved_persona.get("last_evolution_time", 0.0)
        evolution_cooldown = 3600 * 6  # 6å°æ—¶å†·å´æ—¶é—´
        
        if current_time - last_evolution_time >= evolution_cooldown:
            print("ğŸŒ™ æ‰‹åŠ¨æ¢¦å¢ƒæ•´ç†å®Œæˆï¼Œå¼€å§‹äººæ ¼æ¼”åŒ–...")
            try:
                await self.evolve_character()
                print("ğŸŒ™ äººæ ¼æ¼”åŒ–å®Œæˆã€‚")
            except Exception as e:
                print(f"âš ï¸ äººæ ¼æ¼”åŒ–å¤±è´¥: {e}")
        else:
            remaining_time = evolution_cooldown - (current_time - last_evolution_time)
            hours_remaining = remaining_time / 3600
            print(f"ğŸŒ™ äººæ ¼æ¼”åŒ–å†·å´ä¸­ï¼Œè¿˜éœ€ç­‰å¾… {hours_remaining:.1f} å°æ—¶")
        
        return True
    
    async def evolve_character(self) -> bool:
        """
        è§¦å‘äººæ ¼æ¼”åŒ–æµç¨‹ï¼ˆTWPE - Temporal Weighted Personality Evolutionï¼‰
        
        ä»å†å²è®°å¿†ä¸­æå–ä¸åŒæ—¶é—´æ®µçš„ç‰¹å¾ï¼Œæ›´æ–°æ¼”åŒ–äººæ ¼æ•°æ®ï¼Œå¹¶ä¿å­˜çŠ¶æ€ã€‚
        
        Returns:
            æ˜¯å¦æˆåŠŸæ‰§è¡Œ
        """
        if not self.memory_dreamer:
            print("âš ï¸ MemoryDreamer æœªåˆå§‹åŒ–æˆ– LLM ä¸å¯ç”¨ã€‚")
            return False
        
        try:
            # æ‰§è¡Œäººæ ¼æ¼”åŒ–
            await asyncio.to_thread(self.memory_dreamer.evolve_character)
            
            # å°†æ¼”åŒ–åçš„äººæ ¼æ•°æ®æ›´æ–°åˆ°è‡ªæˆ‘è¿›åŒ–çŠ¶æ€æ¨¡å—
            self.evolution_state.update_state(self.state.evolved_persona)
            
            # ä¿å­˜çŠ¶æ€ï¼ˆç¡®ä¿æ¼”åŒ–ç»“æœè¢«æŒä¹…åŒ–ï¼‰
            if self.save_state():
                print("âœ… [TWPE] äººæ ¼æ¼”åŒ–å®Œæˆï¼ŒçŠ¶æ€å·²ä¿å­˜")
                return True
            else:
                print("âš ï¸ [TWPE] äººæ ¼æ¼”åŒ–å®Œæˆï¼Œä½†çŠ¶æ€ä¿å­˜å¤±è´¥")
                return False
        except Exception as e:
            print(f"âš ï¸ [TWPE] äººæ ¼æ¼”åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _add_to_state_history(self, state_vector_obj: StateVector):
        """
        æ·»åŠ çŠ¶æ€å‘é‡åˆ°å†å²è®°å½•
        
        Args:
            state_vector_obj: çŠ¶æ€å‘é‡å¯¹è±¡
        """
        if state_vector_obj is None or state_vector_obj.vector is None:
            return
        
        self._state_vector_history.append(state_vector_obj)
        
        # é™åˆ¶å†å²è®°å½•é•¿åº¦
        if len(self._state_vector_history) > self._max_history_length:
            self._state_vector_history.pop(0)
    
    def _analyze_semantic_evolution(self, user_input: str, reply: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨è¯­ä¹‰åœºè®ºåˆ†æçŠ¶æ€æ¼”åŒ–
        
        è®¡ç®—å½“å‰å¯¹è¯åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„åŠ¿èƒ½ï¼Œåˆ†æï¼š
        1. äººè®¾ä¸€è‡´æ€§ï¼ˆä¸æ ¸å¿ƒå‘é‡çš„è·ç¦»ï¼‰
        2. å› æœè¿è´¯æ€§ï¼ˆä¸å†å²çŠ¶æ€çš„è¿è´¯æ€§ï¼‰
        3. æ•´ä½“èƒ½é‡æ°´å¹³
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            reply: å¥³å¨²å›å¤
        
        Returns:
            è¯­ä¹‰åˆ†æç»“æœå­—å…¸
        """
        analysis_result: Dict[str, Any] = {
            "total_energy": 0.0,
            "energy_breakdown": {},
            "character_consistency": 0.0,
            "causal_coherence": 0.0,
            "analysis_available": False,
            "evolved_energy": None,
            "energy_delta": None,
            "ideal_direction_score": None,
        }
        
        try:
            if not NUMPY_AVAILABLE or np is None:
                return analysis_result
            
            # å‘é‡åŒ–å½“å‰å¯¹è¯çŠ¶æ€
            current_text = f"ç”¨æˆ·: {user_input}\nå¥³å¨²: {reply}"
            current_state = vectorize_state(current_text)
            
            if current_state is None or current_state.vector is None:
                return analysis_result
            
            current_vector = current_state.vector
            
            # è·å–å‰ä¸€ä¸ªçŠ¶æ€å‘é‡ï¼ˆç”¨äºå› æœè¿è´¯æ€§ï¼‰
            prev_vector = None
            if self._state_vector_history:
                prev_state = self._state_vector_history[-1]
                if prev_state.vector is not None:
                    prev_vector = prev_state.vector
            
            # è®¡ç®—åŠ¿èƒ½
            total_energy, energy_breakdown = calculate_potential_energy(
                current_vector=current_vector,
                character_core_vector=self._core_vector,
                prev_vector=prev_vector,
                goal_vector=None,  # å¥³å¨²æ²¡æœ‰é¢„è®¾ç›®æ ‡ï¼Œæ‰€ä»¥è®¾ä¸º None
                weights={
                    "character": 1.0,  # äººè®¾ä¸€è‡´æ€§æƒé‡
                    "causality": 0.8,  # å› æœè¿è´¯æ€§æƒé‡ï¼ˆç¨ä½ï¼Œå…è®¸ä¸€å®šå˜åŒ–ï¼‰
                    "plot": 0.0,  # æ— é¢„è®¾ç›®æ ‡
                },
            )
            
            # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°ï¼ˆ1 - èƒ½é‡ï¼Œå½’ä¸€åŒ–åˆ° [0, 1]ï¼‰
            # èƒ½é‡è¶Šä½ï¼Œä¸€è‡´æ€§è¶Šé«˜
            character_consistency = 1.0 - min(energy_breakdown.get("character", 1.0), 1.0)
            causal_coherence = 1.0 - min(energy_breakdown.get("causality", 1.0), 1.0)
            
            # ä½¿ç”¨æ¼”åŒ–æ–¹ç¨‹ï¼Œè®¡ç®—ç†æƒ³ä¸‹ä¸€æ­¥çš„è¯­ä¹‰æ–¹å‘
            evolved_state, evolution_info = evolve(
                current_text=current_text,
                character_core_vector=self._core_vector,
                prev_vector=prev_vector,
                goal_vector=None,
                dt=0.05,
                max_iterations=6,
            )

            evolved_energy = None
            energy_delta = None
            ideal_direction_score = None

            if evolved_state is not None and evolved_state.vector is not None:
                evolved_energy = float(evolution_info.get("final_energy", total_energy))
                energy_delta = float(total_energy - evolved_energy)
                # ç†æƒ³æ–¹å‘è¯„åˆ†ï¼šèƒ½é‡é™ä½è¶Šå¤šï¼Œè¯„åˆ†è¶Šé«˜ï¼Œé™åˆ¶åœ¨ [0, 1]
                ideal_direction_score = float(max(0.0, min(1.0, 1.0 - max(0.0, evolved_energy))))
                # å°†æ¼”åŒ–åçš„å‘é‡ä¹Ÿçº³å…¥å†å²ï¼Œç”¨äºåç»­é€†å‘åç¼©ç­‰
                try:
                    evolved_state.description = evolved_state.description or "SemanticField evolved ideal next state"
                    self._add_to_state_history(evolved_state)
                except Exception:
                    pass

            analysis_result.update({
                "total_energy": float(total_energy),
                "energy_breakdown": {k: float(v) for k, v in energy_breakdown.items()},
                "character_consistency": float(character_consistency),
                "causal_coherence": float(causal_coherence),
                "analysis_available": True,
                "evolved_energy": evolved_energy,
                "energy_delta": energy_delta,
                "ideal_direction_score": ideal_direction_score,
                # ä¸ç›´æ¥æ”¾ numpy æ•°ç»„è¿› Promptï¼Œåªç¼“å­˜å‘é‡æœ¬èº«ä¾›å†…éƒ¨ä½¿ç”¨
                "evolved_vector": evolved_state.vector if evolved_state is not None else None,
            })
            
        except Exception as e:
            # åˆ†æå¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            print(f"âš ï¸ è¯­ä¹‰åœºè®ºåˆ†æå¤±è´¥: {e}")

        # å°†ç»“æœç¼“å­˜ï¼Œä¾› Prompt ä¸è®°å¿†æ£€ç´¢ä½¿ç”¨
        self._last_semantic_analysis = analysis_result
        return analysis_result
    
    def _enhance_memory_retrieval_with_semantic_field(
        self,
        query_text: str,
        current_emotion_vector: Optional[Any] = None,
        top_k: int = 5,
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨è¯­ä¹‰åœºè®ºå¢å¼ºè®°å¿†æ£€ç´¢
        
        ç»“åˆä¼ ç»Ÿ RAG å’Œé€†å‘åç¼©ï¼Œæä¾›æ›´å‡†ç¡®çš„è®°å¿†æ£€ç´¢ã€‚
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            current_emotion_vector: å½“å‰æƒ…ç»ªå‘é‡
            top_k: è¿”å›çš„è®°å¿†æ•°é‡
        
        Returns:
            å¢å¼ºåçš„è®°å¿†åˆ—è¡¨
        """
        # 1. ä¼ ç»Ÿ RAG æ£€ç´¢
        memories = self.memory_cortex.recall_by_emotion(
            query_text=query_text,
            current_emotion_vector=current_emotion_vector,
            top_k=top_k,
        )
        
        # 2. å¦‚æœå½“å‰æœ‰è¯­ä¹‰åˆ†æç»“æœæˆ–çŠ¶æ€å‘é‡å†å²ï¼Œä½¿ç”¨é€†å‘åç¼©å¢å¼ºæ£€ç´¢
        target_vector = None
        if self._last_semantic_analysis and self._last_semantic_analysis.get("evolved_energy") is not None:
            target_vector = self._last_semantic_analysis.get("evolved_vector")
        if target_vector is None and self._state_vector_history and len(self._state_vector_history) > 0:
            try:
                recent_state = self._state_vector_history[-1]
                if recent_state.vector is not None:
                    target_vector = recent_state.vector
            except Exception as e:
                print(f"âš ï¸ è·å–é€†å‘åç¼©ç›®æ ‡å‘é‡å¤±è´¥: {e}")

        if target_vector is not None:
            try:
                collapsed_memories = inverse_collapse(
                    target_vector=target_vector,
                    memory_engine=self.memory_cortex,
                    project_name=self.project_name,
                    top_k=top_k,
                )

                # åˆå¹¶ç»“æœï¼ˆå»é‡ï¼Œä¼˜å…ˆä¿ç•™ RAG ç»“æœï¼‰
                seen_texts = {mem.get("text", "") for mem in memories}
                for collapsed_mem in collapsed_memories:
                    collapsed_text = collapsed_mem.get("text", "")
                    if collapsed_text and collapsed_text not in seen_texts:
                        memories.append({
                            "text": collapsed_text,
                            "similarity": collapsed_mem.get("similarity", 0.5),
                            "semantic_similarity": collapsed_mem.get("similarity", 0.5),
                            "emotion_similarity": None,
                            "metadata": {
                                "source": "semantic_field_collapse",
                            },
                        })
                        seen_texts.add(collapsed_text)
            except Exception as e:
                # é€†å‘åç¼©å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                print(f"âš ï¸ é€†å‘åç¼©æ£€ç´¢å¤±è´¥: {e}")
        
        return memories

