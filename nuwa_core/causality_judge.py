"""
å‘é‡å› æœåˆ¤å®˜æ¨¡å— (Vector-Based Causality Judge Module)

åœ¨å¤ªä¸€å¼•æ“æ—¶ä»£ï¼Œè¿™ä¸ªæ¨¡å—ç”¨äºåšâ€œå‰§æƒ…é˜²å´©â€ï¼›åœ¨å¥³å¨²å†…æ ¸æ¥ç®¡åï¼Œ
å®ƒæ›´å¤šè¢«è§†ä¸º Chatbot çš„â€œäº‹å®ä¸äººè®¾é˜²å´©â€é˜²çº¿ï¼š

- çºµå‘ï¼šå½“å‰å›å¤æ˜¯å¦è¿èƒŒå·²ç»è®°å½•çš„äº‹å®ï¼ˆfact_book / å†å²è®°å¿†ï¼‰ï¼›
- æ¨ªå‘ï¼šå½“å‰å›å¤æ˜¯å¦è¿èƒŒæ—¢å®šäººè®¾ï¼ˆProfile / Personaï¼‰ï¼›
- å‘é‡ï¼šé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦è¯„ä¼° OOC ç¨‹åº¦ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
- scan_conflicts: æ‰«æå½“å‰èŠ‚ç‚¹ä¸å†å²äº‹å®å’Œè§’è‰²è®¾å®šçš„äººè®¾å†²çª
- calculate_ooc_score: åŸºäºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®— OOC åˆ†æ•°
- ConflictReport: å†²çªæŠ¥å‘Šæ•°æ®ç»“æ„
"""

import json
import re
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum

from .state_machine import ChapterNode, NarrativeState, get_embedding_model, get_character_core_vector

# å¯¼å…¥å‘é‡è®¡ç®—ç›¸å…³åº“
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    np = None
    NUMPY_AVAILABLE = False


class ConflictLevel(Enum):
    """å†²çªçº§åˆ«"""
    CRITICAL = "critical"  # ä¸¥é‡é”™è¯¯ï¼ˆBugï¼‰
    WARNING = "warning"    # é£é™©è­¦å‘Š


@dataclass
class ConflictReport:
    """
    å†²çªæŠ¥å‘Š

    åœ¨ Chatbot / AI ä¼´ä¾£åœºæ™¯ä¸‹ï¼Œå¯ç†è§£ä¸ºâ€œé€»è¾‘é£æ§æŠ¥å‘Šâ€ï¼š
    - critical_errors: äº‹å®é”™è¯¯æˆ–ä¸¥é‡ OOCï¼ˆäººè®¾å´©åç­‰ï¼Œå»ºè®®ç›´æ¥æ‰“å›ï¼‰
    - warnings: è½»é‡çº§é£é™©ï¼ˆè¯­æ°”ç•¥è¿å’Œã€èƒ½é‡è·ƒè¿è¿‡å¤§ä½†å°šå¯å…œä½ç­‰ï¼‰
    - ooc_scores: å„è§’è‰²/äººæ ¼çš„å‘é‡ä¸€è‡´æ€§åˆ†æ•°ï¼ˆè¶Šä½è¶Šåç¦»äººè®¾ï¼‰
    """
    critical_errors: List[Dict[str, Any]] = field(default_factory=list)  # ä¸¥é‡é”™è¯¯åˆ—è¡¨
    warnings: List[Dict[str, Any]] = field(default_factory=list)  # è­¦å‘Šåˆ—è¡¨
    ooc_scores: Dict[str, float] = field(default_factory=dict)  # OOC åˆ†æ•°ï¼ˆæ–°å¢ï¼‰
    # æ ¼å¼: { "äºšç‘Ÿ": 0.35 }  # ä½™å¼¦ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ< 0.4 è¡¨ç¤º OOCï¼‰
    
    def has_conflicts(self) -> bool:
        """æ˜¯å¦æœ‰å†²çª"""
        return len(self.critical_errors) > 0 or len(self.warnings) > 0
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "critical_errors": self.critical_errors,
            "warnings": self.warnings,
            "ooc_scores": self.ooc_scores,
            "total_critical": len(self.critical_errors),
            "total_warnings": len(self.warnings),
        }
    
    def to_json(self) -> str:
        """è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²"""
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)


def scan_conflicts(
    current_node: ChapterNode,
    vector_db=None,
    character_table: Optional[List[Dict[str, str]]] = None,
    project_name: Optional[str] = None,
    # åœ¨ Chatbot åœºæ™¯ä¸‹ï¼Œå¯æ˜¾å¼ä¼ å…¥ fact_bookï¼ˆäº‹å®è´¦æœ¬ï¼‰ï¼Œç”¨äºçºµå‘äº‹å®é˜²å´©
    fact_book: Optional[Dict[str, Any]] = None,
    # æ˜¯å¦å¯ç”¨â€œç‰©å“/è£…å¤‡ä¸€è‡´æ€§æ£€æŸ¥â€ï¼ˆä¸»è¦ç”¨äºé‡åº¦ RP/ä¸–ç•Œè§‚ç©æ³•ï¼‰
    rp_mode: bool = False,
    selected_model: str = "lm_studio",
    base_url: Optional[str] = None,
    model_name: Optional[str] = None,
    api_key: Optional[str] = None,
    gemini_base_url: Optional[str] = None,
) -> ConflictReport:
    """
    æ‰«æå½“å‰èŠ‚ç‚¹ä¸å†å²å’Œè§’è‰²è®¾å®šçš„å†²çªï¼ˆåŸºäºè¯­ä¹‰ï¼‰
    
    å®ç°ä¸‰ç§æ ¡éªŒï¼š
    1. çºµå‘æ ¡éªŒ (History / Fact Check): æ£€æŸ¥ä¸å†å²äº‹å® / fact_book çš„å†²çª
    2. æ¨ªå‘æ ¡éªŒ (Profile Check): æ£€æŸ¥ä¸è§’è‰²è®¾å®šçš„å†²çª
    3. çŠ¶æ€å»¶ç»­æ€§æ£€æŸ¥ (State Continuity Check): æ£€æŸ¥è§’è‰²çŠ¶æ€çš„è¯­ä¹‰å»¶ç»­æ€§
    4. ç‰©å“ä¸€è‡´æ€§æ£€æŸ¥ (Equipment Consistency Check): æ£€æŸ¥ç‰©å“ä½¿ç”¨çš„é€»è¾‘ä¸€è‡´æ€§
    
    Args:
        current_node: å½“å‰ç« èŠ‚èŠ‚ç‚¹
        vector_db: å‘é‡æ•°æ®åº“ï¼ˆLanceDBï¼‰è¿æ¥ï¼Œç”¨äºæ£€ç´¢å†å²çŠ¶æ€
        character_table: è§’è‰²è¡¨ï¼Œæ ¼å¼ä¸º [{"name": "è§’è‰²å", "description": "æè¿°"}, ...]
        project_name: é¡¹ç›®åç§°ï¼Œç”¨äºæ£€ç´¢è®°å¿†
        selected_model: æ¨¡å‹ç±»å‹ï¼ˆç”¨äºè¯­ä¹‰å†²çªæ£€æµ‹ï¼‰
        base_url: LM Studio base_url
        model_name: æ¨¡å‹åç§°
        api_key: Gemini API Key
        gemini_base_url: Gemini base_url
    
    Returns:
        ConflictReport: åŒ…å«æ‰€æœ‰å†²çªå’Œè­¦å‘Šçš„æŠ¥å‘Š
    """
    report = ConflictReport()
    
    if not current_node:
        return report
    
    # 1. çŠ¶æ€å»¶ç»­æ€§æ£€æŸ¥ï¼šæ£€æŸ¥è§’è‰²çŠ¶æ€çš„è¯­ä¹‰å»¶ç»­æ€§
    if current_node.chapter_id > 1:
        continuity_conflicts = _check_state_continuity(
            current_node=current_node,
            project_name=project_name,
            selected_model=selected_model,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            gemini_base_url=gemini_base_url,
        )
        report.critical_errors.extend(continuity_conflicts["critical"])
        report.warnings.extend(continuity_conflicts["warnings"])
    
    # 2. ç‰©å“ä¸€è‡´æ€§æ£€æŸ¥ï¼šåœ¨ Chatbot ä¸­é»˜è®¤å…³é—­ï¼Œåªåœ¨ rp_mode=True çš„æ²‰æµ¸å¼ RP åœºæ™¯å¯ç”¨
    if rp_mode:
        equipment_conflicts = _check_equipment_consistency(
            current_node=current_node,
            project_name=project_name,
            selected_model=selected_model,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            gemini_base_url=gemini_base_url,
        )
        report.critical_errors.extend(equipment_conflicts["critical"])
        report.warnings.extend(equipment_conflicts["warnings"])
    
    # 3. çºµå‘æ ¡éªŒï¼šæ£€æŸ¥ä¸å†å²äº‹å® / fact_book çš„å†²çª
    # Chatbot åœºæ™¯ä¼˜å…ˆä½¿ç”¨ fact_bookï¼›å¦‚æœæœªæä¾›ï¼Œåˆ™é€€å›å¤ªä¸€å¼•æ“æ—§é€»è¾‘
    if (fact_book and isinstance(fact_book, dict)) or (vector_db and project_name):
        history_conflicts = _check_history_conflicts(
            current_node=current_node,
            vector_db=vector_db,
            project_name=project_name,
            fact_book=fact_book,
        )
        report.critical_errors.extend(history_conflicts["critical"])
        report.warnings.extend(history_conflicts["warnings"])
    
    # 4. æ¨ªå‘æ ¡éªŒï¼šæ£€æŸ¥ä¸è§’è‰²è®¾å®šçš„å†²çª
    if character_table:
        profile_conflicts = _check_profile_conflicts(
            current_node=current_node,
            character_table=character_table,
        )
        report.critical_errors.extend(profile_conflicts["critical"])
        report.warnings.extend(profile_conflicts["warnings"])
    
    # 5. å‘é‡ OOC æ£€æµ‹ï¼šåŸºäºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®— OOC åˆ†æ•°ï¼ˆäº‹å®é˜²å´©ä¸­çš„â€œäººè®¾ä¸€è‡´æ€§å±‚â€ï¼‰
    if character_table and NUMPY_AVAILABLE and np is not None:
        ooc_scores = calculate_ooc_scores(
            current_node=current_node,
            character_table=character_table,
        )
        report.ooc_scores = ooc_scores

        # å½“å‘é‡ä¸€è‡´æ€§è¿‡ä½æ—¶ï¼Œç›´æ¥è½å…¥é€»è¾‘é£æ§ï¼Œæ ‡è®°ä¸ºä¸¥é‡/è½»å¾® OOC
        for char_name, score in ooc_scores.items():
            if score < 0.4:
                report.critical_errors.append({
                    "type": "ooc_vector",
                    "level": ConflictLevel.CRITICAL.value,
                    "character": char_name,
                    "ooc_score": round(score, 3),
                    "message": f"è§’è‰²ã€Œ{char_name}ã€äººè®¾å´©å¡Œï¼ˆOOCï¼‰ï¼šå½“å‰è¡Œä¸ºä¸è§’è‰²è®¾å®šçš„ä½™å¼¦ç›¸ä¼¼åº¦ä»…ä¸º {score:.3f}ï¼ˆé˜ˆå€¼ 0.4ï¼‰ï¼Œæ•°å­¦ä¸Šåˆ¤å®šä¸ºä¸¥é‡åç¦»äººè®¾",
                })
            elif score < 0.6:
                report.warnings.append({
                    "type": "ooc_vector",
                    "level": ConflictLevel.WARNING.value,
                    "character": char_name,
                    "ooc_score": round(score, 3),
                    "message": f"è§’è‰²ã€Œ{char_name}ã€è¡Œä¸ºå¯èƒ½åç¦»äººè®¾ï¼šä½™å¼¦ç›¸ä¼¼åº¦ä¸º {score:.3f}ï¼ˆå»ºè®® > 0.6ï¼‰",
                })
    else:
        # è°ƒè¯•ä¿¡æ¯ï¼šä¸ºä»€ä¹ˆ OOC æ£€æµ‹æ²¡æœ‰è¿è¡Œ
        if not character_table:
            print("âš ï¸ OOC æ£€æµ‹æœªè¿è¡Œï¼šcharacter_table ä¸ºç©º")
        elif not NUMPY_AVAILABLE:
            print("âš ï¸ OOC æ£€æµ‹æœªè¿è¡Œï¼šNUMPY_AVAILABLE ä¸º False")
        elif np is None:
            print("âš ï¸ OOC æ£€æµ‹æœªè¿è¡Œï¼šnp ä¸º None")
    
    # 6. NTD å‡çº§ï¼šè®¡ç®—å™äº‹èƒ½é‡ï¼ˆèƒ½é‡å‡½æ•°ï¼‰
    if NUMPY_AVAILABLE and np is not None and current_node.state_vector is not None:
        # åŠ è½½å‰ä¸€ç« èŠ‚ç‚¹ï¼ˆç”¨äºè®¡ç®—ä¸€è‡´æ€§åŠ¿èƒ½ï¼‰
        prev_node = None
        if current_node.chapter_id > 1 and project_name:
            try:
                import os
                nodes_dir = os.path.join("data", project_name or "", "nodes")
                prev_chapter_id = current_node.chapter_id - 1
                prev_node_path = os.path.join(nodes_dir, f"{prev_chapter_id}.json")
                if os.path.exists(prev_node_path):
                    with open(prev_node_path, 'r', encoding='utf-8') as f:
                        prev_data = json.load(f)
                        prev_node = ChapterNode.from_dict(prev_data.get("node", {}))
            except Exception as e:
                print(f"åŠ è½½å‰ä¸€ç« èŠ‚ç‚¹å¤±è´¥ï¼ˆç”¨äºèƒ½é‡è®¡ç®—ï¼‰: {e}")
        
        # è®¡ç®—å™äº‹èƒ½é‡
        energy, energy_breakdown = calculate_narrative_energy(
            current_node=current_node,
            prev_node=prev_node,
            target_vector=None,  # å¯ä»¥ä¼ å…¥å¤§çº²å‘é‡ï¼ˆå¦‚æœæœ‰ï¼‰
        )
        
        # èƒ½é‡é˜ˆå€¼ï¼ˆå¯è°ƒï¼‰
        ENERGY_THRESHOLD = 0.8  # å¦‚æœèƒ½é‡ > 0.8ï¼Œåˆ¤å®šä¸ºé«˜é£é™©
        
        if energy > ENERGY_THRESHOLD:
            report.warnings.append({
                "type": "narrative_energy",
                "level": ConflictLevel.WARNING.value,
                "energy": round(energy, 3),
                "energy_breakdown": energy_breakdown,
                "message": f"âš ï¸ é«˜èƒ½é¢„è­¦ï¼ˆé€»è¾‘å´©åé£é™©ï¼‰ï¼šå™äº‹èƒ½é‡ä¸º {energy:.3f}ï¼ˆé˜ˆå€¼ {ENERGY_THRESHOLD}ï¼‰ã€‚çŠ¶æ€çªå˜è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´é€»è¾‘ä¸ä¸€è‡´ã€‚",
            })
    
    return report


def _check_state_continuity(
    current_node: ChapterNode,
    project_name: Optional[str],
    selected_model: str,
    base_url: Optional[str],
    model_name: Optional[str],
    api_key: Optional[str],
    gemini_base_url: Optional[str],
) -> Dict[str, List[Dict[str, Any]]]:
    """
    çŠ¶æ€å»¶ç»­æ€§æ£€æŸ¥ï¼šæ£€æŸ¥è§’è‰²çŠ¶æ€çš„è¯­ä¹‰å»¶ç»­æ€§
    
    å¦‚æœä¸Šä¸€ç«  physique æ˜¯ "åŒè…¿éª¨æŠ˜"ï¼Œè€Œæœ¬ç« æ­£æ–‡å†™ "ä»–é£èµ·ä¸€è„š"ï¼Œ
    åˆ¤å®˜åº”è¯†åˆ«å‡ºè¯­ä¹‰å†²çªã€‚
    
    Args:
        current_node: å½“å‰ç« èŠ‚èŠ‚ç‚¹
        project_name: é¡¹ç›®åç§°
        selected_model: æ¨¡å‹ç±»å‹
        base_url: LM Studio base_url
        model_name: æ¨¡å‹åç§°
        api_key: Gemini API Key
        gemini_base_url: Gemini base_url
    
    Returns:
        åŒ…å« critical å’Œ warnings çš„å­—å…¸
    """
    conflicts = {
        "critical": [],
        "warnings": [],
    }
    
    if not current_node.narrative_state or not current_node.narrative_state.characters:
        return conflicts
    
    # åŠ è½½å‰ä¸€ç« çš„çŠ¶æ€
    try:
        import os
        nodes_dir = os.path.join("data", project_name or "", "nodes")
        prev_chapter_id = current_node.chapter_id - 1
        
        if prev_chapter_id < 1:
            return conflicts
        
        prev_node_path = os.path.join(nodes_dir, f"{prev_chapter_id}.json")
        if not os.path.exists(prev_node_path):
            return conflicts
        
        with open(prev_node_path, 'r', encoding='utf-8') as f:
            prev_data = json.load(f)
            prev_node = ChapterNode.from_dict(prev_data.get("node", {}))
        
        if not prev_node.narrative_state:
            return conflicts
        
        prev_narrative = prev_node.narrative_state
        current_narrative = current_node.narrative_state
        
        # å¯¹æ¯ä¸ªè§’è‰²è¿›è¡ŒçŠ¶æ€å»¶ç»­æ€§æ£€æŸ¥
        for char_name, current_char_state in current_narrative.characters.items():
            if char_name not in prev_narrative.characters:
                continue
            
            prev_char_state = prev_narrative.characters[char_name]
            
            # æ£€æŸ¥ç”Ÿç†çŠ¶æ€çš„å»¶ç»­æ€§
            prev_physique = prev_char_state.get("physique", "").strip()
            current_physique = current_char_state.get("physique", "").strip()
            current_text = current_node.text_content
            
            if prev_physique and current_text:
                # ä½¿ç”¨ LLM æ£€æŸ¥è¯­ä¹‰å†²çª
                conflict_detected = _check_semantic_conflict_with_llm(
                    prev_state=prev_physique,
                    current_text=current_text,
                    char_name=char_name,
                    selected_model=selected_model,
                    base_url=base_url,
                    model_name=model_name,
                    api_key=api_key,
                    gemini_base_url=gemini_base_url,
                )
                
                if conflict_detected:
                    conflicts["critical"].append({
                        "type": "state_continuity",
                        "level": ConflictLevel.CRITICAL.value,
                        "character": char_name,
                        "prev_physique": prev_physique,
                        "current_text_snippet": current_text[:200],
                        "message": f"è§’è‰²ã€Œ{char_name}ã€çš„ç”Ÿç†çŠ¶æ€ä¸æ­£æ–‡æè¿°å†²çªï¼šä¸Šä¸€ç« çŠ¶æ€ä¸ºã€Œ{prev_physique}ã€ï¼Œä½†æ­£æ–‡ä¸­å‡ºç°äº†ä¸ä¹‹çŸ›ç›¾çš„è¡Œä¸º",
                    })
        
    except Exception as e:
        print(f"çŠ¶æ€å»¶ç»­æ€§æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        print(traceback.format_exc())
    
    return conflicts


def _check_equipment_consistency(
    current_node: ChapterNode,
    project_name: Optional[str],
    selected_model: str,
    base_url: Optional[str],
    model_name: Optional[str],
    api_key: Optional[str],
    gemini_base_url: Optional[str],
) -> Dict[str, List[Dict[str, Any]]]:
    """
    ç‰©å“ä¸€è‡´æ€§æ£€æŸ¥ï¼šæ£€æŸ¥ç‰©å“ä½¿ç”¨çš„é€»è¾‘ä¸€è‡´æ€§
    
    å¦‚æœ equipment é‡Œæ²¡æœ‰ "æª"ï¼Œæ­£æ–‡å´å†™ "ä»–æ‹”æªå°„å‡»"ï¼Œè§¦å‘è­¦å‘Šã€‚
    
    Args:
        current_node: å½“å‰ç« èŠ‚èŠ‚ç‚¹
        project_name: é¡¹ç›®åç§°
        selected_model: æ¨¡å‹ç±»å‹
        base_url: LM Studio base_url
        model_name: æ¨¡å‹åç§°
        api_key: Gemini API Key
        gemini_base_url: Gemini base_url
    
    Returns:
        åŒ…å« critical å’Œ warnings çš„å­—å…¸
    """
    conflicts = {
        "critical": [],
        "warnings": [],
    }
    
    if not current_node.narrative_state or not current_node.narrative_state.characters:
        return conflicts
    
    narrative_state = current_node.narrative_state
    current_text = current_node.text_content
    
    if not current_text:
        return conflicts
    
    # åªæ£€æŸ¥åœ¨æ­£æ–‡ä¸­å®é™…å‡ºç°çš„è§’è‰²ï¼Œé¿å…æ— ç”¨çš„LLMè°ƒç”¨
    # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœè§’è‰²åä¸åœ¨æ­£æ–‡ä¸­å‡ºç°ï¼Œç›´æ¥è·³è¿‡
    for char_name, char_state in narrative_state.characters.items():
        # å¿«é€Ÿè¿‡æ»¤ï¼šå¦‚æœè§’è‰²åä¸åœ¨æ­£æ–‡ä¸­å‡ºç°ï¼Œè·³è¿‡ï¼ˆé¿å…æ— ç”¨çš„LLMè°ƒç”¨ï¼‰
        if char_name not in current_text:
            continue
        
        equipment = char_state.get("equipment", [])
        equipment_text = ", ".join(equipment) if equipment else "æ— "
        
        # ä½¿ç”¨ LLM æ£€æŸ¥ç‰©å“ä½¿ç”¨æ˜¯å¦ä¸€è‡´
        inconsistency_detected = _check_equipment_inconsistency_with_llm(
            equipment_list=equipment_text,
            current_text=current_text,
            char_name=char_name,
            selected_model=selected_model,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            gemini_base_url=gemini_base_url,
        )
        
        if inconsistency_detected:
            # å°è¯•æå–å…·ä½“ä½¿ç”¨çš„ç‰©å“ï¼ˆé€šè¿‡ LLMï¼‰
            used_item = _extract_used_item_from_text(
                equipment_list=equipment_text,
                current_text=current_text[:1000],
                char_name=char_name,
                selected_model=selected_model,
                base_url=base_url,
                model_name=model_name,
                api_key=api_key,
                gemini_base_url=gemini_base_url,
            )
            
            if used_item:
                message = f"è§’è‰²ã€Œ{char_name}ã€ä½¿ç”¨äº†æœªåœ¨è£…å¤‡åˆ—è¡¨ä¸­çš„ç‰©å“ï¼š{used_item}ï¼ˆè£…å¤‡åˆ—è¡¨ï¼š{equipment_text}ï¼‰"
            else:
                message = f"è§’è‰²ã€Œ{char_name}ã€ä½¿ç”¨äº†æœªåœ¨è£…å¤‡åˆ—è¡¨ä¸­çš„ç‰©å“ï¼ˆè£…å¤‡åˆ—è¡¨ï¼š{equipment_text}ï¼‰"
            
            conflicts["critical"].append({
                "type": "equipment_consistency",
                "level": ConflictLevel.CRITICAL.value,
                "character": char_name,
                "equipment_list": equipment_text,
                "current_text_snippet": current_text[:200],
                "message": message,
            })
    
    return conflicts


def _check_semantic_conflict_with_llm(
    prev_state: str,
    current_text: str,
    char_name: str,
    selected_model: str,
    base_url: Optional[str],
    model_name: Optional[str],
    api_key: Optional[str],
    gemini_base_url: Optional[str],
) -> bool:
    """
    ä½¿ç”¨ LLM æ£€æŸ¥è¯­ä¹‰å†²çª
    
    Args:
        prev_state: å‰ä¸€ç« çš„çŠ¶æ€æè¿°
        current_text: å½“å‰ç« èŠ‚æ­£æ–‡
        char_name: è§’è‰²å
        selected_model: æ¨¡å‹ç±»å‹
        base_url: LM Studio base_url
        model_name: æ¨¡å‹åç§°
        api_key: Gemini API Key
        gemini_base_url: Gemini base_url
    
    Returns:
        æ˜¯å¦æ£€æµ‹åˆ°å†²çª
    """
    if not prev_state or not current_text:
        return False
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´é€»è¾‘æ£€æŸ¥å™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ£€æŸ¥è§’è‰²çŠ¶æ€çš„è¯­ä¹‰å»¶ç»­æ€§ã€‚

å¯¹æ¯”ã€å†å²çŠ¶æ€æè¿°ã€‘å’Œã€æ–°ç”Ÿæˆæ­£æ–‡ã€‘ï¼Œå¯»æ‰¾é€»è¾‘çŸ›ç›¾ç‚¹ã€‚

å¦‚æœå‘ç°æ˜æ˜¾çš„è¯­ä¹‰å†²çªï¼ˆä¾‹å¦‚ï¼šå†å²çŠ¶æ€æ˜¯"åŒè…¿éª¨æŠ˜"ï¼Œä½†æ­£æ–‡ä¸­å†™"ä»–é£èµ·ä¸€è„š"ï¼‰ï¼Œè¯·å›ç­”"å†²çª"ã€‚
å¦‚æœæ²¡æœ‰æ˜æ˜¾å†²çªï¼Œè¯·å›ç­”"æ— å†²çª"ã€‚

åªå›ç­”"å†²çª"æˆ–"æ— å†²çª"ï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ã€‚"""

    user_prompt = f"""è§’è‰²åï¼š{char_name}

ã€å†å²çŠ¶æ€æè¿°ã€‘
{prev_state}

ã€æ–°ç”Ÿæˆæ­£æ–‡ã€‘
{current_text[:1000]}

è¯·åˆ¤æ–­æ˜¯å¦å­˜åœ¨è¯­ä¹‰å†²çªã€‚"""

    try:
        import sys
        
        generate_content_lm_studio = None
        generate_content_gemini = None
        
        possible_module_names = ['app', '__main__']
        
        for module_name in possible_module_names:
            if module_name in sys.modules:
                module = sys.modules[module_name]
                if generate_content_lm_studio is None:
                    generate_content_lm_studio = getattr(module, 'generate_content_lm_studio', None)
                if generate_content_gemini is None:
                    generate_content_gemini = getattr(module, 'generate_content_gemini', None)
                
                if generate_content_lm_studio is not None and generate_content_gemini is not None:
                    break
        
        if generate_content_lm_studio is None or generate_content_gemini is None:
            return False
        
        if selected_model == "gemini":
            if not api_key or not model_name:
                return False
            
            success, result = generate_content_gemini(
                api_key=api_key,
                model_name=model_name,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                base_url=gemini_base_url,
                max_output_tokens=50,
                temperature=0.1,
                stream=False,
            )
            if success and result:
                return "å†²çª" in result.strip()
        else:
            if not base_url or not model_name:
                return False
            
            success, result = generate_content_lm_studio(
                base_url=base_url,
                model_name=model_name,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                max_tokens=50,
                temperature=0.1,
                stream=False,
            )
            if success and result:
                return "å†²çª" in result.strip()
    except Exception as e:
        print(f"è¯­ä¹‰å†²çªæ£€æŸ¥å¤±è´¥: {e}")
        return False
    
    return False


def _check_equipment_inconsistency_with_llm(
    equipment_list: str,
    current_text: str,
    char_name: str,
    selected_model: str,
    base_url: Optional[str],
    model_name: Optional[str],
    api_key: Optional[str],
    gemini_base_url: Optional[str],
) -> bool:
    """
    ä½¿ç”¨ LLM æ£€æŸ¥ç‰©å“ä½¿ç”¨æ˜¯å¦ä¸€è‡´
    
    Args:
        equipment_list: è£…å¤‡åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²ï¼‰
        current_text: å½“å‰ç« èŠ‚æ­£æ–‡
        char_name: è§’è‰²å
        selected_model: æ¨¡å‹ç±»å‹
        base_url: LM Studio base_url
        model_name: æ¨¡å‹åç§°
        api_key: Gemini API Key
        gemini_base_url: Gemini base_url
    
    Returns:
        æ˜¯å¦æ£€æµ‹åˆ°ä¸ä¸€è‡´
    """
    if not current_text:
        return False
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´é€»è¾‘æ£€æŸ¥å™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ£€æŸ¥è§’è‰²ç‰©å“ä½¿ç”¨çš„é€»è¾‘ä¸€è‡´æ€§ã€‚

é‡è¦è§„åˆ™ï¼š
1. **è¯­ä¹‰åŒ¹é…**ï¼šè£…å¤‡åˆ—è¡¨ä¸­çš„ç‰©å“å’Œæ­£æ–‡ä¸­ä½¿ç”¨çš„ç‰©å“åº”è¯¥è¿›è¡Œè¯­ä¹‰åŒ¹é…ï¼Œè€Œä¸æ˜¯ä¸¥æ ¼çš„å­—ç¬¦ä¸²åŒ¹é…ã€‚
   - ä¾‹å¦‚ï¼šè£…å¤‡åˆ—è¡¨æœ‰"æŠ¤ç…§ï¼ˆæ—¥æœ¬ç­¾è¯ï¼‰"ï¼Œæ­£æ–‡ä¸­ä½¿ç”¨"æŠ¤ç…§"æˆ–"ä»–çš„æŠ¤ç…§" â†’ åº”è¯¥åˆ¤æ–­ä¸º"ä¸€è‡´"ï¼ˆæ˜¯åŒä¸€ä¸ªç‰©å“ï¼‰
   - ä¾‹å¦‚ï¼šè£…å¤‡åˆ—è¡¨æœ‰"æ‰‹æœº"ï¼Œæ­£æ–‡ä¸­ä½¿ç”¨"æ™ºèƒ½æ‰‹æœº"æˆ–"ä»–çš„æ‰‹æœº" â†’ åº”è¯¥åˆ¤æ–­ä¸º"ä¸€è‡´"
   - ä¾‹å¦‚ï¼šè£…å¤‡åˆ—è¡¨æœ‰"æ–­è£‚çš„å®¶å¾½å‰‘"ï¼Œæ­£æ–‡ä¸­ä½¿ç”¨"å‰‘"æˆ–"å®¶å¾½å‰‘" â†’ åº”è¯¥åˆ¤æ–­ä¸º"ä¸€è‡´"ï¼ˆæ˜¯åŒä¸€ä¸ªç‰©å“ï¼‰

2. **åªæ£€æŸ¥æ˜ç¡®ä½¿ç”¨**ï¼šåªæ£€æŸ¥è§’è‰²**æ˜ç¡®æŒæœ‰æˆ–ä½¿ç”¨**çš„ç‰©å“ï¼Œè€Œä¸æ˜¯æ­£æ–‡ä¸­**ä»…ä»…æåˆ°**çš„ç‰©å“ã€‚
   - å¦‚æœæ­£æ–‡ä¸­åªæ˜¯æè¿°ç¯å¢ƒä¸­çš„ç‰©å“ã€å…¶ä»–è§’è‰²çš„ç‰©å“ã€æˆ–è€…åªæ˜¯æåˆ°ç‰©å“åç§°ä½†æ²¡æœ‰æ˜ç¡®è¡¨ç¤ºè¯¥è§’è‰²æŒæœ‰æˆ–ä½¿ç”¨ï¼Œåº”è¯¥åˆ¤æ–­ä¸º"ä¸€è‡´"ã€‚

3. **è·å–ç‰©å“çš„å¤„ç†**ï¼š
   - **å…³é”®**ï¼šå¦‚æœæ­£æ–‡ä¸­æè¿°è§’è‰²"è·å–"ã€"è·å¾—"ã€"æ¡åˆ°"ã€"æ‰¾åˆ°"ã€"è´­ä¹°"ã€"æ”¶åˆ°"æŸä¸ªç‰©å“ï¼Œè¿™è¡¨ç¤ºç‰©å“æ˜¯**æ–°è·å¾—çš„**ï¼Œä¸åº”è¯¥åˆ¤æ–­ä¸º"ä¸ä¸€è‡´"ã€‚
   - è£…å¤‡åˆ—è¡¨è®°å½•çš„æ˜¯**å½“å‰æŒæœ‰çš„ç‰©å“**ï¼Œå¦‚æœæ­£æ–‡ä¸­æè¿°è§’è‰²è·å–äº†æ–°ç‰©å“ï¼Œè¿™æ˜¯æ­£å¸¸çš„å‰§æƒ…å‘å±•ï¼Œåº”è¯¥åˆ¤æ–­ä¸º"ä¸€è‡´"ã€‚
   - ä¾‹å¦‚ï¼šè£…å¤‡åˆ—è¡¨ï¼š"æŠ¤ç…§"ï¼Œæ­£æ–‡ï¼š"ä»–æ¡åˆ°äº†ä¸€æŠŠé’¥åŒ™" â†’ "ä¸€è‡´"ï¼ˆè·å–æ–°ç‰©å“ï¼Œä¸æ˜¯ä½¿ç”¨æœªåˆ—å‡ºçš„ç‰©å“ï¼‰
   - ä¾‹å¦‚ï¼šè£…å¤‡åˆ—è¡¨ï¼š"æŠ¤ç…§"ï¼Œæ­£æ–‡ï¼š"ä»–æ”¶åˆ°äº†ä¸€ä¸ªåŒ…è£¹" â†’ "ä¸€è‡´"ï¼ˆè·å–æ–°ç‰©å“ï¼‰

4. **åŸæœ¬å°±æœ‰çš„ç‰©å“**ï¼š
   - å¦‚æœæ­£æ–‡ä¸­æè¿°è§’è‰²ä½¿ç”¨æŸä¸ªç‰©å“ï¼Œä½†è¯¥ç‰©å“åœ¨è£…å¤‡åˆ—è¡¨ä¸­æ²¡æœ‰ï¼Œéœ€è¦åˆ¤æ–­ï¼š
     * å¦‚æœæ­£æ–‡ä¸­æ˜ç¡®è¡¨ç¤ºè¿™æ˜¯è§’è‰²"åŸæœ¬å°±æœ‰"ã€"ä¸€ç›´å¸¦ç€"ã€"éšèº«æºå¸¦"çš„ç‰©å“ï¼Œä¸”è£…å¤‡åˆ—è¡¨ä¸ºç©ºæˆ–å¾ˆå°‘ï¼Œè¿™å¯èƒ½è¡¨ç¤ºè£…å¤‡åˆ—è¡¨ä¸å®Œæ•´ï¼Œåº”è¯¥åˆ¤æ–­ä¸º"ä¸€è‡´"ï¼ˆé¿å…è¯¯æŠ¥ï¼‰ã€‚
     * å¦‚æœæ­£æ–‡ä¸­æè¿°è§’è‰²ä½¿ç”¨æŸä¸ªç‰©å“ï¼Œä¸”è¯¥ç‰©å“åœ¨è¯­ä¹‰ä¸Šä¸è£…å¤‡åˆ—è¡¨ä¸­çš„ç‰©å“ä¸åŒ¹é…ï¼Œä½†æ­£æ–‡ä¸­æ²¡æœ‰æ˜ç¡®è¡¨ç¤ºè¿™æ˜¯æ–°è·å–çš„ï¼Œæ‰åˆ¤æ–­ä¸º"ä¸ä¸€è‡´"ã€‚

5. **ä¸ä¸€è‡´çš„åˆ¤æ–­æ ‡å‡†**ï¼šåªæœ‰å½“æ­£æ–‡ä¸­æ˜ç¡®è¡¨ç¤ºè¯¥è§’è‰²æŒæœ‰ã€ä½¿ç”¨ã€æ“ä½œæŸä¸ªç‰©å“ï¼Œä¸”è¯¥ç‰©å“åœ¨è¯­ä¹‰ä¸Šä¸è£…å¤‡åˆ—è¡¨ä¸­çš„ä»»ä½•ç‰©å“éƒ½ä¸åŒ¹é…ï¼Œä¸”ä¸æ˜¯"è·å–æ–°ç‰©å“"çš„æƒ…å†µæ—¶ï¼Œæ‰åˆ¤æ–­ä¸º"ä¸ä¸€è‡´"ã€‚

6. **ç©ºåˆ—è¡¨å¤„ç†**ï¼š
   - å¦‚æœè£…å¤‡åˆ—è¡¨ä¸ºç©ºï¼ˆ"æ— "ï¼‰ï¼Œä½†æ­£æ–‡ä¸­æ˜ç¡®è¡¨ç¤ºè§’è‰²æŒæœ‰æˆ–ä½¿ç”¨äº†æŸä¸ªç‰©å“ï¼Œéœ€è¦åŒºåˆ†ï¼š
     * å¦‚æœæ˜¯"è·å–"æ–°ç‰©å“ â†’ "ä¸€è‡´"ï¼ˆæ­£å¸¸å‰§æƒ…ï¼‰
     * å¦‚æœæ˜¯"ä½¿ç”¨"ç‰©å“ä½†æ²¡æœ‰è·å–æè¿° â†’ "ä¸ä¸€è‡´"ï¼ˆå¯èƒ½é—æ¼äº†è£…å¤‡åˆ—è¡¨æ›´æ–°ï¼‰

7. **ç‰©å“æè¿°å˜ä½“**ï¼šè£…å¤‡åˆ—è¡¨ä¸­å¯èƒ½åŒ…å«ç‰©å“çš„æè¿°æ€§ä¿¡æ¯ï¼ˆå¦‚"æŠ¤ç…§ï¼ˆæ—¥æœ¬ç­¾è¯ï¼‰"ã€"æ–­è£‚çš„å®¶å¾½å‰‘"ï¼‰ï¼Œæ­£æ–‡ä¸­å¯èƒ½åªä½¿ç”¨æ ¸å¿ƒç‰©å“åç§°ï¼ˆå¦‚"æŠ¤ç…§"ã€"å‰‘"ï¼‰ï¼Œè¿™åº”è¯¥è¢«è®¤ä¸ºæ˜¯åŒ¹é…çš„ã€‚

ç¤ºä¾‹ï¼š
- è£…å¤‡åˆ—è¡¨ï¼š"æŠ¤ç…§ï¼ˆæ—¥æœ¬ç­¾è¯ï¼‰"ï¼Œæ­£æ–‡ï¼š"ä»–æ‹¿å‡ºæŠ¤ç…§" â†’ "ä¸€è‡´"ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰
- è£…å¤‡åˆ—è¡¨ï¼š"æŠ¤ç…§ï¼ˆæ—¥æœ¬ç­¾è¯ï¼‰"ï¼Œæ­£æ–‡ï¼š"ä»–æ£€æŸ¥äº†æŠ¤ç…§ä¸Šçš„ç­¾è¯" â†’ "ä¸€è‡´"ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰
- è£…å¤‡åˆ—è¡¨ï¼š"æ— "ï¼Œæ­£æ–‡ï¼š"ä»–çœ‹åˆ°äº†æ¡Œä¸Šçš„æª" â†’ "ä¸€è‡´"ï¼ˆåªæ˜¯çœ‹åˆ°ï¼Œæ²¡æœ‰æŒæœ‰ï¼‰
- è£…å¤‡åˆ—è¡¨ï¼š"æ— "ï¼Œæ­£æ–‡ï¼š"ä»–æ‹”æªå°„å‡»" â†’ "ä¸ä¸€è‡´"ï¼ˆæ˜ç¡®ä½¿ç”¨ï¼Œä¸”æ²¡æœ‰è·å–æè¿°ï¼‰
- è£…å¤‡åˆ—è¡¨ï¼š"æŠ¤ç…§"ï¼Œæ­£æ–‡ï¼š"ä»–æ‹”æªå°„å‡»" â†’ "ä¸ä¸€è‡´"ï¼ˆæ˜ç¡®ä½¿ç”¨ä½†ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œä¸”è¯­ä¹‰ä¸åŒ¹é…ï¼‰
- è£…å¤‡åˆ—è¡¨ï¼š"æª"ï¼Œæ­£æ–‡ï¼š"ä»–æ‹”æªå°„å‡»" â†’ "ä¸€è‡´"ï¼ˆè£…å¤‡åˆ—è¡¨ä¸­æœ‰ï¼‰
- è£…å¤‡åˆ—è¡¨ï¼š"æ–­è£‚çš„å®¶å¾½å‰‘"ï¼Œæ­£æ–‡ï¼š"ä»–æŒ¥èˆç€å‰‘" â†’ "ä¸€è‡´"ï¼ˆè¯­ä¹‰åŒ¹é…ï¼Œæ˜¯åŒä¸€ä¸ªç‰©å“ï¼‰
- è£…å¤‡åˆ—è¡¨ï¼š"æŠ¤ç…§ï¼ˆæ—¥æœ¬ç­¾è¯ï¼‰"ï¼Œæ­£æ–‡ï¼š"ä»–æ‹¿å‡ºæ‰‹æœº" â†’ "ä¸ä¸€è‡´"ï¼ˆæ˜ç¡®ä½¿ç”¨ä½†ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œä¸”è¯­ä¹‰ä¸åŒ¹é…ï¼‰
- **è£…å¤‡åˆ—è¡¨ï¼š"æŠ¤ç…§"ï¼Œæ­£æ–‡ï¼š"ä»–æ¡åˆ°äº†ä¸€æŠŠé’¥åŒ™" â†’ "ä¸€è‡´"ï¼ˆè·å–æ–°ç‰©å“ï¼Œä¸æ˜¯ä½¿ç”¨æœªåˆ—å‡ºçš„ç‰©å“ï¼‰**
- **è£…å¤‡åˆ—è¡¨ï¼š"æŠ¤ç…§"ï¼Œæ­£æ–‡ï¼š"ä»–æ‰¾åˆ°äº†ä¸€ä¸ªé’±åŒ…" â†’ "ä¸€è‡´"ï¼ˆè·å–æ–°ç‰©å“ï¼‰**
- **è£…å¤‡åˆ—è¡¨ï¼š"æŠ¤ç…§"ï¼Œæ­£æ–‡ï¼š"ä»–æ”¶åˆ°äº†ä¸€ä¸ªåŒ…è£¹" â†’ "ä¸€è‡´"ï¼ˆè·å–æ–°ç‰©å“ï¼‰**
- **è£…å¤‡åˆ—è¡¨ï¼š"æ— "ï¼Œæ­£æ–‡ï¼š"ä»–æ¡èµ·åœ°ä¸Šçš„æª" â†’ "ä¸€è‡´"ï¼ˆè·å–æ–°ç‰©å“ï¼Œæ­£å¸¸å‰§æƒ…ï¼‰**
- **è£…å¤‡åˆ—è¡¨ï¼š"æŠ¤ç…§"ï¼Œæ­£æ–‡ï¼š"ä»–ä¸€ç›´å¸¦ç€çš„æ‰‹æœºå“äº†" â†’ "ä¸€è‡´"ï¼ˆåŸæœ¬å°±æœ‰ï¼Œè£…å¤‡åˆ—è¡¨å¯èƒ½ä¸å®Œæ•´ï¼Œé¿å…è¯¯æŠ¥ï¼‰**

è¯·ä»”ç»†åˆ†æè£…å¤‡åˆ—è¡¨å’Œæ­£æ–‡ï¼Œè¿›è¡Œè¯­ä¹‰åŒ¹é…åˆ¤æ–­ã€‚

åªå›ç­”"ä¸ä¸€è‡´"æˆ–"ä¸€è‡´"ï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ã€‚"""

    user_prompt = f"""è§’è‰²åï¼š{char_name}

ã€è£…å¤‡åˆ—è¡¨ã€‘
{equipment_list}

ã€æ–°ç”Ÿæˆæ­£æ–‡ã€‘
{current_text[:1000]}

è¯·ä»”ç»†åˆ¤æ–­ï¼šæ­£æ–‡ä¸­æ˜¯å¦æ˜ç¡®è¡¨ç¤ºè¯¥è§’è‰²æŒæœ‰æˆ–ä½¿ç”¨äº†æœªåœ¨è£…å¤‡åˆ—è¡¨ä¸­çš„ç‰©å“ï¼Ÿ
æ³¨æ„ï¼šè¯·è¿›è¡Œè¯­ä¹‰åŒ¹é…ï¼Œè€Œä¸æ˜¯ä¸¥æ ¼çš„å­—ç¬¦ä¸²åŒ¹é…ã€‚ä¾‹å¦‚"æŠ¤ç…§ï¼ˆæ—¥æœ¬ç­¾è¯ï¼‰"å’Œ"æŠ¤ç…§"åº”è¯¥è¢«è®¤ä¸ºæ˜¯åŒä¸€ä¸ªç‰©å“ã€‚"""

    try:
        import sys
        
        generate_content_lm_studio = None
        generate_content_gemini = None
        
        possible_module_names = ['app', '__main__']
        
        for module_name in possible_module_names:
            if module_name in sys.modules:
                module = sys.modules[module_name]
                if generate_content_lm_studio is None:
                    generate_content_lm_studio = getattr(module, 'generate_content_lm_studio', None)
                if generate_content_gemini is None:
                    generate_content_gemini = getattr(module, 'generate_content_gemini', None)
                
                if generate_content_lm_studio is not None and generate_content_gemini is not None:
                    break
        
        if generate_content_lm_studio is None or generate_content_gemini is None:
            return False
        
        if selected_model == "gemini":
            if not api_key or not model_name:
                return False
            
            success, result = generate_content_gemini(
                api_key=api_key,
                model_name=model_name,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                base_url=gemini_base_url,
                max_output_tokens=50,
                temperature=0.1,
                stream=False,
            )
            if success and result:
                return "ä¸ä¸€è‡´" in result.strip()
        else:
            if not base_url or not model_name:
                return False
            
            success, result = generate_content_lm_studio(
                base_url=base_url,
                model_name=model_name,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                max_tokens=50,
                temperature=0.1,
                stream=False,
            )
            if success and result:
                return "ä¸ä¸€è‡´" in result.strip()
    except Exception as e:
        print(f"ç‰©å“ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
        return False
    
    return False


def _extract_used_item_from_text(
    equipment_list: str,
    current_text: str,
    char_name: str,
    selected_model: str,
    base_url: Optional[str],
    model_name: Optional[str],
    api_key: Optional[str],
    gemini_base_url: Optional[str],
) -> Optional[str]:
    """
    ä»æ­£æ–‡ä¸­æå–è§’è‰²ä½¿ç”¨çš„ç‰©å“åç§°ï¼ˆç”¨äºæ›´è¯¦ç»†çš„é”™è¯¯æ¶ˆæ¯ï¼‰
    
    Args:
        equipment_list: è£…å¤‡åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²ï¼‰
        current_text: å½“å‰ç« èŠ‚æ­£æ–‡
        char_name: è§’è‰²å
        selected_model: æ¨¡å‹ç±»å‹
        base_url: LM Studio base_url
        model_name: æ¨¡å‹åç§°
        api_key: Gemini API Key
        gemini_base_url: Gemini base_url
    
    Returns:
        ä½¿ç”¨çš„ç‰©å“åç§°ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å› None
    """
    if not current_text:
        return None
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´åˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»æ­£æ–‡ä¸­æå–è§’è‰²æ˜ç¡®ä½¿ç”¨çš„ç‰©å“åç§°ã€‚

è¯·ä»”ç»†åˆ†ææ­£æ–‡ï¼Œæ‰¾å‡ºè§’è‰²æ˜ç¡®æŒæœ‰ã€ä½¿ç”¨ã€æ“ä½œçš„ç‰©å“åç§°ã€‚
å¦‚æœæ‰¾åˆ°äº†ï¼Œåªè¿”å›ç‰©å“åç§°ï¼ˆä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ï¼‰ã€‚
å¦‚æœæ²¡æœ‰æ‰¾åˆ°æˆ–æ— æ³•ç¡®å®šï¼Œè¿”å›"æ— æ³•ç¡®å®š"ã€‚

åªè¿”å›ç‰©å“åç§°æˆ–"æ— æ³•ç¡®å®š"ï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ã€‚"""

    user_prompt = f"""è§’è‰²åï¼š{char_name}

ã€è£…å¤‡åˆ—è¡¨ã€‘
{equipment_list}

ã€æ­£æ–‡ç‰‡æ®µã€‘
{current_text[:800]}

è¯·æå–è¯¥è§’è‰²åœ¨æ­£æ–‡ä¸­æ˜ç¡®ä½¿ç”¨çš„ç‰©å“åç§°ï¼ˆè¯¥ç‰©å“ä¸åœ¨è£…å¤‡åˆ—è¡¨ä¸­ï¼‰ã€‚"""

    try:
        import sys
        
        generate_content_lm_studio = None
        generate_content_gemini = None
        
        possible_module_names = ['app', '__main__']
        
        for module_name in possible_module_names:
            if module_name in sys.modules:
                module = sys.modules[module_name]
                if generate_content_lm_studio is None:
                    generate_content_lm_studio = getattr(module, 'generate_content_lm_studio', None)
                if generate_content_gemini is None:
                    generate_content_gemini = getattr(module, 'generate_content_gemini', None)
                
                if generate_content_lm_studio is not None and generate_content_gemini is not None:
                    break
        
        if generate_content_lm_studio is None or generate_content_gemini is None:
            return None
        
        if selected_model == "gemini":
            if not api_key or not model_name:
                return None
            
            success, result = generate_content_gemini(
                api_key=api_key,
                model_name=model_name,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                base_url=gemini_base_url,
                max_output_tokens=50,
                temperature=0.1,
                stream=False,
            )
            if success and result:
                result = result.strip()
                if result and result != "æ— æ³•ç¡®å®š":
                    return result
        else:
            if not base_url or not model_name:
                return None
            
            success, result = generate_content_lm_studio(
                base_url=base_url,
                model_name=model_name,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                max_tokens=50,
                temperature=0.1,
                stream=False,
            )
            if success and result:
                result = result.strip()
                if result and result != "æ— æ³•ç¡®å®š":
                    return result
    except Exception as e:
        print(f"æå–ç‰©å“åç§°å¤±è´¥: {e}")
        return None
    
    return None


def _check_history_conflicts(
    current_node: ChapterNode,
    vector_db: Any,
    project_name: str,
    fact_book: Optional[Dict[str, Any]] = None,
) -> Dict[str, List[Dict[str, Any]]]:
    """
    çºµå‘æ ¡éªŒï¼šæ£€æŸ¥ä¸å†å²äº‹å® / fact_book çš„å†²çª

    Chatbot åœºæ™¯ä¸‹ä¼˜å…ˆè§†ä¸ºâ€œäº‹å®è´¦æœ¬é˜²å´©â€å±‚ï¼š
    - å°†å½“å‰å›å¤è§†ä¸ºä¸€ä¸ªæ–°çš„â€œäº‹å®å£°æ˜â€
    - ä¸ fact_book ä¸­çš„å·²çŸ¥çœŸå€¼åšç®€å•å­—ç¬¦ä¸²/æ¨¡å¼åŒ¹é…ï¼Œå‘ç°æ˜æ˜¾è‡ªç›¸çŸ›ç›¾çš„è¯´æ³•

    åœ¨æœªæä¾› fact_book æ—¶ï¼Œä¼šé€€å›å¤ªä¸€å¼•æ“æ—¶ä»£çš„å‰§æƒ…æ ‡å¿—å†²çªæ£€æŸ¥é€»è¾‘ï¼Œ
    ä»¥ä¿æŒå¯¹æ—§å°è¯´é¡¹ç›®çš„å…¼å®¹æ€§ã€‚

    Args:
        current_node: å½“å‰ç« èŠ‚èŠ‚ç‚¹ï¼ˆåœ¨ Chatbot ä¸­å¯è§†ä¸ºâ€œå½“å‰è½®å¯¹è¯èŠ‚ç‚¹â€ï¼‰
        vector_db: å‘é‡æ•°æ®åº“è¿æ¥ï¼ˆå…¼å®¹å‚æ•°ï¼Œå½“å‰å®ç°ä¸ä¾èµ–ï¼‰
        project_name: é¡¹ç›®åç§°
        fact_book: äº‹å®è´¦æœ¬ï¼ˆå¦‚ {"user_name": "åäºŒ", "user_location": "å¹¿ä¸œ"}ï¼‰

    Returns:
        åŒ…å« critical å’Œ warnings çš„å­—å…¸
    """
    conflicts: Dict[str, List[Dict[str, Any]]] = {
        "critical": [],
        "warnings": [],
    }

    reply_text = (current_node.text_content or "").strip() if current_node else ""

    # ========== ä¼˜å…ˆåˆ†æ”¯ï¼šåŸºäº fact_book çš„äº‹å®é˜²å´© ==========
    if fact_book and isinstance(fact_book, dict) and reply_text:
        for key, value in fact_book.items():
            if value is None:
                continue
            # ç»Ÿä¸€è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œå»æ‰å¤šä½™ç©ºç™½
            str_value = str(value).strip()
            if not str_value:
                continue

            conflict_info = _detect_fact_conflict(
                fact_key=str(key),
                fact_value=str_value,
                reply_text=reply_text,
            )
            if not conflict_info:
                continue

            conflicts["critical"].append({
                "type": "fact_conflict",
                "level": ConflictLevel.CRITICAL.value,
                "fact_key": key,
                "fact_value": str_value,
                "evidence": conflict_info.get("evidence", ""),
                "message": conflict_info["message"],
            })

        return conflicts

    # ========== å…¼å®¹åˆ†æ”¯ï¼šä¿ç•™åŸæœ‰â€œå‰§æƒ…æ ‡å¿— vs å†å²è®°å¿†â€é€»è¾‘ ==========
    if not current_node.narrative_state or not current_node.narrative_state.plot_flags:
        return conflicts

    try:
        # å°è¯•å¯¼å…¥ memory_engine
        from memory_engine import MemoryEngine, get_memory_engine

        # è·å–è®°å¿†å¼•æ“
        memory_engine = get_memory_engine(project_name=project_name)
        if not memory_engine:
            return conflicts

        # å¯¹æ¯ä¸ªå‰§æƒ…æ ‡å¿—è¿›è¡Œæ£€ç´¢
        for flag in current_node.narrative_state.plot_flags:
            if not flag or len(flag.strip()) < 2:
                continue

            # æ„å»ºæŸ¥è¯¢ï¼šæŸ¥æ‰¾ä¸å½“å‰æ ‡å¿—å†²çªçš„å†å²è®°å½•
            query_text = f"{flag} å†²çª çŸ›ç›¾ ä¸ä¸€è‡´"

            try:
                success, results = memory_engine.search_memory(
                    query_text=query_text,
                    top_k=5,
                    novel_name=project_name,
                    use_summary=True,
                    enhance_query=False,
                    current_chapter_id=current_node.chapter_id,
                )

                if success and results:
                    # æ£€æŸ¥ç»“æœä¸­æ˜¯å¦æœ‰å†²çª
                    for result in results:
                        result_text = result.get("text", "") or result.get("summary", "")
                        result_chapter_id = result.get("chapter_id", 0)

                        # ç®€å•çš„å†²çªæ£€æµ‹ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸åæˆ–çŸ›ç›¾çš„æè¿°
                        if _is_conflicting(flag, result_text):
                            conflicts["critical"].append({
                                "type": "history_conflict",
                                "level": ConflictLevel.CRITICAL.value,
                                "current_flag": flag,
                                "conflicting_text": result_text[:200],
                                "conflicting_chapter_id": result_chapter_id,
                                "message": f"å‰§æƒ…æ ‡å¿—ã€Œ{flag}ã€ä¸ç¬¬ {result_chapter_id} ç« çš„å†å²è®°å½•å†²çª",
                            })
            except Exception as e:
                print(f"å†å²å†²çªæ£€æŸ¥å¤±è´¥: {e}")
                continue

    except ImportError:
        # memory_engine ä¸å¯ç”¨
        pass
    except Exception as e:
        print(f"å†å²å†²çªæ£€æŸ¥å¼‚å¸¸: {e}")

    return conflicts


def _check_profile_conflicts(
    current_node: ChapterNode,
    character_table: List[Dict[str, str]],
) -> Dict[str, List[Dict[str, Any]]]:
    """
    æ¨ªå‘æ ¡éªŒï¼šæ£€æŸ¥ä¸è§’è‰²è®¾å®šçš„å†²çª
    
    æ‹¿å½“å‰çš„ character_states å»æ¯”å¯¹ character_tableã€‚
    å¦‚æœè¡Œä¸ºè¿èƒŒäººè®¾æ ¸å¿ƒï¼ˆå¦‚"æ‡¦å¼±è€…çªç„¶æ€äºº"ä¸”æ— é“ºå«ï¼‰ï¼Œç”Ÿæˆè­¦å‘Šã€‚
    
    Args:
        current_node: å½“å‰ç« èŠ‚èŠ‚ç‚¹
        character_table: è§’è‰²è¡¨
    
    Returns:
        åŒ…å« critical å’Œ warnings çš„å­—å…¸
    """
    conflicts = {
        "critical": [],
        "warnings": [],
    }
    
    if not current_node.narrative_state or not current_node.narrative_state.characters or not character_table:
        return conflicts
    
    # æ„å»ºè§’è‰²è®¾å®šå­—å…¸
    character_profiles = {}
    for char in character_table:
        char_name = char.get("name", "").strip()
        char_desc = char.get("description", "").strip()
        if char_name:
            character_profiles[char_name] = char_desc
    
    # æ£€æŸ¥æ¯ä¸ªè§’è‰²çš„çŠ¶æ€æ˜¯å¦ä¸è®¾å®šå†²çª
    narrative_state = current_node.narrative_state
    for char_name, char_state in narrative_state.characters.items():
        if char_name not in character_profiles:
            continue
        
        profile = character_profiles[char_name].lower()
        psyche = char_state.get("psyche", "").lower()
        focus = char_state.get("focus", "").lower()
        
        # æ£€æŸ¥å¿ƒç†çŠ¶æ€/è¡ŒåŠ¨å…ƒæ˜¯å¦ä¸è§’è‰²è®¾å®šå†²çª
        if psyche or focus:
            # ç®€å•çš„å…³é”®è¯åŒ¹é…æ£€æŸ¥
            conflict_keywords = _detect_profile_conflict(profile, psyche, focus)
            
            if conflict_keywords:
                # æ£€æŸ¥æ˜¯å¦æœ‰é“ºå«ï¼ˆé€šè¿‡æ£€æŸ¥å‰ä¸€ç« çš„çŠ¶æ€ï¼‰
                has_setup = False  # è¿™é‡Œå¯ä»¥æ‰©å±•ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰åˆç†çš„é“ºå«
                
                if has_setup:
                    # æœ‰é“ºå«ï¼Œåªæ˜¯è­¦å‘Š
                    conflicts["warnings"].append({
                        "type": "profile_conflict",
                        "level": ConflictLevel.WARNING.value,
                        "character": char_name,
                        "profile": profile[:100],
                        "current_state": f"å¿ƒç†: {psyche}, è¡ŒåŠ¨å…ƒ: {focus}",
                        "conflict_keywords": conflict_keywords,
                        "message": f"è§’è‰²ã€Œ{char_name}ã€çš„è¡Œä¸ºå¯èƒ½ä¸è§’è‰²è®¾å®šå­˜åœ¨åå·®ï¼Œä½†å¯èƒ½æœ‰åˆç†é“ºå«",
                    })
                else:
                    # æ— é“ºå«ï¼Œä¸¥é‡é”™è¯¯
                    conflicts["critical"].append({
                        "type": "profile_conflict",
                        "level": ConflictLevel.CRITICAL.value,
                        "character": char_name,
                        "profile": profile[:100],
                        "current_state": f"å¿ƒç†: {psyche}, è¡ŒåŠ¨å…ƒ: {focus}",
                        "conflict_keywords": conflict_keywords,
                        "message": f"è§’è‰²ã€Œ{char_name}ã€çš„è¡Œä¸ºè¿èƒŒè§’è‰²è®¾å®šæ ¸å¿ƒï¼Œä¸”æ— åˆç†é“ºå«ï¼ˆOOCï¼‰",
                    })
    
    return conflicts


def _is_conflicting(flag: str, history_text: str) -> bool:
    """
    æ£€æŸ¥å‰§æƒ…æ ‡å¿—æ˜¯å¦ä¸å†å²æ–‡æœ¬å†²çª
    
    Args:
        flag: å½“å‰å‰§æƒ…æ ‡å¿—
        history_text: å†å²æ–‡æœ¬
    
    Returns:
        æ˜¯å¦å†²çª
    """
    if not flag or not history_text:
        return False
    
    flag_lower = flag.lower()
    text_lower = history_text.lower()
    
    # å®šä¹‰ä¸€äº›å†²çªæ¨¡å¼
    conflict_patterns = {
        "å·²æ­»": ["ä¸æ­»", "å¤æ´»", "æ´»ç€", "æœªæ­»"],
        "æ­»äº¡": ["ä¸æ­»", "å¤æ´»", "æ´»ç€", "æœªæ­»"],
        "å¤±å»": ["æ‹¥æœ‰", "è·å¾—", "å¾—åˆ°"],
        "ç ´å": ["å®Œå¥½", "ä¿®å¤", "é‡å»º"],
        "å¤±è´¥": ["æˆåŠŸ", "èƒœåˆ©", "å®Œæˆ"],
        "ç¦»å¼€": ["åˆ°è¾¾", "åœ¨", "ä½äº"],
    }
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å†²çªæ¨¡å¼
    for key, opposites in conflict_patterns.items():
        if key in flag_lower:
            for opposite in opposites:
                if opposite in text_lower:
                    return True
    
    return False


def _detect_fact_conflict(
    fact_key: str,
    fact_value: str,
    reply_text: str,
) -> Optional[Dict[str, str]]:
    """
    æ£€æµ‹å½“å‰å›å¤æ˜¯å¦ä¸ fact_book ä¸­çš„æŸæ¡äº‹å®æ˜æ˜¾çŸ›ç›¾ï¼ˆè½»é‡çº§å¯å‘å¼è§„åˆ™ï¼‰ã€‚

    è®¾è®¡ç›®æ ‡ï¼š
    - é«˜ç½®ä¿¡åº¦æ‹¦æˆªâ€œè‡ªæˆ‘å¦å®šâ€æˆ–â€œæ˜æ˜¾æ”¹å£â€çš„æƒ…å†µï¼›
    - ä½¿ç”¨ç®€å•å­—ç¬¦ä¸²/æ­£åˆ™ï¼Œä¸å¼•å…¥é¢å¤– NLP ä¾èµ–ï¼›
    - å°½é‡é¿å…è¿‡åº¦æ•æ„Ÿï¼ˆå®å¯å°‘æŠ¥ä¹Ÿä¸è¦ä¹±æŠ¥ï¼‰ã€‚

    è¿”å›:
        åŒ…å« message å’Œ evidence çš„å­—å…¸ï¼Œå¦‚æœæœªå‘ç°å†²çªåˆ™è¿”å› None
    """
    if not fact_key or not fact_value or not reply_text:
        return None

    text = reply_text
    value = fact_value

    # 1. ç›´æ¥å¦å®šæ¨¡å¼ï¼ˆâ€œä¸æ˜¯X / ä¸åœ¨X / æ²¡åœ¨Xâ€ï¼‰â€”â€”é€‚ç”¨äºä»»æ„äº‹å®ç±»å‹
    neg_patterns = [
        f"ä¸æ˜¯{re.escape(value)}",
        f"ä¸åœ¨{re.escape(value)}",
        f"æ²¡åœ¨{re.escape(value)}",
        f"å¹¶é{re.escape(value)}",
    ]
    for p in neg_patterns:
        if p in text:
            return {
                "message": f"æ¨¡å‹åœ¨å›å¤ä¸­æ˜¾å¼å¦å®šå·²è®°å½•äº‹å®ã€Œ{fact_key}={value}ã€ï¼Œå­˜åœ¨è‡ªç›¸çŸ›ç›¾çš„é£é™©ã€‚",
                "evidence": p,
            }

    key_lower = fact_key.lower()

    # 2. ç”¨æˆ·å§“åç±»äº‹å®ï¼šå¦‚ user_name / ç”¨æˆ·å / åå­— ç­‰
    if (
        "name" in key_lower
        or "æ˜µç§°" in fact_key
        or "åå­—" in fact_key
        or "ç§°å‘¼" in fact_key
    ):
        # åŒ¹é…è¯¸å¦‚â€œä½ å«XXâ€â€œä½ åå­—æ˜¯XXâ€â€œæˆ‘è®°å¾—ä½ å«XXâ€ä¹‹ç±»çš„è¯´æ³•
        name_patterns = [
            r"ä½ å«([^\sï¼Œã€‚,ï¼!ï¼Ÿ?]+)",
            r"ä½ çš„åå­—æ˜¯([^\sï¼Œã€‚,ï¼!ï¼Ÿ?]+)",
            r"æˆ‘è®°å¾—ä½ å«([^\sï¼Œã€‚,ï¼!ï¼Ÿ?]+)",
        ]
        for pat in name_patterns:
            m = re.search(pat, text)
            if m:
                mentioned = m.group(1).strip()
                if mentioned and mentioned != value:
                    return {
                        "message": f"ç”¨æˆ·å§“ååœ¨ fact_book ä¸­è®°å½•ä¸ºã€Œ{value}ã€ï¼Œä½†å½“å‰å›å¤ä¸­ç§°å‘¼ä¸ºã€Œ{mentioned}ã€ï¼Œç–‘ä¼¼è‡ªç›¸çŸ›ç›¾ã€‚",
                        "evidence": m.group(0),
                    }
        return None

    # 3. ä½ç½®/åŸå¸‚ç±»äº‹å®ï¼šå¦‚ location / city / çœ / åŸå¸‚ / åœ°åŒº ç­‰
    if (
        "location" in key_lower
        or "city" in key_lower
        or "åŸå¸‚" in fact_key
        or "åœ°åŒº" in fact_key
        or "çœ" in fact_key
    ):
        # ç®€å•åŒ¹é…â€œåœ¨XXXâ€è¿™ç§å¥å¼ï¼Œæ’é™¤ä¸å·²çŸ¥ value å®Œå…¨ä¸€è‡´çš„æƒ…å†µ
        # ä¾‹å¦‚ï¼šfact_book ä¸­ä¸ºâ€œå¹¿ä¸œâ€ï¼Œä½†å›å¤è¯´â€œä½ ç°åœ¨åœ¨ä¸Šæµ·â€
        loc_pattern = r"åœ¨([^\sï¼Œã€‚,ï¼!ï¼Ÿ?]+)"
        m = re.search(loc_pattern, text)
        if m:
            loc = m.group(1).strip()
            if loc and loc != value and value not in text:
                return {
                    "message": f"ç”¨æˆ·å¸¸é©»åœ°ç‚¹åœ¨ fact_book ä¸­è®°å½•ä¸ºã€Œ{value}ã€ï¼Œä½†å½“å‰å›å¤ä¸­æåˆ°ã€Œ{loc}ã€ï¼Œå¯èƒ½ä¸æ—¢æœ‰äº‹å®ä¸ä¸€è‡´ã€‚",
                    "evidence": m.group(0),
                }
        return None

    # 4. å…¶ä»–é”®ï¼šå½“å‰ä»…åšä¿å®ˆå¤„ç†ï¼Œä¸ä¸»åŠ¨æŠ¥å†²çªï¼ˆé¿å…è¯¯ä¼¤ï¼‰
    return None


def _detect_profile_conflict(profile: str, psyche: str, focus: str) -> List[str]:
    """
    æ£€æµ‹è§’è‰²çŠ¶æ€æ˜¯å¦ä¸è®¾å®šå†²çª
    
    Args:
        profile: è§’è‰²è®¾å®šæè¿°
        psyche: å½“å‰å¿ƒç†çŠ¶æ€
        focus: å½“å‰è¡ŒåŠ¨å…ƒ
    
    Returns:
        å†²çªå…³é”®è¯åˆ—è¡¨
    """
    conflicts = []
    
    # å®šä¹‰ä¸€äº›å†²çªæ¨¡å¼
    conflict_rules = [
        {
            "profile_keywords": ["æ‡¦å¼±", "èƒ†å°", "æ€¯æ‡¦"],
            "state_keywords": ["æ„¤æ€’", "æ€äºº", "æ”»å‡»", "æš´åŠ›"],
            "conflict": "æ‡¦å¼±è€…çªç„¶æš´åŠ›",
        },
        {
            "profile_keywords": ["å–„è‰¯", "ä»æ…ˆ", "æ¸©å’Œ"],
            "state_keywords": ["æ®‹å¿", "æ€æˆ®", "æ— æƒ…"],
            "conflict": "å–„è‰¯è€…çªç„¶æ®‹å¿",
        },
        {
            "profile_keywords": ["å†·é™", "ç†æ™º", "æ²‰ç€"],
            "state_keywords": ["å´©æºƒ", "å¤±æ§", "ç–¯ç‹‚"],
            "conflict": "å†·é™è€…çªç„¶å¤±æ§",
        },
    ]
    
    state_text = f"{psyche} {focus}".lower()
    
    for rule in conflict_rules:
        profile_match = any(kw in profile for kw in rule["profile_keywords"])
        state_match = any(kw in state_text for kw in rule["state_keywords"])
        
        if profile_match and state_match:
            conflicts.append(rule["conflict"])
    
    return conflicts


# ==================== å‘é‡ OOC æ£€æµ‹ ====================

def calculate_ooc_scores(
    current_node: ChapterNode,
    character_table: List[Dict[str, str]],
) -> Dict[str, float]:
    """
    æ‰¹é‡è®¡ç®—æ‰€æœ‰è§’è‰²çš„ OOC åˆ†æ•°ï¼ˆåŸºäºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    
    Args:
        current_node: å½“å‰ç« èŠ‚èŠ‚ç‚¹
        character_table: è§’è‰²è¡¨ï¼Œæ ¼å¼ä¸º [{"name": "è§’è‰²å", "description": "æè¿°"}, ...]
    
    Returns:
        OOC åˆ†æ•°å­—å…¸ï¼Œæ ¼å¼ä¸º {è§’è‰²å: ä½™å¼¦ç›¸ä¼¼åº¦åˆ†æ•°}
        - åˆ†æ•° < 0.4: ä¸¥é‡ OOC
        - åˆ†æ•° < 0.6: è­¦å‘Š
        - åˆ†æ•° >= 0.6: æ­£å¸¸
    """
    ooc_scores = {}
    
    if not NUMPY_AVAILABLE or np is None:
        return ooc_scores
    
    if not current_node.narrative_state or not current_node.narrative_state.characters:
        return ooc_scores
    
    # è·å– Embedding æ¨¡å‹
    embedding_model = get_embedding_model()
    if embedding_model is None:
        print("âš ï¸ OOC æ£€æµ‹ï¼šget_embedding_model() è¿”å› Noneï¼ˆembedding æ¨¡å‹æœªåŠ è½½ï¼‰")
        return ooc_scores
    
    # æ„å»ºè§’è‰²è®¾å®šå­—å…¸ï¼ˆåªæ£€æµ‹ character_table ä¸­å®šä¹‰çš„è§’è‰²ï¼‰
    character_profiles = {}
    for char in character_table:
        char_name = char.get("name", "").strip()
        char_desc = char.get("description", "").strip()
        if char_name:
            if not char_desc:
                print(f"âš ï¸ OOC æ£€æµ‹ï¼šè§’è‰² {char_name} çš„æè¿°ä¸ºç©ºï¼Œè·³è¿‡è¯¥è§’è‰²çš„ OOC æ£€æµ‹")
            else:
                character_profiles[char_name] = char_desc
    
    if not character_profiles:
        print(f"âš ï¸ OOC æ£€æµ‹ï¼šcharacter_profiles ä¸ºç©ºï¼ˆcharacter_table é•¿åº¦ï¼š{len(character_table)}ï¼Œæœ‰æ•ˆè§’è‰²æ•°ï¼š{len(character_profiles)}ï¼‰")
        return ooc_scores
    
    narrative_state = current_node.narrative_state
    if not narrative_state:
        print("âš ï¸ OOC æ£€æµ‹ï¼šnarrative_state ä¸ºç©º")
        return ooc_scores
    
    if not narrative_state.characters:
        print("âš ï¸ OOC æ£€æµ‹ï¼šnarrative_state.characters ä¸ºç©º")
        return ooc_scores
    
    print(f"ğŸ” OOC æ£€æµ‹ï¼šå¼€å§‹æ£€æµ‹ {len(narrative_state.characters)} ä¸ªè§’è‰²ï¼Œcharacter_table ä¸­æœ‰ {len(character_profiles)} ä¸ªè§’è‰²å®šä¹‰")
    
    # ä¼˜å…ˆä½¿ç”¨ character_vectorsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    character_vectors = narrative_state.character_vectors if narrative_state.character_vectors else {}
    
    # æ˜µç§° / ç®€ç§°è§£æï¼šå°è¯•å°†ã€Œå‡›ã€æ˜ å°„åˆ°ã€Œä¸‰æ¡¥å‡›ã€ç­‰
    def _resolve_character_name(short_name: str) -> Optional[str]:
        """
        å°†ç« èŠ‚ä¸­çš„ç§°å‘¼æ˜ å°„ä¸ºè§’è‰²è¡¨ä¸­çš„å…¨åã€‚
        
        ç­–ç•¥ï¼ˆä¿å®ˆï¼‰ï¼š
        - å®Œå…¨åŒ¹é…ä¼˜å…ˆ
        - å¦åˆ™æŸ¥æ‰¾åŒ…å«å…³ç³»ï¼ˆå¦‚ 'å‡›' æ˜¯ 'ä¸‰æ¡¥å‡›' çš„å­ä¸²ï¼‰ï¼Œä¸”å€™é€‰å”¯ä¸€æ—¶æ‰æ¥å—
        """
        if short_name in character_profiles:
            return short_name
        
        candidates = []
        for full_name in character_profiles.keys():
            if not full_name:
                continue
            if short_name in full_name or full_name in short_name:
                candidates.append(full_name)
        
        if len(candidates) == 1:
            resolved = candidates[0]
            print(f"ğŸ” OOC æ£€æµ‹ï¼šå°†ç§°å‘¼ã€Œ{short_name}ã€è§†ä¸ºè§’è‰²ã€Œ{resolved}ã€çš„æ˜µç§°/ç®€ç§°")
            return resolved
        
        return None
    
    # åªæ£€æµ‹åœ¨å½“å‰ç« èŠ‚çŠ¶æ€ä¸­å‡ºç°çš„è§’è‰²ï¼Œä¸”è¿™äº›è§’è‰²ä¹Ÿåœ¨ character_table ä¸­ï¼ˆæˆ–èƒ½é€šè¿‡æ˜µç§°æ˜ å°„åˆ°å…¶ä¸­ï¼‰
    # è¿™æ ·å¯ä»¥é¿å…éå†æ‰€æœ‰è§’è‰²ï¼Œåªæ£€æµ‹å®é™…å‡ºç°çš„è§’è‰²
    checked_count = 0
    for char_name, char_state in narrative_state.characters.items():
        resolved_name = _resolve_character_name(char_name)
        if not resolved_name:
            # å¦‚æœæ— æ³•åœ¨è§’è‰²è¡¨ä¸­æ‰¾åˆ°å¯¹åº”æ¡ç›®ï¼Œåˆ™è·³è¿‡
            print(f"âš ï¸ OOC æ£€æµ‹ï¼šè§’è‰² {char_name} ä¸åœ¨ character_table ä¸­ï¼Œä¸”æ— æ³•é€šè¿‡æ˜µç§°æ˜ å°„ï¼Œè·³è¿‡")
            continue
        
        checked_count += 1
        char_desc = character_profiles[resolved_name]
        print(f"ğŸ” OOC æ£€æµ‹ï¼šæ­£åœ¨æ£€æµ‹è§’è‰² {resolved_name}ï¼ˆç§°å‘¼ï¼š{char_name}ï¼‰...")
        # è·å–è§’è‰²æ ¸å¿ƒå‘é‡ï¼ˆäººè®¾ï¼‰
        character_core_vector = get_character_core_vector(
            character_name=resolved_name,
            character_description=char_desc,
            embedding_model=embedding_model,
        )
        
        if character_core_vector is None:
            print(f"âš ï¸ æ— æ³•ç”Ÿæˆè§’è‰² {resolved_name} çš„æ ¸å¿ƒå‘é‡ï¼Œè·³è¿‡ OOC æ£€æµ‹")
            continue
        
        # è·å–è§’è‰²å½“å‰çŠ¶æ€å‘é‡ï¼ˆä¼˜å…ˆä½¿ç”¨ character_vectorsï¼‰
        current_vector = None
        
        if resolved_name in character_vectors and character_vectors[resolved_name]:
            # ä¼˜å…ˆä½¿ç”¨å·²è®¡ç®—çš„ character_vectors
            vector_data = character_vectors[resolved_name]
            if isinstance(vector_data, list) and len(vector_data) > 0:
                current_vector = np.array(vector_data)
            elif isinstance(vector_data, np.ndarray) and len(vector_data) > 0:
                current_vector = vector_data
        
        # å¦‚æœ character_vectors ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»å½“å‰çŠ¶æ€ç”Ÿæˆ
        if current_vector is None:
            physique = char_state.get("physique", "")
            psyche = char_state.get("psyche", "")
            if physique or psyche:
                state_text = f"{physique} {psyche}".strip()
                try:
                    current_vector = embedding_model.encode(state_text, convert_to_numpy=True)
                except Exception as e:
                    print(f"ç”Ÿæˆè§’è‰² {resolved_name} çš„çŠ¶æ€å‘é‡å¤±è´¥: {e}")
                    continue
            else:
                # å¦‚æœçŠ¶æ€ä¸ºç©ºï¼Œè·³è¿‡è¯¥è§’è‰²
                print(f"âš ï¸ è§’è‰² {resolved_name} çš„çŠ¶æ€ä¸ºç©ºï¼ˆphysique å’Œ psyche éƒ½ä¸ºç©ºï¼‰ï¼Œè·³è¿‡ OOC æ£€æµ‹")
                continue
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        try:
            dot_product = np.dot(character_core_vector, current_vector)
            norm_core = np.linalg.norm(character_core_vector)
            norm_current = np.linalg.norm(current_vector)
            
            if norm_core > 0 and norm_current > 0:
                cosine_similarity = dot_product / (norm_core * norm_current)
                ooc_scores[resolved_name] = float(cosine_similarity)
                print(f"âœ… OOC æ£€æµ‹ï¼šè§’è‰² {resolved_name} çš„ OOC åˆ†æ•° = {cosine_similarity:.3f}")
            else:
                ooc_scores[resolved_name] = 0.0
                print(f"âš ï¸ OOC æ£€æµ‹ï¼šè§’è‰² {resolved_name} çš„å‘é‡èŒƒæ•°ä¸º 0ï¼Œè®¾ç½® OOC åˆ†æ•°ä¸º 0.0")
        except Exception as e:
            print(f"âŒ è®¡ç®—è§’è‰² {resolved_name} çš„ OOC åˆ†æ•°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            ooc_scores[resolved_name] = 0.0
    
    print(f"ğŸ” OOC æ£€æµ‹å®Œæˆï¼šæ£€æµ‹äº† {checked_count} ä¸ªè§’è‰²ï¼Œè¿”å› {len(ooc_scores)} ä¸ªåˆ†æ•°")
    return ooc_scores


# ==================== NTD å‡çº§ï¼šå™äº‹èƒ½é‡å‡½æ•° ====================

def calculate_narrative_energy(
    current_node: ChapterNode,
    prev_node: Optional[ChapterNode] = None,
    target_vector: Optional[List[float]] = None,
) -> Tuple[float, Dict[str, float]]:
    """
    è®¡ç®—å™äº‹èƒ½é‡ï¼ˆNTD å‡çº§ï¼‰
    
    èƒ½é‡å‡½æ•°ï¼šE = E_consistency + E_target
    
    - E_consistencyï¼ˆä¸€è‡´æ€§åŠ¿èƒ½ï¼‰ï¼šè®¡ç®— current_node.state_vector ä¸ prev_node çš„ä½™å¼¦è·ç¦»ã€‚
      å¦‚æœè·ç¦»è¿‡å¤§ï¼ˆçªå˜ï¼‰ï¼Œèƒ½é‡é£™å‡ã€‚
    - E_targetï¼ˆç›®æ ‡åŠ¿èƒ½ï¼‰ï¼šï¼ˆå¦‚æœæœ‰å¤§çº²å‘é‡ï¼‰è®¡ç®—ä¸å¤§çº²çš„è·ç¦»ã€‚
    
    Args:
        current_node: å½“å‰ç« èŠ‚èŠ‚ç‚¹
        prev_node: å‰ä¸€ç« èŠ‚ç‚¹ï¼ˆå¯é€‰ï¼‰
        target_vector: ç›®æ ‡å‘é‡ï¼ˆå¤§çº²å‘é‡ï¼Œå¯é€‰ï¼‰
    
    Returns:
        (æ€»èƒ½é‡, èƒ½é‡åˆ†è§£å­—å…¸)
    """
    if not NUMPY_AVAILABLE or np is None:
        return 0.0, {}
    
    if current_node.state_vector is None:
        return 0.0, {}
    
    energy_breakdown = {}
    total_energy = 0.0
    
    # è½¬æ¢ä¸º numpy æ•°ç»„
    current_vector = np.array(current_node.state_vector)
    
    # 1. ä¸€è‡´æ€§åŠ¿èƒ½ E_consistency
    if prev_node and prev_node.state_vector is not None:
        prev_vector = np.array(prev_node.state_vector)
        
        # è®¡ç®—ä½™å¼¦è·ç¦»ï¼ˆ1 - ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        dot_product = np.dot(current_vector, prev_vector)
        norm_current = np.linalg.norm(current_vector)
        norm_prev = np.linalg.norm(prev_vector)
        
        if norm_current > 0 and norm_prev > 0:
            cosine_similarity = dot_product / (norm_current * norm_prev)
            cosine_distance = 1.0 - cosine_similarity
            
            # ä¸€è‡´æ€§åŠ¿èƒ½ï¼šè·ç¦»è¶Šå¤§ï¼Œèƒ½é‡è¶Šé«˜
            # ä½¿ç”¨å¹³æ–¹å‡½æ•°ï¼Œä½¿çªå˜æ›´æ˜æ˜¾
            e_consistency = cosine_distance ** 2
            energy_breakdown["consistency"] = float(e_consistency)
            total_energy += e_consistency
    else:
        energy_breakdown["consistency"] = 0.0
    
    # 2. ç›®æ ‡åŠ¿èƒ½ E_target
    if target_vector is not None:
        target_vec = np.array(target_vector)
        
        # è®¡ç®—ä¸ç›®æ ‡çš„ä½™å¼¦è·ç¦»
        dot_product = np.dot(current_vector, target_vec)
        norm_current = np.linalg.norm(current_vector)
        norm_target = np.linalg.norm(target_vec)
        
        if norm_current > 0 and norm_target > 0:
            cosine_similarity = dot_product / (norm_current * norm_target)
            cosine_distance = 1.0 - cosine_similarity
            
            # ç›®æ ‡åŠ¿èƒ½ï¼šè·ç¦»è¶Šå¤§ï¼Œèƒ½é‡è¶Šé«˜
            e_target = cosine_distance ** 2
            energy_breakdown["target"] = float(e_target)
            total_energy += e_target
    else:
        energy_breakdown["target"] = 0.0
    
    return float(total_energy), energy_breakdown
