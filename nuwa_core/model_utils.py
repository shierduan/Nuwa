import os
import threading
from typing import Optional, Callable

EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
EMBEDDING_ENV_VAR = "NUWA_EMBEDDING_MODEL_PATH"
MODEL_CACHE_ROOT_VAR = "NUWA_MODEL_CACHE_DIR"

# é»˜è®¤ç¼“å­˜ç›®å½•ï¼š~/.nuwa/models/all-MiniLM-L6-v2
# å…¼å®¹æ—§ç‰ˆæœ¬ï¼šå¦‚æœ NUWA ç¯å¢ƒå˜é‡ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨ TAIYI ç¯å¢ƒå˜é‡
_default_cache_root = os.environ.get(
    MODEL_CACHE_ROOT_VAR,
    os.environ.get(
        "TAIYI_MODEL_CACHE_DIR",  # å…¼å®¹æ—§ç‰ˆæœ¬
        os.path.join(os.path.expanduser("~"), ".nuwa", "models"),
    )
)
DEFAULT_EMBEDDING_DIR = os.path.join(_default_cache_root, EMBEDDING_MODEL_NAME)

_embedding_dir_cache: Optional[str] = None
_embedding_dir_lock = threading.Lock()


def _path_has_model(path: Optional[str]) -> bool:
    if not path:
        return False
    config_path = os.path.join(path, "config.json")
    return os.path.isdir(path) and os.path.exists(config_path)


def ensure_embedding_model_dir(loader_cls: Optional[Callable], verbose: bool = True) -> Optional[str]:
    """
    ç¡®ä¿æœ¬åœ°å­˜åœ¨ embedding æ¨¡å‹ç›®å½•ï¼›å¦‚æœæ²¡æœ‰åˆ™ä¸‹è½½ä¸€æ¬¡ã€‚

    Args:
        loader_cls: SentenceTransformer ç±»ï¼ˆæˆ–å…¼å®¹æ¥å£ï¼‰ï¼Œç”¨äºä¸‹è½½æ¨¡å‹
        verbose: æ˜¯å¦æ‰“å°æç¤ºä¿¡æ¯

    Returns:
        å¯ç”¨çš„æœ¬åœ°æ¨¡å‹ç›®å½•è·¯å¾„ï¼›è‹¥å¤±è´¥åˆ™è¿”å› None
    """
    global _embedding_dir_cache

    if loader_cls is None:
        return None

    with _embedding_dir_lock:
        if _path_has_model(_embedding_dir_cache):
            return _embedding_dir_cache

        candidates = []

        # 1) ç¯å¢ƒå˜é‡æ˜¾å¼æŒ‡å®šï¼ˆä¼˜å…ˆä½¿ç”¨ NUWAï¼Œå…¼å®¹ TAIYIï¼‰
        env_path = os.environ.get(EMBEDDING_ENV_VAR) or os.environ.get("TAIYI_EMBEDDING_MODEL_PATH")
        if env_path:
            candidates.append(env_path)

        # 2) é¡¹ç›®å†…è‡ªå¸¦çš„æ¨¡å‹ç›®å½•
        repo_dir = os.path.join(os.path.dirname(__file__), "models", EMBEDDING_MODEL_NAME)
        candidates.append(repo_dir)

        # 3) é»˜è®¤ç¼“å­˜ç›®å½•
        candidates.append(DEFAULT_EMBEDDING_DIR)

        for candidate in candidates:
            if _path_has_model(candidate):
                _embedding_dir_cache = candidate
                return candidate

        # è‹¥æ‰€æœ‰å€™é€‰å‡ä¸å­˜åœ¨ï¼Œå°è¯•ä¸‹è½½åˆ°é»˜è®¤ç¼“å­˜
        target_dir = DEFAULT_EMBEDDING_DIR
        os.makedirs(target_dir, exist_ok=True)

        try:
            if verbose:
                print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½åµŒå…¥æ¨¡å‹ {EMBEDDING_MODEL_NAME} åˆ° {target_dir} ...")
            model = loader_cls(EMBEDDING_MODEL_NAME)
            model.save(target_dir)
            _embedding_dir_cache = target_dir
            if verbose:
                print(f"âœ… åµŒå…¥æ¨¡å‹å·²ç¼“å­˜åˆ° {target_dir}")
            return target_dir
        except Exception as e:
            if verbose:
                print(f"âŒ ä¸‹è½½åµŒå…¥æ¨¡å‹å¤±è´¥ï¼š{e}")
            return None

